{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 07e: MAML with GRU Warm-Start (XuetangX)\n",
    "\n",
    "**Purpose:** MAML initialized from pre-trained GRU baseline (warm-start meta-learning)\n",
    "\n",
    "**Key Idea:**\n",
    "- Standard MAML (NB 07): Random init â†’ Meta-train â†’ 30.52% Acc@1 âŒ (below baseline)\n",
    "- **Warm-start MAML (NB 07e)**: GRU init â†’ Meta-train â†’ **Expected: 35-38%** âœ… (beats baseline!)\n",
    "\n",
    "**Why This Works:**\n",
    "- GRU baseline (33.73%) is a strong initialization for the task\n",
    "- MAML meta-training **refines** it to be more adaptable to new users\n",
    "- Combines: Task performance (GRU) + Adaptation ability (MAML)\n",
    "\n",
    "**This IS Still Meta-Learning:**\n",
    "- âœ… Uses MAML algorithm (meta-gradients, inner/outer loop)\n",
    "- âœ… Learns adaptation strategy via meta-training\n",
    "- âœ… Only difference: Better initialization (standard practice in meta-learning)\n",
    "- ðŸ“š Similar to \"How to train your MAML\" (Antoniou et al., 2019)\n",
    "\n",
    "**Comparison:**\n",
    "- GRU Baseline (NB 06): 33.73% Acc@1\n",
    "- MAML Random Init (NB 07): 30.52% Acc@1 (-9.51%)\n",
    "- MAML Meta-SGD (NB 07c): 3.79% Acc@1 (failed)\n",
    "- **MAML Warm-Start (NB 07e): Target > 33.73%** âœ…\n",
    "\n",
    "**Inputs:**\n",
    "- Pre-trained GRU: `models/baselines/gru_global.pth` (33.73% Acc@1)\n",
    "- Episodes: Same as Notebook 07\n",
    "- Config: Same MAML hyperparameters as Notebook 07\n",
    "\n",
    "**Outputs:**\n",
    "- Warm-start MAML model: `models/maml/maml_warmstart_gru_K5.pth`\n",
    "- Results: `results/maml_warmstart_K5_Q10.json`\n",
    "- Report: `reports/07e_maml_warmstart_xuetangx/<run_tag>/report.json`\n",
    "\n",
    "**Training Time:** ~24 hours (10,000 meta-iterations, same as NB 07)\n",
    "\n",
    "**Expected Outcome:** MAML with warm-start beats GRU baseline (35-38% vs 33.73%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 07e-00] start=2026-01-11T17:55:16\n",
      "[CELL 07e-00] CWD: C:\\Users\\User\\Documents\\ml-workspace\\anonymous-users-mooc-session-meta\\notebooks\n",
      "[CELL 07e-00] REPO_ROOT: C:\\Users\\User\\Documents\\ml-workspace\\anonymous-users-mooc-session-meta\n",
      "[CELL 07e-00] META_REGISTRY=C:\\Users\\User\\Documents\\ml-workspace\\anonymous-users-mooc-session-meta\\meta.json\n",
      "[CELL 07e-00] DATA_INTERIM=C:\\Users\\User\\Documents\\ml-workspace\\anonymous-users-mooc-session-meta\\data\\interim\n",
      "[CELL 07e-00] DATA_PROCESSED=C:\\Users\\User\\Documents\\ml-workspace\\anonymous-users-mooc-session-meta\\data\\processed\n",
      "[CELL 07e-00] MODELS=C:\\Users\\User\\Documents\\ml-workspace\\anonymous-users-mooc-session-meta\\models\n",
      "[CELL 07e-00] RESULTS=C:\\Users\\User\\Documents\\ml-workspace\\anonymous-users-mooc-session-meta\\results\n",
      "[CELL 07e-00] REPORTS=C:\\Users\\User\\Documents\\ml-workspace\\anonymous-users-mooc-session-meta\\reports\n",
      "[CELL 07e-00] PyTorch device: cpu\n",
      "[CELL 07e-00] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07e-00] Bootstrap: repo root + paths + logger\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import pickle\n",
    "import hashlib\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "t0 = datetime.now()\n",
    "print(f\"[CELL 07e-00] start={t0.isoformat(timespec='seconds')}\")\n",
    "print(\"[CELL 07e-00] CWD:\", Path.cwd().resolve())\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists():\n",
    "            return p\n",
    "    raise RuntimeError(\"Could not find PROJECT_STATE.md. Open notebook from within the repo.\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "print(\"[CELL 07e-00] REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "PATHS = {\n",
    "    \"META_REGISTRY\": REPO_ROOT / \"meta.json\",\n",
    "    \"DATA_INTERIM\": REPO_ROOT / \"data\" / \"interim\",\n",
    "    \"DATA_PROCESSED\": REPO_ROOT / \"data\" / \"processed\",\n",
    "    \"MODELS\": REPO_ROOT / \"models\",\n",
    "    \"RESULTS\": REPO_ROOT / \"results\",\n",
    "    \"REPORTS\": REPO_ROOT / \"reports\",\n",
    "}\n",
    "for k, v in PATHS.items():\n",
    "    print(f\"[CELL 07e-00] {k}={v}\")\n",
    "\n",
    "def cell_start(cell_id: str, title: str, **kwargs: Any) -> float:\n",
    "    t = time.time()\n",
    "    print(f\"\\n[{cell_id}] {title}\")\n",
    "    print(f\"[{cell_id}] start={datetime.now().isoformat(timespec='seconds')}\")\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    return t\n",
    "\n",
    "def cell_end(cell_id: str, t0: float, **kwargs: Any) -> None:\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    print(f\"[{cell_id}] elapsed={time.time()-t0:.2f}s\")\n",
    "    print(f\"[{cell_id}] done\")\n",
    "\n",
    "# Check GPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[CELL 07e-00] PyTorch device: {DEVICE}\")\n",
    "print(\"[CELL 07e-00] done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-01] Seed everything\n",
      "[CELL 07e-01] start=2026-01-11T17:55:16\n",
      "[CELL 07e-01] seed=20260107\n",
      "[CELL 07e-01] elapsed=0.03s\n",
      "[CELL 07e-01] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07e-01] Reproducibility: seed everything\n",
    "\n",
    "t0 = cell_start(\"CELL 07e-01\", \"Seed everything\")\n",
    "\n",
    "GLOBAL_SEED = 20260107\n",
    "\n",
    "def seed_everything(seed: int) -> None:\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(GLOBAL_SEED)\n",
    "\n",
    "cell_end(\"CELL 07e-01\", t0, seed=GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-02] IO helpers\n",
      "[CELL 07e-02] start=2026-01-11T17:55:16\n",
      "[CELL 07e-02] elapsed=0.00s\n",
      "[CELL 07e-02] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07e-02] JSON/Pickle IO + hashing helpers\n",
    "\n",
    "t0 = cell_start(\"CELL 07e-02\", \"IO helpers\")\n",
    "\n",
    "def write_json_atomic(path: Path, obj: Any, indent: int = 2) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tmp = path.with_suffix(path.suffix + f\".tmp_{uuid.uuid4().hex}\")\n",
    "    with tmp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=indent)\n",
    "    tmp.replace(path)\n",
    "\n",
    "def read_json(path: Path) -> Any:\n",
    "    if not path.exists():\n",
    "        raise RuntimeError(f\"Missing JSON file: {path}\")\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_pickle(path: Path, obj: Any) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_pickle(path: Path) -> Any:\n",
    "    with path.open(\"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "cell_end(\"CELL 07e-02\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-03] Start run + init files\n",
      "[CELL 07e-03] start=2026-01-11T17:55:16\n",
      "[CELL 07e-03] K=5, Q=10\n",
      "[CELL 07e-03] MAML config: Î±=0.01, Î²=0.001, inner_steps=5, meta_batch=32\n",
      "[CELL 07e-03] â­ WARM-START: Initializing from GRU baseline\n",
      "[CELL 07e-03] GRU baseline: C:\\Users\\User\\Documents\\ml-workspace\\anonymous-users-mooc-session-meta\\models\\baselines\\gru_global.pth\n",
      "[CELL 07e-03] out_dir=C:\\Users\\User\\Documents\\ml-workspace\\anonymous-users-mooc-session-meta\\reports\\07e_maml_warmstart_xuetangx\\20260111_175516\n",
      "[CELL 07e-03] elapsed=0.02s\n",
      "[CELL 07e-03] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07e-03] Run tagging + config + meta.json\n",
    "\n",
    "t0 = cell_start(\"CELL 07e-03\", \"Start run + init files\")\n",
    "\n",
    "NOTEBOOK_NAME = \"07e_maml_warmstart_xuetangx\"\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_ID = uuid.uuid4().hex\n",
    "\n",
    "OUT_DIR = PATHS[\"REPORTS\"] / NOTEBOOK_NAME / RUN_TAG\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REPORT_PATH = OUT_DIR / \"report.json\"\n",
    "CONFIG_PATH = OUT_DIR / \"config.json\"\n",
    "MANIFEST_PATH = OUT_DIR / \"manifest.json\"\n",
    "\n",
    "# Paths\n",
    "EPISODES_DIR = PATHS[\"DATA_PROCESSED\"] / \"xuetangx\" / \"episodes\"\n",
    "PAIRS_DIR = PATHS[\"DATA_PROCESSED\"] / \"xuetangx\" / \"pairs\"\n",
    "VOCAB_DIR = PATHS[\"DATA_PROCESSED\"] / \"xuetangx\" / \"vocab\"\n",
    "MODELS_DIR = PATHS[\"MODELS\"] / \"maml\"\n",
    "CHECKPOINTS_DIR = MODELS_DIR / \"checkpoints_warmstart\"\n",
    "RESULTS_DIR = PATHS[\"RESULTS\"]\n",
    "\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# GRU baseline checkpoint path\n",
    "GRU_BASELINE_PATH = PATHS[\"MODELS\"] / \"baselines\" / \"gru_global.pth\"\n",
    "\n",
    "# K-shot config\n",
    "K, Q = 5, 10\n",
    "\n",
    "CFG = {\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"seed\": GLOBAL_SEED,\n",
    "    \"device\": str(DEVICE),\n",
    "    \"k_shot_config\": {\"K\": K, \"Q\": Q},\n",
    "    \"inputs\": {\n",
    "        \"episodes_train\": str(EPISODES_DIR / f\"episodes_train_K{K}_Q{Q}.parquet\"),\n",
    "        \"episodes_val\": str(EPISODES_DIR / f\"episodes_val_K{K}_Q{Q}.parquet\"),\n",
    "        \"episodes_test\": str(EPISODES_DIR / f\"episodes_test_K{K}_Q{Q}.parquet\"),\n",
    "        \"pairs_train\": str(PAIRS_DIR / \"pairs_train.parquet\"),\n",
    "        \"pairs_val\": str(PAIRS_DIR / \"pairs_val.parquet\"),\n",
    "        \"pairs_test\": str(PAIRS_DIR / \"pairs_test.parquet\"),\n",
    "        \"vocab\": str(VOCAB_DIR / \"course2id.json\"),\n",
    "        \"gru_baseline\": str(GRU_BASELINE_PATH),  # â† WARM-START FROM HERE\n",
    "    },\n",
    "    \"gru_config\": {\n",
    "        \"embedding_dim\": 64,\n",
    "        \"hidden_dim\": 128,\n",
    "        \"num_layers\": 1,\n",
    "        \"dropout\": 0.2,\n",
    "        \"max_seq_len\": 50,\n",
    "    },\n",
    "    \"maml_config\": {\n",
    "        \"inner_lr\": 0.01,           # Î±: learning rate for inner loop (task adaptation)\n",
    "        \"outer_lr\": 0.001,          # Î²: learning rate for outer loop (meta-update)\n",
    "        \"num_inner_steps\": 5,       # number of gradient steps for adaptation\n",
    "        \"meta_batch_size\": 32,      # number of tasks (users) per meta-batch\n",
    "        \"num_meta_iterations\": 10000,  # total meta-training iterations\n",
    "        \"checkpoint_interval\": 1000,   # save checkpoint every N iterations\n",
    "        \"eval_interval\": 500,          # evaluate on val set every N iterations\n",
    "        \"use_second_order\": True,      # True: MAML (2nd order), False: FOMAML (1st order)\n",
    "        \"warm_start\": True,            # â† NEW: Initialize from GRU baseline\n",
    "    },\n",
    "    \"ablation_configs\": {\n",
    "        \"support_set_sizes\": [1, 3, 5, 10],\n",
    "        \"adaptation_steps\": [1, 3, 5, 10],\n",
    "    },\n",
    "    \"metrics\": [\"accuracy@1\", \"recall@5\", \"recall@10\", \"mrr\"],\n",
    "    \"outputs\": {\n",
    "        \"models_dir\": str(MODELS_DIR),\n",
    "        \"checkpoints_dir\": str(CHECKPOINTS_DIR),\n",
    "        \"results\": str(RESULTS_DIR / f\"maml_warmstart_K{K}_Q{Q}.json\"),\n",
    "        \"out_dir\": str(OUT_DIR),\n",
    "    }\n",
    "}\n",
    "\n",
    "write_json_atomic(CONFIG_PATH, CFG)\n",
    "\n",
    "report = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"repo_root\": str(REPO_ROOT),\n",
    "    \"metrics\": {},\n",
    "    \"key_findings\": [],\n",
    "    \"sanity_samples\": {},\n",
    "    \"data_fingerprints\": {},\n",
    "    \"notes\": [],\n",
    "}\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "manifest = {\"run_id\": RUN_ID, \"notebook\": NOTEBOOK_NAME, \"run_tag\": RUN_TAG, \"artifacts\": []}\n",
    "write_json_atomic(MANIFEST_PATH, manifest)\n",
    "\n",
    "# meta.json\n",
    "META_PATH = PATHS[\"META_REGISTRY\"]\n",
    "if not META_PATH.exists():\n",
    "    write_json_atomic(META_PATH, {\"schema_version\": 1, \"runs\": []})\n",
    "meta = read_json(META_PATH)\n",
    "meta[\"runs\"].append({\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"out_dir\": str(OUT_DIR),\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "})\n",
    "write_json_atomic(META_PATH, meta)\n",
    "\n",
    "print(f\"[CELL 07e-03] K={K}, Q={Q}\")\n",
    "print(f\"[CELL 07e-03] MAML config: Î±={CFG['maml_config']['inner_lr']}, Î²={CFG['maml_config']['outer_lr']}, \"\n",
    "      f\"inner_steps={CFG['maml_config']['num_inner_steps']}, meta_batch={CFG['maml_config']['meta_batch_size']}\")\n",
    "print(f\"[CELL 07e-03] â­ WARM-START: Initializing from GRU baseline\")\n",
    "print(f\"[CELL 07e-03] GRU baseline: {GRU_BASELINE_PATH}\")\n",
    "\n",
    "cell_end(\"CELL 07e-03\", t0, out_dir=str(OUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-04] Load data\n",
      "[CELL 07e-04] start=2026-01-11T17:55:16\n",
      "[CELL 07e-04] Vocabulary: 343 courses\n",
      "[CELL 07e-04] Episodes train: 66,187 episodes (3,006 users)\n",
      "[CELL 07e-04] Episodes val:   340 episodes (340 users)\n",
      "[CELL 07e-04] Episodes test:  346 episodes (346 users)\n",
      "[CELL 07e-04] Pairs train: 212,923 pairs\n",
      "[CELL 07e-04] Pairs val:   24,698 pairs\n",
      "[CELL 07e-04] Pairs test:  26,608 pairs\n",
      "[CELL 07e-04] elapsed=1.09s\n",
      "[CELL 07e-04] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07e-04] Load data: episodes, pairs, vocab\n",
    "\n",
    "t0 = cell_start(\"CELL 07e-04\", \"Load data\")\n",
    "\n",
    "# Vocab\n",
    "course2id = read_json(Path(CFG[\"inputs\"][\"vocab\"]))\n",
    "id2course = {int(v): k for k, v in course2id.items()}\n",
    "n_items = len(course2id)\n",
    "print(f\"[CELL 07e-04] Vocabulary: {n_items} courses\")\n",
    "\n",
    "# Episodes\n",
    "episodes_train = pd.read_parquet(CFG[\"inputs\"][\"episodes_train\"])\n",
    "episodes_val = pd.read_parquet(CFG[\"inputs\"][\"episodes_val\"])\n",
    "episodes_test = pd.read_parquet(CFG[\"inputs\"][\"episodes_test\"])\n",
    "\n",
    "print(f\"[CELL 07e-04] Episodes train: {len(episodes_train):,} episodes ({episodes_train['user_id'].nunique():,} users)\")\n",
    "print(f\"[CELL 07e-04] Episodes val:   {len(episodes_val):,} episodes ({episodes_val['user_id'].nunique():,} users)\")\n",
    "print(f\"[CELL 07e-04] Episodes test:  {len(episodes_test):,} episodes ({episodes_test['user_id'].nunique():,} users)\")\n",
    "\n",
    "# Pairs\n",
    "pairs_train = pd.read_parquet(CFG[\"inputs\"][\"pairs_train\"])\n",
    "pairs_val = pd.read_parquet(CFG[\"inputs\"][\"pairs_val\"])\n",
    "pairs_test = pd.read_parquet(CFG[\"inputs\"][\"pairs_test\"])\n",
    "\n",
    "print(f\"[CELL 07e-04] Pairs train: {len(pairs_train):,} pairs\")\n",
    "print(f\"[CELL 07e-04] Pairs val:   {len(pairs_val):,} pairs\")\n",
    "print(f\"[CELL 07e-04] Pairs test:  {len(pairs_test):,} pairs\")\n",
    "\n",
    "cell_end(\"CELL 07e-04\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-05] Define evaluation metrics\n",
      "[CELL 07e-05] start=2026-01-11T17:55:17\n",
      "[CELL 07e-05] Metrics: accuracy@1, recall@5, recall@10, mrr\n",
      "[CELL 07e-05] elapsed=0.00s\n",
      "[CELL 07e-05] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07e-05] Evaluation metrics (same as Notebook 07)\n",
    "\n",
    "t0 = cell_start(\"CELL 07e-05\", \"Define evaluation metrics\")\n",
    "\n",
    "def compute_metrics(predictions: np.ndarray, labels: np.ndarray, k_values: List[int] = [5, 10]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute ranking metrics.\n",
    "    \n",
    "    Args:\n",
    "        predictions: (n_samples, n_items) score matrix\n",
    "        labels: (n_samples,) true item indices\n",
    "        k_values: list of k for Recall@k\n",
    "    \n",
    "    Returns:\n",
    "        dict with accuracy@1, recall@k, mrr\n",
    "    \"\"\"\n",
    "    n_samples = len(labels)\n",
    "    \n",
    "    # Get top-k predictions (indices)\n",
    "    max_k = max(k_values)\n",
    "    top_k_preds = np.argsort(-predictions, axis=1)[:, :max_k]  # descending order\n",
    "    \n",
    "    # Accuracy@1\n",
    "    top1_preds = top_k_preds[:, 0]\n",
    "    acc1 = (top1_preds == labels).mean()\n",
    "    \n",
    "    # Recall@k\n",
    "    recall_k = {}\n",
    "    for k in k_values:\n",
    "        hits = np.array([labels[i] in top_k_preds[i, :k] for i in range(n_samples)])\n",
    "        recall_k[f\"recall@{k}\"] = hits.mean()\n",
    "    \n",
    "    # MRR (Mean Reciprocal Rank)\n",
    "    ranks = []\n",
    "    for i in range(n_samples):\n",
    "        # Find rank of true label (1-indexed)\n",
    "        rank_idx = np.where(top_k_preds[i] == labels[i])[0]\n",
    "        if len(rank_idx) > 0:\n",
    "            ranks.append(1.0 / (rank_idx[0] + 1))  # reciprocal rank\n",
    "        else:\n",
    "            # Not in top-k, check full ranking\n",
    "            full_rank = np.where(np.argsort(-predictions[i]) == labels[i])[0][0]\n",
    "            ranks.append(1.0 / (full_rank + 1))\n",
    "    mrr = np.mean(ranks)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy@1\": float(acc1),\n",
    "        **{k: float(v) for k, v in recall_k.items()},\n",
    "        \"mrr\": float(mrr),\n",
    "    }\n",
    "\n",
    "print(\"[CELL 07e-05] Metrics: accuracy@1, recall@5, recall@10, mrr\")\n",
    "\n",
    "cell_end(\"CELL 07e-05\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-06] Define GRU model\n",
      "[CELL 07e-06] start=2026-01-11T17:55:17\n",
      "[CELL 07e-06] GRU model defined\n",
      "  - Embedding dim: 64\n",
      "  - Hidden dim: 128\n",
      "  - Num layers: 1\n",
      "[CELL 07e-06] elapsed=0.00s\n",
      "[CELL 07e-06] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07e-06] Define GRU model (exact same as Notebook 06 & 07)\n",
    "\n",
    "t0 = cell_start(\"CELL 07e-06\", \"Define GRU model\")\n",
    "\n",
    "class GRURecommender(nn.Module):\n",
    "    def __init__(self, n_items: int, embedding_dim: int, hidden_dim: int, num_layers: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.n_items = n_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_items, embedding_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim, n_items)\n",
    "    \n",
    "    def forward(self, seq: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            seq: (batch, max_len) padded sequences\n",
    "            lengths: (batch,) actual lengths\n",
    "        Returns:\n",
    "            logits: (batch, n_items)\n",
    "        \"\"\"\n",
    "        # Embed\n",
    "        emb = self.embedding(seq)  # (batch, max_len, embed_dim)\n",
    "        \n",
    "        # Pack for efficiency\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # GRU\n",
    "        _, hidden = self.gru(packed)  # hidden: (num_layers, batch, hidden_dim)\n",
    "        \n",
    "        # Use last layer hidden state\n",
    "        h = hidden[-1]  # (batch, hidden_dim)\n",
    "        \n",
    "        # Predict\n",
    "        logits = self.fc(h)  # (batch, n_items)\n",
    "        return logits\n",
    "\n",
    "print(\"[CELL 07e-06] GRU model defined\")\n",
    "print(f\"  - Embedding dim: {CFG['gru_config']['embedding_dim']}\")\n",
    "print(f\"  - Hidden dim: {CFG['gru_config']['hidden_dim']}\")\n",
    "print(f\"  - Num layers: {CFG['gru_config']['num_layers']}\")\n",
    "\n",
    "cell_end(\"CELL 07e-06\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-07] Initialize meta-model with warm-start\n",
      "[CELL 07e-07] start=2026-01-11T17:55:17\n",
      "[CELL 07e-07] Loading GRU baseline from: gru_global.pth\n",
      "[CELL 07e-07] Loaded directly from checkpoint\n",
      "[CELL 07e-07] GRU baseline Acc@1: 33.73% (from NB 06)\n",
      "\n",
      "[CELL 07e-07] âœ… WARM-START COMPLETE\n",
      "[CELL 07e-07] Meta-model initialized from strong GRU baseline\n",
      "[CELL 07e-07] Model parameters: 140,695\n",
      "[CELL 07e-07] Now will meta-train to make it more adaptable!\n",
      "[CELL 07e-07] elapsed=0.05s\n",
      "[CELL 07e-07] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07e-07] â­ Initialize meta-model with GRU baseline (WARM-START)\n",
    "\n",
    "t0 = cell_start(\"CELL 07e-07\", \"Initialize meta-model with warm-start\")\n",
    "\n",
    "# Create meta-model\n",
    "meta_model = GRURecommender(\n",
    "    n_items=n_items,\n",
    "    embedding_dim=CFG[\"gru_config\"][\"embedding_dim\"],\n",
    "    hidden_dim=CFG[\"gru_config\"][\"hidden_dim\"],\n",
    "    num_layers=CFG[\"gru_config\"][\"num_layers\"],\n",
    "    dropout=CFG[\"gru_config\"][\"dropout\"],\n",
    ").to(DEVICE)\n",
    "\n",
    "# â­ WARM-START: Load GRU baseline checkpoint\n",
    "if not GRU_BASELINE_PATH.exists():\n",
    "    raise FileNotFoundError(f\"GRU baseline not found: {GRU_BASELINE_PATH}\")\n",
    "\n",
    "print(f\"[CELL 07e-07] Loading GRU baseline from: {GRU_BASELINE_PATH.name}\")\n",
    "baseline_checkpoint = torch.load(GRU_BASELINE_PATH, map_location=DEVICE)\n",
    "\n",
    "# Load state dict\n",
    "if \"model_state_dict\" in baseline_checkpoint:\n",
    "    meta_model.load_state_dict(baseline_checkpoint[\"model_state_dict\"])\n",
    "    print(\"[CELL 07e-07] Loaded from 'model_state_dict' key\")\n",
    "else:\n",
    "    meta_model.load_state_dict(baseline_checkpoint)\n",
    "    print(\"[CELL 07e-07] Loaded directly from checkpoint\")\n",
    "\n",
    "# Verify baseline performance\n",
    "if \"metrics\" in baseline_checkpoint:\n",
    "    baseline_acc = baseline_checkpoint[\"metrics\"].get(\"test_accuracy@1\", \"N/A\")\n",
    "    print(f\"[CELL 07e-07] GRU baseline Acc@1: {baseline_acc}\")\n",
    "else:\n",
    "    print(\"[CELL 07e-07] GRU baseline Acc@1: 33.73% (from NB 06)\")\n",
    "\n",
    "print(f\"\\n[CELL 07e-07] âœ… WARM-START COMPLETE\")\n",
    "print(f\"[CELL 07e-07] Meta-model initialized from strong GRU baseline\")\n",
    "print(f\"[CELL 07e-07] Model parameters: {sum(p.numel() for p in meta_model.parameters()):,}\")\n",
    "print(f\"[CELL 07e-07] Now will meta-train to make it more adaptable!\")\n",
    "\n",
    "cell_end(\"CELL 07e-07\", t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¥ Key Difference from Notebook 07\n",
    "\n",
    "**Notebook 07 (Random Init)**:\n",
    "```python\n",
    "meta_model = GRURecommender(...)  # Random initialization\n",
    "# Meta-train from scratch â†’ 30.52% Acc@1\n",
    "```\n",
    "\n",
    "**Notebook 07e (Warm-Start)**:\n",
    "```python\n",
    "meta_model = GRURecommender(...)\n",
    "meta_model.load_state_dict(gru_baseline)  # â† Load GRU baseline (33.73%)\n",
    "# Meta-train from here â†’ Expected: 35-38% Acc@1 âœ…\n",
    "```\n",
    "\n",
    "**Why this works**:\n",
    "- GRU baseline already knows how to recommend courses (33.73%)\n",
    "- MAML meta-training refines it to adapt better to new users\n",
    "- Combines: Strong task initialization + Meta-learned adaptation\n",
    "\n",
    "**From here onwards, the rest of the notebook is IDENTICAL to Notebook 07:**\n",
    "- Same MAML training loop\n",
    "- Same meta-batch sampling\n",
    "- Same inner/outer loop updates\n",
    "- Same evaluation\n",
    "\n",
    "**Only difference**: We started from a better place!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-08] Define helper functions\n",
      "[CELL 07e-08] start=2026-01-11T17:55:17\n",
      "[CELL 07e-08] Helper functions defined\n",
      "[CELL 07e-08] elapsed=0.00s\n",
      "[CELL 07e-08] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07e-08] Helper functions (same as Notebook 07)\n",
    "\n",
    "t0 = cell_start(\"CELL 07e-08\", \"Define helper functions\")\n",
    "\n",
    "def get_episode_data(episode_row, pairs_df):\n",
    "    \"\"\"Extract support and query pairs for an episode.\"\"\"\n",
    "    support_pair_ids = episode_row[\"support_pair_ids\"]\n",
    "    query_pair_ids = episode_row[\"query_pair_ids\"]\n",
    "\n",
    "    support_pairs = pairs_df[pairs_df[\"pair_id\"].isin(support_pair_ids)].sort_values(\"label_ts_epoch\")\n",
    "    query_pairs = pairs_df[pairs_df[\"pair_id\"].isin(query_pair_ids)].sort_values(\"label_ts_epoch\")\n",
    "\n",
    "    return support_pairs, query_pairs\n",
    "\n",
    "def pairs_to_batch(pairs_df, max_len):\n",
    "    \"\"\"Convert pairs to batched tensors.\"\"\"\n",
    "    prefixes = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "\n",
    "    for _, row in pairs_df.iterrows():\n",
    "        prefix = row[\"prefix\"]\n",
    "        if len(prefix) > max_len:\n",
    "            prefix = prefix[-max_len:]\n",
    "        prefixes.append(prefix)\n",
    "        labels.append(row[\"label\"])\n",
    "        lengths.append(len(prefix))\n",
    "\n",
    "    # Pad sequences\n",
    "    max_l = max(lengths)\n",
    "    padded = []\n",
    "    for seq in prefixes:\n",
    "        padded.append(list(seq) + [0] * (max_l - len(seq)))\n",
    "\n",
    "    return (\n",
    "        torch.LongTensor(padded).to(DEVICE),\n",
    "        torch.LongTensor(labels).to(DEVICE),\n",
    "        torch.LongTensor(lengths).to(DEVICE),\n",
    "    )\n",
    "\n",
    "def functional_forward(seq, lengths, params, hidden_dim, n_items):\n",
    "    \"\"\"Functional forward pass using explicit parameters.\"\"\"\n",
    "    batch_size = seq.size(0)\n",
    "    \n",
    "    # 1. Embedding\n",
    "    emb = F.embedding(seq, params[\"embedding.weight\"], padding_idx=0)\n",
    "    \n",
    "    # 2. GRU (manual implementation)\n",
    "    h = torch.zeros(batch_size, hidden_dim, device=seq.device)\n",
    "    w_ih = params[\"gru.weight_ih_l0\"]\n",
    "    w_hh = params[\"gru.weight_hh_l0\"]\n",
    "    b_ih = params[\"gru.bias_ih_l0\"]\n",
    "    b_hh = params[\"gru.bias_hh_l0\"]\n",
    "    \n",
    "    for t in range(emb.size(1)):\n",
    "        x_t = emb[:, t, :]\n",
    "        gi = F.linear(x_t, w_ih, b_ih)\n",
    "        gh = F.linear(h, w_hh, b_hh)\n",
    "        i_r, i_z, i_n = gi.chunk(3, 1)\n",
    "        h_r, h_z, h_n = gh.chunk(3, 1)\n",
    "        \n",
    "        r = torch.sigmoid(i_r + h_r)\n",
    "        z = torch.sigmoid(i_z + h_z)\n",
    "        n = torch.tanh(i_n + r * h_n)\n",
    "        h_new = (1 - z) * n + z * h\n",
    "        \n",
    "        mask = (lengths > t).unsqueeze(1).float()\n",
    "        h = mask * h_new + (1 - mask) * h\n",
    "    \n",
    "    # 3. FC layer\n",
    "    logits = F.linear(h, params[\"fc.weight\"], params[\"fc.bias\"])\n",
    "    return logits\n",
    "\n",
    "print(\"[CELL 07e-08] Helper functions defined\")\n",
    "\n",
    "cell_end(\"CELL 07e-08\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-09] MAML meta-training\n",
      "[CELL 07e-09] start=2026-01-11T17:55:17\n",
      "[CELL 07e-09] Meta-model parameters: 140,695\n",
      "[CELL 07e-09] Using MAML (Second-Order)\n",
      "[CELL 07e-09] Meta-training config:\n",
      "  - Inner LR (Î±): 0.01\n",
      "  - Outer LR (Î²): 0.001\n",
      "  - Inner steps: 5\n",
      "  - Meta-batch size: 32\n",
      "  - Meta-iterations: 10,000\n",
      "\n",
      "[CELL 07e-09] Starting meta-training...\n",
      "[CELL 07e-09] Iter 100/10000: meta_loss=4.1887\n",
      "[CELL 07e-09] Iter 200/10000: meta_loss=3.7896\n",
      "[CELL 07e-09] Iter 300/10000: meta_loss=3.6735\n",
      "[CELL 07e-09] Iter 400/10000: meta_loss=3.8215\n",
      "[CELL 07e-09] Iter 500/10000: meta_loss=3.8139\n",
      "[CELL 07e-09] Evaluating on val set at iter 500...\n",
      "[CELL 07e-09] Val Acc@1: 0.2840, Recall@5: 0.5020, MRR: 0.3884\n",
      "[CELL 07e-09] Iter 600/10000: meta_loss=3.3961\n",
      "[CELL 07e-09] Iter 700/10000: meta_loss=3.2494\n",
      "[CELL 07e-09] Iter 800/10000: meta_loss=3.4691\n",
      "[CELL 07e-09] Iter 900/10000: meta_loss=3.0621\n",
      "[CELL 07e-09] Iter 1000/10000: meta_loss=3.5936\n",
      "[CELL 07e-09] Saved checkpoint: checkpoint_iter1000.pth\n",
      "[CELL 07e-09] Evaluating on val set at iter 1000...\n",
      "[CELL 07e-09] Val Acc@1: 0.3380, Recall@5: 0.5100, MRR: 0.4291\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07e-09] MAML meta-training loop (Functional FOMAML - proper implementation)\n",
    "\n",
    "t0 = cell_start(\"CELL 07e-09\", \"MAML meta-training\")\n",
    "\n",
    "# Initialize meta-model\n",
    "meta_model = GRURecommender(\n",
    "    n_items=n_items,\n",
    "    embedding_dim=CFG[\"gru_config\"][\"embedding_dim\"],\n",
    "    hidden_dim=CFG[\"gru_config\"][\"hidden_dim\"],\n",
    "    num_layers=CFG[\"gru_config\"][\"num_layers\"],\n",
    "    dropout=CFG[\"gru_config\"][\"dropout\"],\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"[CELL 07e-09] Meta-model parameters: {sum(p.numel() for p in meta_model.parameters()):,}\")\n",
    "\n",
    "# Meta-optimizer (outer loop)\n",
    "meta_optimizer = torch.optim.Adam(meta_model.parameters(), lr=CFG[\"maml_config\"][\"outer_lr\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# MAML hyperparameters\n",
    "inner_lr = CFG[\"maml_config\"][\"inner_lr\"]\n",
    "num_inner_steps = CFG[\"maml_config\"][\"num_inner_steps\"]\n",
    "meta_batch_size = CFG[\"maml_config\"][\"meta_batch_size\"]\n",
    "num_meta_iterations = CFG[\"maml_config\"][\"num_meta_iterations\"]\n",
    "max_seq_len = CFG[\"gru_config\"][\"max_seq_len\"]\n",
    "use_second_order = CFG[\"maml_config\"].get(\"use_second_order\", False)  # Default to FOMAML\n",
    "\n",
    "print(f\"[CELL 07e-09] Using {'MAML (Second-Order)' if use_second_order else 'First-Order MAML (FOMAML)'}\")\n",
    "print(f\"[CELL 07e-09] Meta-training config:\")\n",
    "print(f\"  - Inner LR (Î±): {inner_lr}\")\n",
    "print(f\"  - Outer LR (Î²): {CFG['maml_config']['outer_lr']}\")\n",
    "print(f\"  - Inner steps: {num_inner_steps}\")\n",
    "print(f\"  - Meta-batch size: {meta_batch_size}\")\n",
    "print(f\"  - Meta-iterations: {num_meta_iterations:,}\")\n",
    "\n",
    "def get_episode_data(episode_row, pairs_df):\n",
    "    \"\"\"Extract support and query pairs for an episode.\"\"\"\n",
    "    support_pair_ids = episode_row[\"support_pair_ids\"]\n",
    "    query_pair_ids = episode_row[\"query_pair_ids\"]\n",
    "\n",
    "    support_pairs = pairs_df[pairs_df[\"pair_id\"].isin(support_pair_ids)].sort_values(\"label_ts_epoch\")\n",
    "    query_pairs = pairs_df[pairs_df[\"pair_id\"].isin(query_pair_ids)].sort_values(\"label_ts_epoch\")\n",
    "\n",
    "    return support_pairs, query_pairs\n",
    "\n",
    "def pairs_to_batch(pairs_df, max_len):\n",
    "    \"\"\"Convert pairs to batched tensors.\"\"\"\n",
    "    prefixes = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "\n",
    "    for _, row in pairs_df.iterrows():\n",
    "        prefix = row[\"prefix\"]\n",
    "        if len(prefix) > max_len:\n",
    "            prefix = prefix[-max_len:]\n",
    "        prefixes.append(prefix)\n",
    "        labels.append(row[\"label\"])\n",
    "        lengths.append(len(prefix))\n",
    "\n",
    "    # Pad sequences\n",
    "    max_l = max(lengths)\n",
    "    padded = []\n",
    "    for seq in prefixes:\n",
    "        padded.append(list(seq) + [0] * (max_l - len(seq)))\n",
    "\n",
    "    return (\n",
    "        torch.LongTensor(padded).to(DEVICE),\n",
    "        torch.LongTensor(labels).to(DEVICE),\n",
    "        torch.LongTensor(lengths).to(DEVICE),\n",
    "    )\n",
    "\n",
    "# Functional forward pass for GRU (avoids in-place operations)\n",
    "def functional_forward(seq, lengths, params, hidden_dim, n_items):\n",
    "    \"\"\"\n",
    "    Functional forward pass using explicit parameters.\n",
    "    Implements: Embedding -> GRU -> FC\n",
    "    \"\"\"\n",
    "    batch_size = seq.size(0)\n",
    "    \n",
    "    # 1. Embedding\n",
    "    emb = F.embedding(seq, params['embedding.weight'], padding_idx=0)\n",
    "    \n",
    "    # 2. GRU (manual implementation for num_layers=1, batch_first=True)\n",
    "    h = torch.zeros(batch_size, hidden_dim, device=seq.device)\n",
    "    \n",
    "    # GRU parameters\n",
    "    w_ih = params['gru.weight_ih_l0']\n",
    "    w_hh = params['gru.weight_hh_l0']\n",
    "    b_ih = params['gru.bias_ih_l0']\n",
    "    b_hh = params['gru.bias_hh_l0']\n",
    "    \n",
    "    # Process sequence\n",
    "    for t in range(emb.size(1)):\n",
    "        x_t = emb[:, t, :]\n",
    "        \n",
    "        # GRU gates\n",
    "        gi = F.linear(x_t, w_ih, b_ih)\n",
    "        gh = F.linear(h, w_hh, b_hh)\n",
    "        i_r, i_z, i_n = gi.chunk(3, 1)\n",
    "        h_r, h_z, h_n = gh.chunk(3, 1)\n",
    "        \n",
    "        r = torch.sigmoid(i_r + h_r)\n",
    "        z = torch.sigmoid(i_z + h_z)\n",
    "        n = torch.tanh(i_n + r * h_n)\n",
    "        h_new = (1 - z) * n + z * h\n",
    "        \n",
    "        # Mask for actual sequence lengths\n",
    "        mask = (lengths > t).unsqueeze(1).float()\n",
    "        h = mask * h_new + (1 - mask) * h\n",
    "    \n",
    "    # 3. FC layer\n",
    "    logits = F.linear(h, params['fc.weight'], params['fc.bias'])\n",
    "    \n",
    "    return logits\n",
    "\n",
    "# Model config for functional forward\n",
    "hidden_dim = CFG[\"gru_config\"][\"hidden_dim\"]\n",
    "\n",
    "# Training tracking\n",
    "training_history = {\n",
    "    \"meta_iterations\": [],\n",
    "    \"meta_train_loss\": [],\n",
    "    \"val_accuracy\": [],\n",
    "    \"val_iterations\": [],\n",
    "}\n",
    "\n",
    "print(f\"\\n[CELL 07e-09] Starting meta-training...\")\n",
    "\n",
    "# Sample episodes for meta-training\n",
    "train_users = episodes_train[\"user_id\"].unique()\n",
    "\n",
    "for meta_iter in range(num_meta_iterations):\n",
    "    meta_model.train()\n",
    "    meta_optimizer.zero_grad()\n",
    "\n",
    "    # Sample meta-batch of tasks\n",
    "    sampled_users = np.random.choice(train_users, size=min(meta_batch_size, len(train_users)), replace=False)\n",
    "\n",
    "    meta_loss_total = 0.0\n",
    "    valid_tasks = 0\n",
    "\n",
    "    for user_id in sampled_users:\n",
    "        # Sample one episode for this user\n",
    "        user_episodes = episodes_train[episodes_train[\"user_id\"] == user_id]\n",
    "        if len(user_episodes) == 0:\n",
    "            continue\n",
    "\n",
    "        episode = user_episodes.sample(n=1).iloc[0]\n",
    "\n",
    "        # Get support and query sets\n",
    "        support_pairs, query_pairs = get_episode_data(episode, pairs_train)\n",
    "\n",
    "        if len(support_pairs) == 0 or len(query_pairs) == 0:\n",
    "            continue\n",
    "\n",
    "        support_seq, support_labels, support_lengths = pairs_to_batch(support_pairs, max_seq_len)\n",
    "        query_seq, query_labels, query_lengths = pairs_to_batch(query_pairs, max_seq_len)\n",
    "\n",
    "        # ===== INNER LOOP: Adapt parameters using functional approach =====\n",
    "        # Clone initial meta-parameters\n",
    "        fast_weights = OrderedDict()\n",
    "        for name, param in meta_model.named_parameters():\n",
    "            fast_weights[name] = param.clone().requires_grad_()\n",
    "\n",
    "        # Adapt on support set\n",
    "        for _ in range(num_inner_steps):\n",
    "            # Functional forward with current fast_weights\n",
    "            support_logits = functional_forward(\n",
    "                support_seq, support_lengths, fast_weights, hidden_dim, n_items\n",
    "            )\n",
    "            support_loss = criterion(support_logits, support_labels)\n",
    "\n",
    "            # Compute gradients w.r.t. fast_weights\n",
    "            grads = torch.autograd.grad(\n",
    "                support_loss,\n",
    "                fast_weights.values(),\n",
    "                create_graph=use_second_order  # FOMAML: False, MAML: True\n",
    "            )\n",
    "\n",
    "            # Update fast_weights (creates new tensors, no in-place ops)\n",
    "            fast_weights = OrderedDict(\n",
    "                (name, param - inner_lr * grad)\n",
    "                for ((name, param), grad) in zip(fast_weights.items(), grads)\n",
    "            )\n",
    "\n",
    "        # ===== OUTER LOOP: Compute query loss with adapted parameters =====\n",
    "        query_logits = functional_forward(\n",
    "            query_seq, query_lengths, fast_weights, hidden_dim, n_items\n",
    "        )\n",
    "        query_loss = criterion(query_logits, query_labels)\n",
    "\n",
    "        # Accumulate for meta-update\n",
    "        meta_loss_total = meta_loss_total + query_loss\n",
    "        valid_tasks += 1\n",
    "\n",
    "    if valid_tasks == 0:\n",
    "        continue\n",
    "\n",
    "    # ===== META-UPDATE =====\n",
    "    meta_loss = meta_loss_total / valid_tasks\n",
    "    meta_loss.backward()\n",
    "\n",
    "    # Gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(meta_model.parameters(), max_norm=10.0)\n",
    "\n",
    "    meta_optimizer.step()\n",
    "\n",
    "    # Logging\n",
    "    training_history[\"meta_iterations\"].append(meta_iter)\n",
    "    training_history[\"meta_train_loss\"].append(meta_loss.item())\n",
    "\n",
    "    if (meta_iter + 1) % 100 == 0:\n",
    "        print(f\"[CELL 07e-09] Iter {meta_iter+1}/{num_meta_iterations}: meta_loss={meta_loss.item():.4f}\")\n",
    "\n",
    "    # Checkpointing\n",
    "    if (meta_iter + 1) % CFG[\"maml_config\"][\"checkpoint_interval\"] == 0:\n",
    "        checkpoint_path = CHECKPOINTS_DIR / f\"checkpoint_iter{meta_iter+1}.pth\"\n",
    "        torch.save({\n",
    "            \"meta_iter\": meta_iter + 1,\n",
    "            \"model_state_dict\": meta_model.state_dict(),\n",
    "            \"optimizer_state_dict\": meta_optimizer.state_dict(),\n",
    "            \"config\": CFG,\n",
    "            \"training_history\": training_history,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"[CELL 07e-09] Saved checkpoint: {checkpoint_path.name}\")\n",
    "\n",
    "    # Validation (simpler non-functional approach for validation only)\n",
    "    if (meta_iter + 1) % CFG[\"maml_config\"][\"eval_interval\"] == 0:\n",
    "        print(f\"[CELL 07e-09] Evaluating on val set at iter {meta_iter+1}...\")\n",
    "        meta_model.eval()\n",
    "\n",
    "        val_predictions = []\n",
    "        val_labels = []\n",
    "\n",
    "        for _, episode in episodes_val.head(50).iterrows():\n",
    "            support_pairs, query_pairs = get_episode_data(episode, pairs_val)\n",
    "\n",
    "            if len(support_pairs) == 0 or len(query_pairs) == 0:\n",
    "                continue\n",
    "\n",
    "            support_seq, support_labels_val, support_lengths = pairs_to_batch(support_pairs, max_seq_len)\n",
    "            query_seq, query_labels_val, query_lengths = pairs_to_batch(query_pairs, max_seq_len)\n",
    "\n",
    "            # Save original params\n",
    "            original_params = OrderedDict()\n",
    "            for name, param in meta_model.named_parameters():\n",
    "                original_params[name] = param.data.clone()\n",
    "\n",
    "            # Adapt on support using standard approach (no gradients needed for validation)\n",
    "            with torch.enable_grad():\n",
    "                # Clone parameters for adaptation\n",
    "                fast_weights_val = OrderedDict()\n",
    "                for name, param in meta_model.named_parameters():\n",
    "                    fast_weights_val[name] = param.clone().requires_grad_()\n",
    "\n",
    "                # Inner loop adaptation\n",
    "                for _ in range(num_inner_steps):\n",
    "                    support_logits_val = functional_forward(\n",
    "                        support_seq, support_lengths, fast_weights_val, hidden_dim, n_items\n",
    "                    )\n",
    "                    support_loss_val = criterion(support_logits_val, support_labels_val)\n",
    "\n",
    "                    grads_val = torch.autograd.grad(\n",
    "                        support_loss_val,\n",
    "                        fast_weights_val.values(),\n",
    "                        create_graph=False\n",
    "                    )\n",
    "\n",
    "                    fast_weights_val = OrderedDict(\n",
    "                        (name, param - inner_lr * grad)\n",
    "                        for ((name, param), grad) in zip(fast_weights_val.items(), grads_val)\n",
    "                    )\n",
    "\n",
    "            # Evaluate on query (no gradients)\n",
    "            with torch.no_grad():\n",
    "                query_logits_val = functional_forward(\n",
    "                    query_seq, query_lengths, fast_weights_val, hidden_dim, n_items\n",
    "                )\n",
    "                query_probs = torch.softmax(query_logits_val, dim=-1).cpu().numpy()\n",
    "\n",
    "                val_predictions.append(query_probs)\n",
    "                val_labels.extend(query_labels_val.cpu().numpy())\n",
    "\n",
    "            # Restore original params\n",
    "            with torch.no_grad():\n",
    "                for name, param in meta_model.named_parameters():\n",
    "                    param.data.copy_(original_params[name])\n",
    "\n",
    "        if len(val_predictions) > 0:\n",
    "            val_predictions = np.vstack(val_predictions)\n",
    "            val_labels = np.array(val_labels)\n",
    "            val_metrics = compute_metrics(val_predictions, val_labels)\n",
    "\n",
    "            training_history[\"val_accuracy\"].append(val_metrics[\"accuracy@1\"])\n",
    "            training_history[\"val_iterations\"].append(meta_iter + 1)\n",
    "\n",
    "            print(f\"[CELL 07e-09] Val Acc@1: {val_metrics['accuracy@1']:.4f}, \"\n",
    "                  f\"Recall@5: {val_metrics['recall@5']:.4f}, MRR: {val_metrics['mrr']:.4f}\")\n",
    "\n",
    "# Save final model\n",
    "final_model_path = MODELS_DIR / f\"maml_warmstart_gru_K{K}.pth\"\n",
    "torch.save({\n",
    "    \"model_state_dict\": meta_model.state_dict(),\n",
    "    \"config\": CFG,\n",
    "    \"training_history\": training_history,\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"\\n[CELL 07e-09] Saved final meta-model: {final_model_path.name}\")\n",
    "print(f\"[CELL 07e-09] Total training time: {time.time()-t0:.1f}s\")\n",
    "\n",
    "cell_end(\"CELL 07e-09\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114cbe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 07e-10] Meta-testing: Zero-shot (K=0) - no adaptation\n",
    "\n",
    "t0 = cell_start(\"CELL 07e-10\", \"Zero-shot evaluation (K=0)\")\n",
    "\n",
    "print(\"[CELL 07e-10] Evaluating meta-learned model WITHOUT adaptation (zero-shot)...\")\n",
    "\n",
    "meta_model.eval()\n",
    "zeroshot_predictions = []\n",
    "zeroshot_labels = []\n",
    "\n",
    "with torch.no_grad():  # Pure inference, no gradients needed\n",
    "    for _, episode in episodes_test.iterrows():\n",
    "        support_pairs, query_pairs = get_episode_data(episode, pairs_test)\n",
    "\n",
    "        if len(query_pairs) == 0:\n",
    "            continue\n",
    "\n",
    "        # Only use query set (no support set adaptation)\n",
    "        query_seq, query_labels_test, query_lengths = pairs_to_batch(query_pairs, max_seq_len)\n",
    "\n",
    "        # Use original meta-learned model (no adaptation)\n",
    "        query_logits = meta_model(query_seq, query_lengths)\n",
    "        query_probs = torch.softmax(query_logits, dim=-1).cpu().numpy()\n",
    "\n",
    "        zeroshot_predictions.append(query_probs)\n",
    "        zeroshot_labels.extend(query_labels_test.cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "if len(zeroshot_predictions) > 0:\n",
    "    zeroshot_predictions = np.vstack(zeroshot_predictions)\n",
    "    zeroshot_labels = np.array(zeroshot_labels)\n",
    "    zeroshot_metrics = compute_metrics(zeroshot_predictions, zeroshot_labels)\n",
    "\n",
    "    print(f\"\\n[CELL 07e-10] Zero-shot Results (No Adaptation):\")\n",
    "    print(f\"  Accuracy@1:  {zeroshot_metrics['accuracy@1']:.4f}\")\n",
    "    print(f\"  Recall@5:    {zeroshot_metrics['recall@5']:.4f}\")\n",
    "    print(f\"  Recall@10:   {zeroshot_metrics['recall@10']:.4f}\")\n",
    "    print(f\"  MRR:         {zeroshot_metrics['mrr']:.4f}\")\n",
    "else:\n",
    "    print(\"[CELL 07e-10] WARNING: No predictions generated\")\n",
    "    zeroshot_metrics = {}\n",
    "\n",
    "cell_end(\"CELL 07e-10\", t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 07e-11] Meta-testing: Few-shot K=5 (with adaptation using functional forward)\n",
    "\n",
    "t0 = cell_start(\"CELL 07e-11\", \"Few-shot evaluation (K=5)\")\n",
    "\n",
    "print(\"[CELL 07e-11] Evaluating meta-learned model WITH adaptation (few-shot K=5)...\")\n",
    "\n",
    "meta_model.eval()\n",
    "fewshot_predictions = []\n",
    "fewshot_labels = []\n",
    "\n",
    "for _, episode in episodes_test.iterrows():\n",
    "    support_pairs, query_pairs = get_episode_data(episode, pairs_test)\n",
    "    \n",
    "    if len(support_pairs) == 0 or len(query_pairs) == 0:\n",
    "        continue\n",
    "    \n",
    "    support_seq, support_labels_test, support_lengths = pairs_to_batch(support_pairs, max_seq_len)\n",
    "    query_seq, query_labels_test, query_lengths = pairs_to_batch(query_pairs, max_seq_len)\n",
    "    \n",
    "    # Adapt using functional forward (consistent with training)\n",
    "    with torch.enable_grad():\n",
    "        # Clone parameters for adaptation\n",
    "        fast_weights_test = OrderedDict()\n",
    "        for name, param in meta_model.named_parameters():\n",
    "            fast_weights_test[name] = param.clone().requires_grad_()\n",
    "        \n",
    "        # Inner loop adaptation\n",
    "        for _ in range(num_inner_steps):\n",
    "            support_logits_test = functional_forward(\n",
    "                support_seq, support_lengths, fast_weights_test, hidden_dim, n_items\n",
    "            )\n",
    "            support_loss_test = criterion(support_logits_test, support_labels_test)\n",
    "            \n",
    "            grads_test = torch.autograd.grad(\n",
    "                support_loss_test,\n",
    "                fast_weights_test.values(),\n",
    "                create_graph=False  # No second-order needed for testing\n",
    "            )\n",
    "            \n",
    "            fast_weights_test = OrderedDict(\n",
    "                (name, param - inner_lr * grad)\n",
    "                for ((name, param), grad) in zip(fast_weights_test.items(), grads_test)\n",
    "            )\n",
    "    \n",
    "    # Evaluate on query (no gradients)\n",
    "    with torch.no_grad():\n",
    "        query_logits_test = functional_forward(\n",
    "            query_seq, query_lengths, fast_weights_test, hidden_dim, n_items\n",
    "        )\n",
    "        probs = torch.softmax(query_logits_test, dim=-1).cpu().numpy()\n",
    "        \n",
    "        fewshot_predictions.append(probs)\n",
    "        fewshot_labels.extend(query_labels_test.cpu().numpy())\n",
    "\n",
    "fewshot_predictions = np.vstack(fewshot_predictions)\n",
    "fewshot_labels = np.array(fewshot_labels)\n",
    "\n",
    "fewshot_metrics = compute_metrics(fewshot_predictions, fewshot_labels)\n",
    "\n",
    "print(f\"\\n[CELL 07e-11] Few-shot Results (K=5 adaptation):\")\n",
    "print(f\"  - Accuracy@1:  {fewshot_metrics['accuracy@1']:.4f}\")\n",
    "print(f\"  - Recall@5:    {fewshot_metrics['recall@5']:.4f}\")\n",
    "print(f\"  - Recall@10:   {fewshot_metrics['recall@10']:.4f}\")\n",
    "print(f\"  - MRR:         {fewshot_metrics['mrr']:.4f}\")\n",
    "\n",
    "cell_end(\"CELL 07e-11\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 07e-12] Ablation Study 1: Support set size (K=1,3,5,10) - functional forward\n",
    "\n",
    "t0 = cell_start(\"CELL 07e-12\", \"Ablation: support set size\")\n",
    "\n",
    "print(\"[CELL 07e-12] Ablation Study: Varying support set size K...\")\n",
    "\n",
    "support_sizes = CFG[\"ablation_configs\"][\"support_set_sizes\"]\n",
    "ablation_support_results = {}\n",
    "\n",
    "meta_model.eval()\n",
    "\n",
    "for K_test in support_sizes:\n",
    "    print(f\"\\n[CELL 07e-12] Testing with K={K_test}...\")\n",
    "    \n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, episode in episodes_test.iterrows():\n",
    "        support_pairs, query_pairs = get_episode_data(episode, pairs_test)\n",
    "        \n",
    "        if len(support_pairs) < K_test or len(query_pairs) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Use only K_test support pairs\n",
    "        support_pairs_k = support_pairs.head(K_test)\n",
    "        \n",
    "        support_seq, support_labels_abl, support_lengths = pairs_to_batch(support_pairs_k, max_seq_len)\n",
    "        query_seq, query_labels_abl, query_lengths = pairs_to_batch(query_pairs, max_seq_len)\n",
    "        \n",
    "        # Adapt using functional forward\n",
    "        with torch.enable_grad():\n",
    "            fast_weights_abl = OrderedDict()\n",
    "            for name, param in meta_model.named_parameters():\n",
    "                fast_weights_abl[name] = param.clone().requires_grad_()\n",
    "            \n",
    "            for _ in range(num_inner_steps):\n",
    "                support_logits_abl = functional_forward(\n",
    "                    support_seq, support_lengths, fast_weights_abl, hidden_dim, n_items\n",
    "                )\n",
    "                support_loss_abl = criterion(support_logits_abl, support_labels_abl)\n",
    "                \n",
    "                grads_abl = torch.autograd.grad(\n",
    "                    support_loss_abl,\n",
    "                    fast_weights_abl.values(),\n",
    "                    create_graph=False\n",
    "                )\n",
    "                \n",
    "                fast_weights_abl = OrderedDict(\n",
    "                    (name, param - inner_lr * grad)\n",
    "                    for ((name, param), grad) in zip(fast_weights_abl.items(), grads_abl)\n",
    "                )\n",
    "        \n",
    "        # Evaluate on query\n",
    "        with torch.no_grad():\n",
    "            query_logits_abl = functional_forward(\n",
    "                query_seq, query_lengths, fast_weights_abl, hidden_dim, n_items\n",
    "            )\n",
    "            probs = torch.softmax(query_logits_abl, dim=-1).cpu().numpy()\n",
    "            \n",
    "            predictions.append(probs)\n",
    "            labels.extend(query_labels_abl.cpu().numpy())\n",
    "    \n",
    "    if len(predictions) > 0:\n",
    "        predictions = np.vstack(predictions)\n",
    "        labels = np.array(labels)\n",
    "        metrics = compute_metrics(predictions, labels)\n",
    "        ablation_support_results[K_test] = metrics\n",
    "        \n",
    "        print(f\"[CELL 07e-12] K={K_test}: Acc@1={metrics['accuracy@1']:.4f}, \"\n",
    "              f\"Recall@5={metrics['recall@5']:.4f}, MRR={metrics['mrr']:.4f}\")\n",
    "\n",
    "print(f\"\\n[CELL 07e-12] Ablation complete: tested K âˆˆ {support_sizes}\")\n",
    "\n",
    "cell_end(\"CELL 07e-12\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 07e-13] Ablation Study 2: Adaptation steps (1,3,5,10) - functional forward\n",
    "\n",
    "t0 = cell_start(\"CELL 07e-13\", \"Ablation: adaptation steps\")\n",
    "\n",
    "print(\"[CELL 07e-13] Ablation Study: Varying adaptation steps...\")\n",
    "\n",
    "adaptation_steps = CFG[\"ablation_configs\"][\"adaptation_steps\"]\n",
    "ablation_steps_results = {}\n",
    "\n",
    "meta_model.eval()\n",
    "\n",
    "for num_steps in adaptation_steps:\n",
    "    print(f\"\\n[CELL 07e-13] Testing with {num_steps} adaptation steps...\")\n",
    "    \n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, episode in episodes_test.iterrows():\n",
    "        support_pairs, query_pairs = get_episode_data(episode, pairs_test)\n",
    "        \n",
    "        if len(support_pairs) == 0 or len(query_pairs) == 0:\n",
    "            continue\n",
    "        \n",
    "        support_seq, support_labels_steps, support_lengths = pairs_to_batch(support_pairs, max_seq_len)\n",
    "        query_seq, query_labels_steps, query_lengths = pairs_to_batch(query_pairs, max_seq_len)\n",
    "        \n",
    "        # Adapt using functional forward with varying steps\n",
    "        with torch.enable_grad():\n",
    "            fast_weights_steps = OrderedDict()\n",
    "            for name, param in meta_model.named_parameters():\n",
    "                fast_weights_steps[name] = param.clone().requires_grad_()\n",
    "            \n",
    "            for _ in range(num_steps):  # Use num_steps instead of num_inner_steps\n",
    "                support_logits_steps = functional_forward(\n",
    "                    support_seq, support_lengths, fast_weights_steps, hidden_dim, n_items\n",
    "                )\n",
    "                support_loss_steps = criterion(support_logits_steps, support_labels_steps)\n",
    "                \n",
    "                grads_steps = torch.autograd.grad(\n",
    "                    support_loss_steps,\n",
    "                    fast_weights_steps.values(),\n",
    "                    create_graph=False\n",
    "                )\n",
    "                \n",
    "                fast_weights_steps = OrderedDict(\n",
    "                    (name, param - inner_lr * grad)\n",
    "                    for ((name, param), grad) in zip(fast_weights_steps.items(), grads_steps)\n",
    "                )\n",
    "        \n",
    "        # Evaluate on query\n",
    "        with torch.no_grad():\n",
    "            query_logits_steps = functional_forward(\n",
    "                query_seq, query_lengths, fast_weights_steps, hidden_dim, n_items\n",
    "            )\n",
    "            probs = torch.softmax(query_logits_steps, dim=-1).cpu().numpy()\n",
    "            \n",
    "            predictions.append(probs)\n",
    "            labels.extend(query_labels_steps.cpu().numpy())\n",
    "    \n",
    "    if len(predictions) > 0:\n",
    "        predictions = np.vstack(predictions)\n",
    "        labels = np.array(labels)\n",
    "        metrics = compute_metrics(predictions, labels)\n",
    "        ablation_steps_results[num_steps] = metrics\n",
    "        \n",
    "        print(f\"[CELL 07e-13] Steps={num_steps}: Acc@1={metrics['accuracy@1']:.4f}, \"\n",
    "              f\"Recall@5={metrics['recall@5']:.4f}, MRR={metrics['mrr']:.4f}\")\n",
    "\n",
    "print(f\"\\n[CELL 07e-13] Ablation complete: tested adaptation steps âˆˆ {adaptation_steps}\")\n",
    "\n",
    "cell_end(\"CELL 07e-13\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 07e-14] Analysis: Parameter update visualization - functional forward\n",
    "\n",
    "t0 = cell_start(\"CELL 07e-14\", \"Parameter update analysis\")\n",
    "\n",
    "print(\"[CELL 07e-14] Analyzing parameter updates during adaptation...\")\n",
    "\n",
    "# Select one test episode for analysis\n",
    "sample_episode = episodes_test.iloc[0]\n",
    "support_pairs, query_pairs = get_episode_data(sample_episode, pairs_test)\n",
    "\n",
    "if len(support_pairs) > 0:\n",
    "    support_seq, support_labels_viz, support_lengths = pairs_to_batch(support_pairs, max_seq_len)\n",
    "    \n",
    "    # NOTE: Do NOT call meta_model.eval() here - we need gradients for functional_forward\n",
    "    # The functional forward approach doesn't use the model's forward(), so eval mode doesn't matter\n",
    "    \n",
    "    # Get original parameters (before adaptation)\n",
    "    param_norms_before = {}\n",
    "    for name, param in meta_model.named_parameters():\n",
    "        param_norms_before[name] = param.data.norm().item()\n",
    "    \n",
    "    # Adapt using functional forward\n",
    "    with torch.enable_grad():\n",
    "        fast_weights_viz = OrderedDict()\n",
    "        for name, param in meta_model.named_parameters():\n",
    "            fast_weights_viz[name] = param.clone().requires_grad_()\n",
    "        \n",
    "        for _ in range(num_inner_steps):\n",
    "            support_logits_viz = functional_forward(\n",
    "                support_seq, support_lengths, fast_weights_viz, hidden_dim, n_items\n",
    "            )\n",
    "            support_loss_viz = criterion(support_logits_viz, support_labels_viz)\n",
    "            \n",
    "            grads_viz = torch.autograd.grad(\n",
    "                support_loss_viz,\n",
    "                fast_weights_viz.values(),\n",
    "                create_graph=False\n",
    "            )\n",
    "            \n",
    "            fast_weights_viz = OrderedDict(\n",
    "                (name, param - inner_lr * grad)\n",
    "                for ((name, param), grad) in zip(fast_weights_viz.items(), grads_viz)\n",
    "            )\n",
    "    \n",
    "    # Compute parameter changes\n",
    "    param_norms_after = {}\n",
    "    param_changes = {}\n",
    "    \n",
    "    for name in fast_weights_viz.keys():\n",
    "        adapted_norm = fast_weights_viz[name].data.norm().item()\n",
    "        original_norm = param_norms_before[name]\n",
    "        change = adapted_norm - original_norm\n",
    "        \n",
    "        param_norms_after[name] = adapted_norm\n",
    "        param_changes[name] = {\n",
    "            \"before\": original_norm,\n",
    "            \"after\": adapted_norm,\n",
    "            \"change\": change,\n",
    "            \"change_pct\": (change / original_norm * 100) if original_norm > 0 else 0,\n",
    "        }\n",
    "    \n",
    "    print(f\"\\n[CELL 07e-14] Parameter changes after {num_inner_steps} adaptation steps:\")\n",
    "    print(f\"{'Parameter':<30} {'Before':>12} {'After':>12} {'Change':>12} {'Change %':>10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for name, stats in list(param_changes.items())[:10]:  # Show first 10\n",
    "        print(f\"{name:<30} {stats['before']:>12.4f} {stats['after']:>12.4f} \"\n",
    "              f\"{stats['change']:>12.4f} {stats['change_pct']:>9.2f}%\")\n",
    "    \n",
    "    # Visualization: parameter change distribution\n",
    "    VIZ_DIR = OUT_DIR / \"visualizations\"\n",
    "    VIZ_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    change_pcts = [stats[\"change_pct\"] for stats in param_changes.values()]\n",
    "    ax.hist(change_pcts, bins=30, color='#3498db', alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(0, color='red', linestyle='--', linewidth=2, label='No change')\n",
    "    ax.set_xlabel('Parameter Change (%)', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Number of Parameters', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('Parameter Change Distribution After Adaptation (MAML)', fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='upper right', fontsize=10)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(VIZ_DIR / \"param_change_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\n[CELL 07e-14] Saved: param_change_distribution.png\")\n",
    "\n",
    "cell_end(\"CELL 07e-14\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 07e-14] Analysis: Parameter update visualization\n",
    "\n",
    "t0 = cell_start(\"CELL 07e-14\", \"Parameter update analysis\")\n",
    "\n",
    "print(\"[CELL 07e-14] Analyzing parameter updates during adaptation...\")\n",
    "\n",
    "# Select one test episode for analysis\n",
    "sample_episode = episodes_test.iloc[0]\n",
    "support_pairs, query_pairs = get_episode_data(sample_episode, pairs_test)\n",
    "\n",
    "if len(support_pairs) > 0:\n",
    "    support_seq, support_labels, support_lengths = pairs_to_batch(support_pairs, max_seq_len)\n",
    "    \n",
    "    # Track parameter changes during inner loop\n",
    "    param_norms_before = {}\n",
    "    param_norms_after = {}\n",
    "    param_changes = {}\n",
    "    \n",
    "    meta_model.eval()\n",
    "    \n",
    "    # Before adaptation\n",
    "    for name, param in meta_model.named_parameters():\n",
    "        param_norms_before[name] = param.data.norm().item()\n",
    "    \n",
    "    # Save original params\n",
    "    original_params = [param.clone() for param in meta_model.parameters()]\n",
    "    \n",
    "    # Adapt\n",
    "    inner_optimizer = torch.optim.SGD(meta_model.parameters(), lr=inner_lr)\n",
    "    for inner_step in range(num_inner_steps):\n",
    "        inner_optimizer.zero_grad()\n",
    "        support_logits = meta_model(support_seq, support_lengths)\n",
    "        support_loss = criterion(support_logits, support_labels)\n",
    "        support_loss.backward()\n",
    "        inner_optimizer.step()\n",
    "    \n",
    "    # After adaptation - collect adapted parameters\n",
    "    adapted_params = {}\n",
    "    for name, param in meta_model.named_parameters():\n",
    "        adapted_params[name] = param.clone().detach()\n",
    "        param_norms_after[name] = param.data.norm().item()\n",
    "        original_norm = param_norms_before[name]\n",
    "        change = param_norms_after[name] - original_norm\n",
    "        param_changes[name] = {\n",
    "            \"before\": original_norm,\n",
    "            \"after\": param_norms_after[name],\n",
    "            \"change\": change,\n",
    "            \"change_pct\": (change / original_norm * 100) if original_norm > 0 else 0,\n",
    "        }\n",
    "    \n",
    "    # Restore params\n",
    "    for param, orig_param in zip(meta_model.parameters(), original_params):\n",
    "        param.copy_(orig_param)\n",
    "    \n",
    "    print(f\"\\n[CELL 07e-14] Parameter changes after {num_inner_steps} adaptation steps:\")\n",
    "    print(f\"{'Parameter':<30} {'Before':>12} {'After':>12} {'Change':>12} {'Change %':>10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for name, stats in list(param_changes.items())[:10]:  # Show first 10\n",
    "        print(f\"{name:<30} {stats['before']:>12.4f} {stats['after']:>12.4f} \"\n",
    "              f\"{stats['change']:>12.4f} {stats['change_pct']:>9.2f}%\")\n",
    "    \n",
    "    # Visualization: parameter change distribution\n",
    "    VIZ_DIR = OUT_DIR / \"visualizations\"\n",
    "    VIZ_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    change_pcts = [stats[\"change_pct\"] for stats in param_changes.values()]\n",
    "    ax.hist(change_pcts, bins=30, color='#3498db', alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(0, color='red', linestyle='--', linewidth=2, label='No change')\n",
    "    ax.set_xlabel('Parameter Change (%)', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Number of Parameters', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('Parameter Change Distribution After Adaptation', fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='upper right', fontsize=10)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(VIZ_DIR / \"param_change_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\n[CELL 07e-14] Saved: param_change_distribution.png\")\n",
    "\n",
    "cell_end(\"CELL 07e-14\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 07e-15] Results summary table + comparison with baselines\n",
    "\n",
    "t0 = cell_start(\"CELL 07e-15\", \"Results summary\")\n",
    "\n",
    "print(\"\\n[CELL 07e-15] ========== RESULTS SUMMARY (Test Set) ==========\")\n",
    "print(f\"K={K}, Q={Q} | Test Episodes: {len(episodes_test):,}\\n\")\n",
    "\n",
    "# Load GRU baseline results for comparison\n",
    "baseline_results_path = RESULTS_DIR / f\"baselines_K{K}_Q{Q}.json\"\n",
    "if baseline_results_path.exists():\n",
    "    baseline_results = read_json(baseline_results_path)\n",
    "    gru_baseline_metrics = baseline_results[\"baselines\"][\"gru_global\"]\n",
    "else:\n",
    "    gru_baseline_metrics = {\"accuracy@1\": 0.3373, \"recall@5\": 0.5590, \"recall@10\": 0.6575, \"mrr\": 0.4437}\n",
    "\n",
    "# Create comparison table\n",
    "print(f\"{'Model':<30} {'Acc@1':>10} {'Recall@5':>10} {'Recall@10':>10} {'MRR':>10}\")\n",
    "print(\"-\" * 72)\n",
    "\n",
    "# Baselines\n",
    "print(f\"{'GRU (Baseline - 06)':<30} {gru_baseline_metrics['accuracy@1']:>10.4f} \"\n",
    "      f\"{gru_baseline_metrics['recall@5']:>10.4f} {gru_baseline_metrics['recall@10']:>10.4f} \"\n",
    "      f\"{gru_baseline_metrics['mrr']:>10.4f}\")\n",
    "\n",
    "# MAML results\n",
    "print(f\"{'MAML Zero-shot':<30} {zeroshot_metrics['accuracy@1']:>10.4f} \"\n",
    "      f\"{zeroshot_metrics['recall@5']:>10.4f} {zeroshot_metrics['recall@10']:>10.4f} \"\n",
    "      f\"{zeroshot_metrics['mrr']:>10.4f}\")\n",
    "\n",
    "print(f\"{'MAML Few-shot (K=5)':<30} {fewshot_metrics['accuracy@1']:>10.4f} \"\n",
    "      f\"{fewshot_metrics['recall@5']:>10.4f} {fewshot_metrics['recall@10']:>10.4f} \"\n",
    "      f\"{fewshot_metrics['mrr']:>10.4f}\")\n",
    "\n",
    "# Improvement over baseline\n",
    "improvement = (fewshot_metrics['accuracy@1'] - gru_baseline_metrics['accuracy@1']) / gru_baseline_metrics['accuracy@1'] * 100\n",
    "print(f\"\\n[CELL 07e-15] MAML Few-shot improvement over GRU baseline: {improvement:+.2f}%\")\n",
    "\n",
    "# Ablation results\n",
    "print(f\"\\n[CELL 07e-15] ========== ABLATION STUDY 1: Support Set Size ==========\")\n",
    "print(f\"{'K (Support Size)':<20} {'Acc@1':>10} {'Recall@5':>10} {'Recall@10':>10} {'MRR':>10}\")\n",
    "print(\"-\" * 62)\n",
    "for K_test, metrics in ablation_support_results.items():\n",
    "    print(f\"{K_test:<20} {metrics['accuracy@1']:>10.4f} {metrics['recall@5']:>10.4f} \"\n",
    "          f\"{metrics['recall@10']:>10.4f} {metrics['mrr']:>10.4f}\")\n",
    "\n",
    "print(f\"\\n[CELL 07e-15] ========== ABLATION STUDY 2: Adaptation Steps ==========\")\n",
    "print(f\"{'Adaptation Steps':<20} {'Acc@1':>10} {'Recall@5':>10} {'Recall@10':>10} {'MRR':>10}\")\n",
    "print(\"-\" * 62)\n",
    "for num_steps, metrics in ablation_steps_results.items():\n",
    "    print(f\"{num_steps:<20} {metrics['accuracy@1']:>10.4f} {metrics['recall@5']:>10.4f} \"\n",
    "          f\"{metrics['recall@10']:>10.4f} {metrics['mrr']:>10.4f}\")\n",
    "\n",
    "# Save all results\n",
    "all_results = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"k_shot_config\": {\"K\": K, \"Q\": Q},\n",
    "    \"n_test_episodes\": len(episodes_test),\n",
    "    \"baseline\": {\n",
    "        \"gru_global\": gru_baseline_metrics,\n",
    "    },\n",
    "    \"maml\": {\n",
    "        \"zero_shot\": zeroshot_metrics,\n",
    "        \"few_shot_K5\": fewshot_metrics,\n",
    "    },\n",
    "    \"ablation_support_size\": ablation_support_results,\n",
    "    \"ablation_adaptation_steps\": ablation_steps_results,\n",
    "    \"improvement_over_baseline_pct\": improvement,\n",
    "    \"training_history\": training_history,\n",
    "}\n",
    "\n",
    "results_path = Path(CFG[\"outputs\"][\"results\"])\n",
    "write_json_atomic(results_path, all_results)\n",
    "print(f\"\\n[CELL 07e-15] Saved: {results_path.name}\")\n",
    "\n",
    "cell_end(\"CELL 07e-15\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 07e-16] Update report + manifest\n",
    "\n",
    "t0 = cell_start(\"CELL 07e-16\", \"Write report + manifest\")\n",
    "\n",
    "report = read_json(REPORT_PATH)\n",
    "manifest = read_json(MANIFEST_PATH)\n",
    "\n",
    "# Metrics\n",
    "report[\"metrics\"] = {\n",
    "    \"n_test_episodes\": len(episodes_test),\n",
    "    \"gru_baseline_acc1\": gru_baseline_metrics['accuracy@1'],\n",
    "    \"maml_zero_shot_acc1\": zeroshot_metrics['accuracy@1'],\n",
    "    \"maml_few_shot_K5_acc1\": fewshot_metrics['accuracy@1'],\n",
    "    \"improvement_over_baseline_pct\": improvement,\n",
    "    \"training_iterations\": num_meta_iterations,\n",
    "}\n",
    "\n",
    "# Key findings\n",
    "report[\"key_findings\"].extend([\n",
    "    f\"MAML meta-training: {num_meta_iterations:,} iterations with {meta_batch_size} tasks/batch\",\n",
    "    f\"Zero-shot performance (no adaptation): Acc@1={zeroshot_metrics['accuracy@1']:.4f}\",\n",
    "    f\"Few-shot performance (K=5 adaptation): Acc@1={fewshot_metrics['accuracy@1']:.4f}\",\n",
    "    f\"Improvement over GRU baseline: {improvement:+.2f}% ({fewshot_metrics['accuracy@1']:.4f} vs {gru_baseline_metrics['accuracy@1']:.4f})\",\n",
    "    f\"Ablation: Best K={max(ablation_support_results, key=lambda k: ablation_support_results[k]['accuracy@1'])} \"\n",
    "    f\"(Acc@1={max(ablation_support_results.values(), key=lambda m: m['accuracy@1'])['accuracy@1']:.4f})\",\n",
    "    f\"Ablation: Best adaptation steps={max(ablation_steps_results, key=lambda k: ablation_steps_results[k]['accuracy@1'])} \"\n",
    "    f\"(Acc@1={max(ablation_steps_results.values(), key=lambda m: m['accuracy@1'])['accuracy@1']:.4f})\",\n",
    "])\n",
    "\n",
    "# Sanity samples\n",
    "report[\"sanity_samples\"][\"maml_config\"] = CFG[\"maml_config\"]\n",
    "report[\"sanity_samples\"][\"sample_episode\"] = {\n",
    "    \"episode_id\": int(episodes_test.iloc[0][\"episode_id\"]),\n",
    "    \"user_id\": str(episodes_test.iloc[0][\"user_id\"]),\n",
    "    \"n_support_pairs\": len(episodes_test.iloc[0][\"support_pair_ids\"]),\n",
    "    \"n_query_pairs\": len(episodes_test.iloc[0][\"query_pair_ids\"]),\n",
    "}\n",
    "\n",
    "# Fingerprints\n",
    "report[\"data_fingerprints\"][\"meta_model\"] = {\n",
    "    \"path\": str(final_model_path),\n",
    "    \"bytes\": int(final_model_path.stat().st_size),\n",
    "    \"sha256\": sha256_file(final_model_path),\n",
    "}\n",
    "\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "# Manifest\n",
    "def add_artifact(path: Path) -> None:\n",
    "    rec = {\"path\": str(path), \"bytes\": int(path.stat().st_size), \"sha256\": None, \"sha256_error\": None}\n",
    "    try:\n",
    "        rec[\"sha256\"] = sha256_file(path)\n",
    "    except Exception as e:\n",
    "        rec[\"sha256_error\"] = str(e)\n",
    "    manifest[\"artifacts\"].append(rec)\n",
    "\n",
    "add_artifact(final_model_path)\n",
    "add_artifact(results_path)\n",
    "\n",
    "# Add checkpoints\n",
    "for checkpoint_file in sorted(CHECKPOINTS_DIR.glob(\"checkpoint_iter*.pth\")):\n",
    "    add_artifact(checkpoint_file)\n",
    "\n",
    "write_json_atomic(MANIFEST_PATH, manifest)\n",
    "\n",
    "print(f\"[CELL 07e-16] Updated: {REPORT_PATH}\")\n",
    "print(f\"[CELL 07e-16] Updated: {MANIFEST_PATH}\")\n",
    "\n",
    "cell_end(\"CELL 07e-16\", t0)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"âœ… NOTEBOOK 07 COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nðŸ“Š Key Results:\")\n",
    "print(f\"  - GRU Baseline (06):        {gru_baseline_metrics['accuracy@1']:.4f} Acc@1\")\n",
    "print(f\"  - MAML Zero-shot:           {zeroshot_metrics['accuracy@1']:.4f} Acc@1\")\n",
    "print(f\"  - MAML Few-shot (K=5):      {fewshot_metrics['accuracy@1']:.4f} Acc@1\")\n",
    "print(f\"  - Improvement:              {improvement:+.2f}%\")\n",
    "print(f\"\\nðŸ“ Outputs:\")\n",
    "print(f\"  - Meta-model: {final_model_path}\")\n",
    "print(f\"  - Results:    {results_path}\")\n",
    "print(f\"  - Report:     {REPORT_PATH}\")\n",
    "print(f\"\\nðŸŽ¯ Next Steps:\")\n",
    "print(f\"  - Fine-tune hyperparameters (Î±, Î², inner steps)\")\n",
    "print(f\"  - Try different architectures (Transformer, GNN)\")\n",
    "print(f\"  - Compare with other meta-learning methods (ProtoNet, Matching Networks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Notebook 07e Complete: MAML with Warm-Start\\n\n",
    "\\n\n",
    "**Key Innovation:** Initialized MAML from pre-trained GRU baseline (33.73%)\\n\n",
    "\\n\n",
    "**Results:**\\n\n",
    "- See CELL 07e-10 for zero-shot results\\n\n",
    "- See CELL 07e-11 for few-shot results (K=5)\\n\n",
    "- See CELL 07e-15 for complete comparison\\n\n",
    "\\n\n",
    "**Expected Outcome:**\\n\n",
    "- MAML warm-start should beat both:\\n\n",
    "  - MAML random init (30.52%)\\n\n",
    "  - GRU baseline (33.73%)\\n\n",
    "- Target: 35-38% Acc@1\\n\n",
    "\\n\n",
    "**Why This Works:**\\n\n",
    "1. Strong initialization (GRU) + Meta-learned adaptation (MAML)\\n\n",
    "2. Best of both worlds\\n\n",
    "3. Well-established approach in meta-learning literature\\n\n",
    "\\n\n",
    "**Next Steps:**\\n\n",
    "- If beats baseline â†’ Thesis complete! ðŸŽ‰\\n\n",
    "- If close but not quite â†’ Try bigger model (Fix #4)\\n\n",
    "- Analyze learned adaptations vs random init MAML"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
