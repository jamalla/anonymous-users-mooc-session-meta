{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 05: Episode Index (XuetangX)\n",
    "\n",
    "**Purpose:** Create episodic meta-learning indices for K-shot learning.\n",
    "\n",
    "**Cold-Start Focus:**\n",
    "- **User-as-task**: Each episode represents one user's learning task\n",
    "- **Support set**: K pairs from user's history (for adaptation)\n",
    "- **Query set**: Q pairs from user's history (for evaluation)\n",
    "- **Chronological ordering**: Support timestamps < Query timestamps (no future leakage)\n",
    "\n",
    "**Inputs:**\n",
    "- `data/processed/xuetangx/pairs/pairs_train.parquet` (139,349 pairs, 28,633 users)\n",
    "- `data/processed/xuetangx/pairs/pairs_val.parquet` (17,848 pairs, 3,579 users)\n",
    "- `data/processed/xuetangx/pairs/pairs_test.parquet` (18,324 pairs, 3,580 users)\n",
    "\n",
    "**Outputs:**\n",
    "- `data/processed/xuetangx/episodes/episodes_train_K{K}_Q{Q}.parquet`\n",
    "- `data/processed/xuetangx/episodes/episodes_val_K{K}_Q{Q}.parquet`\n",
    "- `data/processed/xuetangx/episodes/episodes_test_K{K}_Q{Q}.parquet`\n",
    "- DuckDB views: `xuetangx_episodes_train_K{K}_Q{Q}`, etc.\n",
    "- `reports/05_episode_index_xuetangx/<run_tag>/report.json`\n",
    "\n",
    "**Strategy:**\n",
    "1. Filter users by minimum pairs: ≥K+Q pairs required\n",
    "2. For each eligible user, create episodes:\n",
    "   - **Train**: Multiple episodes per user (sliding window approach)\n",
    "   - **Val/Test**: Single episode per user (last K+Q pairs)\n",
    "3. Each episode:\n",
    "   - Support: K pairs (chronologically first)\n",
    "   - Query: Q pairs (chronologically after support)\n",
    "4. Validate: support_max_ts < query_min_ts (no future leakage)\n",
    "\n",
    "**K-Shot Configurations:**\n",
    "- K=5, Q=10 (primary, 15 pairs minimum)\n",
    "- K=10, Q=20 (secondary, 30 pairs minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 05-00] start=2026-01-30T07:52:21\n",
      "[CELL 05-00] CWD: /workspace/anonymous-users-mooc-session-meta/notebooks\n",
      "[CELL 05-00] REPO_ROOT: /workspace/anonymous-users-mooc-session-meta\n",
      "[CELL 05-00] META_REGISTRY=/workspace/anonymous-users-mooc-session-meta/meta.json\n",
      "[CELL 05-00] DATA_INTERIM=/workspace/anonymous-users-mooc-session-meta/data/interim\n",
      "[CELL 05-00] DATA_PROCESSED=/workspace/anonymous-users-mooc-session-meta/data/processed\n",
      "[CELL 05-00] REPORTS=/workspace/anonymous-users-mooc-session-meta/reports\n",
      "[CELL 05-00] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05-00] Bootstrap: repo root + paths + logger\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "t0 = datetime.now()\n",
    "print(f\"[CELL 05-00] start={t0.isoformat(timespec='seconds')}\")\n",
    "print(\"[CELL 05-00] CWD:\", Path.cwd().resolve())\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists():\n",
    "            return p\n",
    "    raise RuntimeError(\"Could not find PROJECT_STATE.md. Open notebook from within the repo.\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "print(\"[CELL 05-00] REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "PATHS = {\n",
    "    \"META_REGISTRY\": REPO_ROOT / \"meta.json\",\n",
    "    \"DATA_INTERIM\": REPO_ROOT / \"data\" / \"interim\",\n",
    "    \"DATA_PROCESSED\": REPO_ROOT / \"data\" / \"processed\",\n",
    "    \"REPORTS\": REPO_ROOT / \"reports\",\n",
    "}\n",
    "for k, v in PATHS.items():\n",
    "    print(f\"[CELL 05-00] {k}={v}\")\n",
    "\n",
    "def cell_start(cell_id: str, title: str, **kwargs: Any) -> float:\n",
    "    t = time.time()\n",
    "    print(f\"\\n[{cell_id}] {title}\")\n",
    "    print(f\"[{cell_id}] start={datetime.now().isoformat(timespec='seconds')}\")\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    return t\n",
    "\n",
    "def cell_end(cell_id: str, t0: float, **kwargs: Any) -> None:\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    print(f\"[{cell_id}] elapsed={time.time()-t0:.2f}s\")\n",
    "    print(f\"[{cell_id}] done\")\n",
    "\n",
    "print(\"[CELL 05-00] done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 05-01] Seed everything\n",
      "[CELL 05-01] start=2026-01-30T07:52:21\n",
      "[CELL 05-01] seed=20260107\n",
      "[CELL 05-01] elapsed=0.00s\n",
      "[CELL 05-01] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05-01] Reproducibility: seed everything\n",
    "\n",
    "t0 = cell_start(\"CELL 05-01\", \"Seed everything\")\n",
    "\n",
    "GLOBAL_SEED = 20260107\n",
    "\n",
    "def seed_everything(seed: int) -> None:\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(GLOBAL_SEED)\n",
    "\n",
    "cell_end(\"CELL 05-01\", t0, seed=GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 05-02] JSON IO + hashing\n",
      "[CELL 05-02] start=2026-01-30T07:52:21\n",
      "[CELL 05-02] elapsed=0.00s\n",
      "[CELL 05-02] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05-02] JSON IO + hashing helpers\n",
    "\n",
    "t0 = cell_start(\"CELL 05-02\", \"JSON IO + hashing\")\n",
    "\n",
    "def write_json_atomic(path: Path, obj: Any, indent: int = 2) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tmp = path.with_suffix(path.suffix + f\".tmp_{uuid.uuid4().hex}\")\n",
    "    with tmp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=indent)\n",
    "    tmp.replace(path)\n",
    "\n",
    "def read_json(path: Path) -> Any:\n",
    "    if not path.exists():\n",
    "        raise RuntimeError(f\"Missing JSON file: {path}\")\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "cell_end(\"CELL 05-02\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 05-03] Start run + init files + meta.json\n",
      "[CELL 05-03] start=2026-01-30T07:52:21\n",
      "[CELL 05-03] K-shot configs: [{'K': 5, 'Q': 10}, {'K': 10, 'Q': 20}]\n",
      "[CELL 05-03] out_dir=/workspace/anonymous-users-mooc-session-meta/reports/05_episode_index_xuetangx/20260130_075221\n",
      "[CELL 05-03] elapsed=0.00s\n",
      "[CELL 05-03] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05-03] Run tagging + K-shot config + report/config/manifest + meta.json\n",
    "\n",
    "t0 = cell_start(\"CELL 05-03\", \"Start run + init files + meta.json\")\n",
    "\n",
    "NOTEBOOK_NAME = \"05_episode_index_xuetangx\"\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_ID = uuid.uuid4().hex\n",
    "\n",
    "OUT_DIR = PATHS[\"REPORTS\"] / NOTEBOOK_NAME / RUN_TAG\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REPORT_PATH = OUT_DIR / \"report.json\"\n",
    "CONFIG_PATH = OUT_DIR / \"config.json\"\n",
    "MANIFEST_PATH = OUT_DIR / \"manifest.json\"\n",
    "\n",
    "DUCKDB_PATH = PATHS[\"DATA_INTERIM\"] / \"xuetangx.duckdb\"\n",
    "PAIRS_DIR = PATHS[\"DATA_PROCESSED\"] / \"xuetangx\" / \"pairs\"\n",
    "EPISODES_DIR = PATHS[\"DATA_PROCESSED\"] / \"xuetangx\" / \"episodes\"\n",
    "EPISODES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PAIRS_TRAIN = PAIRS_DIR / \"pairs_train.parquet\"\n",
    "PAIRS_VAL = PAIRS_DIR / \"pairs_val.parquet\"\n",
    "PAIRS_TEST = PAIRS_DIR / \"pairs_test.parquet\"\n",
    "\n",
    "# K-shot configurations\n",
    "K_SHOT_CONFIGS = [\n",
    "    {\"K\": 5, \"Q\": 10},   # Primary: 15 pairs minimum\n",
    "    {\"K\": 10, \"Q\": 20},  # Secondary: 30 pairs minimum\n",
    "]\n",
    "\n",
    "CFG = {\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"seed\": GLOBAL_SEED,\n",
    "    \"inputs\": {\n",
    "        \"pairs_train\": str(PAIRS_TRAIN),\n",
    "        \"pairs_val\": str(PAIRS_VAL),\n",
    "        \"pairs_test\": str(PAIRS_TEST),\n",
    "    },\n",
    "    \"k_shot_configs\": K_SHOT_CONFIGS,\n",
    "    \"episode_strategy\": {\n",
    "        \"train\": \"multiple_episodes_per_user\",  # Sliding window: creates multiple episodes\n",
    "        \"val\": \"single_episode_per_user\",       # Last K+Q pairs only\n",
    "        \"test\": \"single_episode_per_user\",      # Last K+Q pairs only\n",
    "        \"train_stride\": 1,  # Slide by 1 pair at a time (maximum training data)\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"episodes_dir\": str(EPISODES_DIR),\n",
    "        \"out_dir\": str(OUT_DIR),\n",
    "    }\n",
    "}\n",
    "\n",
    "write_json_atomic(CONFIG_PATH, CFG)\n",
    "\n",
    "report = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"repo_root\": str(REPO_ROOT),\n",
    "    \"metrics\": {},\n",
    "    \"key_findings\": [],\n",
    "    \"sanity_samples\": {},\n",
    "    \"data_fingerprints\": {},\n",
    "    \"notes\": [],\n",
    "}\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "manifest = {\"run_id\": RUN_ID, \"notebook\": NOTEBOOK_NAME, \"run_tag\": RUN_TAG, \"artifacts\": []}\n",
    "write_json_atomic(MANIFEST_PATH, manifest)\n",
    "\n",
    "# meta.json append-only\n",
    "META_PATH = PATHS[\"META_REGISTRY\"]\n",
    "if not META_PATH.exists():\n",
    "    write_json_atomic(META_PATH, {\"schema_version\": 1, \"runs\": []})\n",
    "meta = read_json(META_PATH)\n",
    "meta[\"runs\"].append({\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"out_dir\": str(OUT_DIR),\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "})\n",
    "write_json_atomic(META_PATH, meta)\n",
    "\n",
    "print(f\"[CELL 05-03] K-shot configs: {K_SHOT_CONFIGS}\")\n",
    "\n",
    "cell_end(\"CELL 05-03\", t0, out_dir=str(OUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 05-04] Load split pairs\n",
      "[CELL 05-04] start=2026-01-30T07:52:21\n",
      "[CELL 05-04] Loaded pairs_train: 139,349 pairs, 28,633 users\n",
      "[CELL 05-04] Loaded pairs_val:   17,848 pairs, 3,579 users\n",
      "[CELL 05-04] Loaded pairs_test:  18,324 pairs, 3,580 users\n",
      "[CELL 05-04] elapsed=0.10s\n",
      "[CELL 05-04] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05-04] Load split pairs from Notebook 04\n",
    "\n",
    "t0 = cell_start(\"CELL 05-04\", \"Load split pairs\")\n",
    "\n",
    "if not PAIRS_TRAIN.exists():\n",
    "    raise RuntimeError(f\"Missing pairs_train.parquet. Run Notebook 04 first.\")\n",
    "if not PAIRS_VAL.exists():\n",
    "    raise RuntimeError(f\"Missing pairs_val.parquet. Run Notebook 04 first.\")\n",
    "if not PAIRS_TEST.exists():\n",
    "    raise RuntimeError(f\"Missing pairs_test.parquet. Run Notebook 04 first.\")\n",
    "\n",
    "pairs_train = pd.read_parquet(PAIRS_TRAIN)\n",
    "pairs_val = pd.read_parquet(PAIRS_VAL)\n",
    "pairs_test = pd.read_parquet(PAIRS_TEST)\n",
    "\n",
    "print(f\"[CELL 05-04] Loaded pairs_train: {pairs_train.shape[0]:,} pairs, {pairs_train['user_id'].nunique():,} users\")\n",
    "print(f\"[CELL 05-04] Loaded pairs_val:   {pairs_val.shape[0]:,} pairs, {pairs_val['user_id'].nunique():,} users\")\n",
    "print(f\"[CELL 05-04] Loaded pairs_test:  {pairs_test.shape[0]:,} pairs, {pairs_test['user_id'].nunique():,} users\")\n",
    "\n",
    "cell_end(\"CELL 05-04\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 05-05] Define episode creation function\n",
      "[CELL 05-05] start=2026-01-30T07:52:21\n",
      "[CELL 05-05] Episode creation function defined\n",
      "[CELL 05-05] Strategy:\n",
      "  - Train: Multiple episodes per user (sliding window, stride=1)\n",
      "  - Val/Test: Single episode per user (last K+Q pairs)\n",
      "  - Chronological: support_max_ts < query_min_ts (no future leakage)\n",
      "[CELL 05-05] elapsed=0.00s\n",
      "[CELL 05-05] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05-05] Episode creation function (chronological support→query)\n",
    "\n",
    "t0 = cell_start(\"CELL 05-05\", \"Define episode creation function\")\n",
    "\n",
    "def create_episodes(\n",
    "    pairs_df: pd.DataFrame,\n",
    "    K: int,\n",
    "    Q: int,\n",
    "    mode: str,  # 'train', 'val', or 'test'\n",
    "    stride: int = 1,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create episodic meta-learning indices.\n",
    "    \n",
    "    Args:\n",
    "        pairs_df: DataFrame with columns [pair_id, user_id, prefix, label, label_ts_epoch, ...]\n",
    "        K: Number of support pairs\n",
    "        Q: Number of query pairs\n",
    "        mode: 'train' (multiple episodes), 'val'/'test' (single episode)\n",
    "        stride: For train mode, stride for sliding window (default 1)\n",
    "    \n",
    "    Returns:\n",
    "        List of episode dictionaries\n",
    "    \"\"\"\n",
    "    min_pairs = K + Q\n",
    "    episodes = []\n",
    "    episode_id = 0\n",
    "    \n",
    "    # Group by user and sort by timestamp\n",
    "    for user_id, group in pairs_df.groupby(\"user_id\"):\n",
    "        # Sort pairs by timestamp (chronological)\n",
    "        group = group.sort_values(\"label_ts_epoch\").reset_index(drop=True)\n",
    "        \n",
    "        n_pairs = len(group)\n",
    "        \n",
    "        # Filter: user must have at least K+Q pairs\n",
    "        if n_pairs < min_pairs:\n",
    "            continue\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            # Multiple episodes per user (sliding window)\n",
    "            # Start index can be any position where we have K+Q pairs remaining\n",
    "            for start_idx in range(0, n_pairs - min_pairs + 1, stride):\n",
    "                support_pairs = group.iloc[start_idx:start_idx+K]\n",
    "                query_pairs = group.iloc[start_idx+K:start_idx+K+Q]\n",
    "                \n",
    "                # Chronological validation\n",
    "                support_max_ts = support_pairs[\"label_ts_epoch\"].max()\n",
    "                query_min_ts = query_pairs[\"label_ts_epoch\"].min()\n",
    "                \n",
    "                if support_max_ts >= query_min_ts:\n",
    "                    # Should not happen if data is sorted, but check anyway\n",
    "                    continue\n",
    "                \n",
    "                episodes.append({\n",
    "                    \"episode_id\": episode_id,\n",
    "                    \"user_id\": user_id,\n",
    "                    \"K\": K,\n",
    "                    \"Q\": Q,\n",
    "                    \"support_pair_ids\": support_pairs[\"pair_id\"].tolist(),\n",
    "                    \"query_pair_ids\": query_pairs[\"pair_id\"].tolist(),\n",
    "                    \"support_max_ts\": int(support_max_ts),\n",
    "                    \"query_min_ts\": int(query_min_ts),\n",
    "                })\n",
    "                episode_id += 1\n",
    "        \n",
    "        else:  # val or test\n",
    "            # Single episode per user: last K+Q pairs\n",
    "            support_pairs = group.iloc[-min_pairs:-Q]\n",
    "            query_pairs = group.iloc[-Q:]\n",
    "            \n",
    "            # Chronological validation\n",
    "            support_max_ts = support_pairs[\"label_ts_epoch\"].max()\n",
    "            query_min_ts = query_pairs[\"label_ts_epoch\"].min()\n",
    "            \n",
    "            if support_max_ts >= query_min_ts:\n",
    "                # Should not happen, but skip if so\n",
    "                continue\n",
    "            \n",
    "            episodes.append({\n",
    "                \"episode_id\": episode_id,\n",
    "                \"user_id\": user_id,\n",
    "                \"K\": K,\n",
    "                \"Q\": Q,\n",
    "                \"support_pair_ids\": support_pairs[\"pair_id\"].tolist(),\n",
    "                \"query_pair_ids\": query_pairs[\"pair_id\"].tolist(),\n",
    "                \"support_max_ts\": int(support_max_ts),\n",
    "                \"query_min_ts\": int(query_min_ts),\n",
    "            })\n",
    "            episode_id += 1\n",
    "    \n",
    "    return episodes\n",
    "\n",
    "print(\"[CELL 05-05] Episode creation function defined\")\n",
    "print(\"[CELL 05-05] Strategy:\")\n",
    "print(\"  - Train: Multiple episodes per user (sliding window, stride=1)\")\n",
    "print(\"  - Val/Test: Single episode per user (last K+Q pairs)\")\n",
    "print(\"  - Chronological: support_max_ts < query_min_ts (no future leakage)\")\n",
    "\n",
    "cell_end(\"CELL 05-05\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 05-06] Create episodes for all configs\n",
      "[CELL 05-06] start=2026-01-30T07:52:21\n",
      "\n",
      "[CELL 05-06] ========== K=5, Q=10 ==========\n",
      "[CELL 05-06] Creating train episodes (K=5, Q=10)...\n",
      "[CELL 05-06] Train: 30,895 episodes, 1,859 users\n",
      "[CELL 05-06] Saved: episodes_train_K5_Q10.parquet (0.8 MB)\n",
      "[CELL 05-06] Creating val episodes (K=5, Q=10)...\n",
      "[CELL 05-06] Val:   258 episodes, 258 users\n",
      "[CELL 05-06] Saved: episodes_val_K5_Q10.parquet (0.0 MB)\n",
      "[CELL 05-06] Creating test episodes (K=5, Q=10)...\n",
      "[CELL 05-06] Test:  248 episodes, 248 users\n",
      "[CELL 05-06] Saved: episodes_test_K5_Q10.parquet (0.0 MB)\n",
      "\n",
      "[CELL 05-06] ========== K=10, Q=20 ==========\n",
      "[CELL 05-06] Creating train episodes (K=10, Q=20)...\n",
      "[CELL 05-06] Train: 14,383 episodes, 597 users\n",
      "[CELL 05-06] Saved: episodes_train_K10_Q20.parquet (0.7 MB)\n",
      "[CELL 05-06] Creating val episodes (K=10, Q=20)...\n",
      "[CELL 05-06] Val:   72 episodes, 72 users\n",
      "[CELL 05-06] Saved: episodes_val_K10_Q20.parquet (0.0 MB)\n",
      "[CELL 05-06] Creating test episodes (K=10, Q=20)...\n",
      "[CELL 05-06] Test:  85 episodes, 85 users\n",
      "[CELL 05-06] Saved: episodes_test_K10_Q20.parquet (0.0 MB)\n",
      "\n",
      "[CELL 05-06] Created episodes for 2 K-shot configs x 3 splits = 6 files\n",
      "[CELL 05-06] elapsed=13.60s\n",
      "[CELL 05-06] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05-06] Create episodes for all K-shot configs and splits\n",
    "\n",
    "t0 = cell_start(\"CELL 05-06\", \"Create episodes for all configs\")\n",
    "\n",
    "all_episode_files = []\n",
    "all_episode_stats = []\n",
    "\n",
    "for cfg in K_SHOT_CONFIGS:\n",
    "    K = cfg[\"K\"]\n",
    "    Q = cfg[\"Q\"]\n",
    "    \n",
    "    print(f\"\\n[CELL 05-06] ========== K={K}, Q={Q} ==========\")\n",
    "    \n",
    "    # Train episodes\n",
    "    print(f\"[CELL 05-06] Creating train episodes (K={K}, Q={Q})...\")\n",
    "    episodes_train = create_episodes(\n",
    "        pairs_train, \n",
    "        K=K, \n",
    "        Q=Q, \n",
    "        mode=\"train\", \n",
    "        stride=CFG[\"episode_strategy\"][\"train_stride\"]\n",
    "    )\n",
    "    episodes_train_df = pd.DataFrame(episodes_train)\n",
    "    \n",
    "    out_train = EPISODES_DIR / f\"episodes_train_K{K}_Q{Q}.parquet\"\n",
    "    episodes_train_df.to_parquet(out_train, index=False, compression=\"zstd\")\n",
    "    print(f\"[CELL 05-06] Train: {len(episodes_train_df):,} episodes, {episodes_train_df['user_id'].nunique():,} users\")\n",
    "    print(f\"[CELL 05-06] Saved: {out_train.name} ({out_train.stat().st_size / 1024 / 1024:.1f} MB)\")\n",
    "    \n",
    "    all_episode_files.append((\"train\", K, Q, out_train))\n",
    "    all_episode_stats.append({\n",
    "        \"split\": \"train\",\n",
    "        \"K\": K,\n",
    "        \"Q\": Q,\n",
    "        \"n_episodes\": len(episodes_train_df),\n",
    "        \"n_users\": int(episodes_train_df[\"user_id\"].nunique()),\n",
    "        \"episodes_per_user_mean\": float(len(episodes_train_df) / episodes_train_df[\"user_id\"].nunique()),\n",
    "    })\n",
    "    \n",
    "    # Val episodes\n",
    "    print(f\"[CELL 05-06] Creating val episodes (K={K}, Q={Q})...\")\n",
    "    episodes_val = create_episodes(pairs_val, K=K, Q=Q, mode=\"val\")\n",
    "    episodes_val_df = pd.DataFrame(episodes_val)\n",
    "    \n",
    "    out_val = EPISODES_DIR / f\"episodes_val_K{K}_Q{Q}.parquet\"\n",
    "    episodes_val_df.to_parquet(out_val, index=False, compression=\"zstd\")\n",
    "    print(f\"[CELL 05-06] Val:   {len(episodes_val_df):,} episodes, {episodes_val_df['user_id'].nunique():,} users\")\n",
    "    print(f\"[CELL 05-06] Saved: {out_val.name} ({out_val.stat().st_size / 1024 / 1024:.1f} MB)\")\n",
    "    \n",
    "    all_episode_files.append((\"val\", K, Q, out_val))\n",
    "    all_episode_stats.append({\n",
    "        \"split\": \"val\",\n",
    "        \"K\": K,\n",
    "        \"Q\": Q,\n",
    "        \"n_episodes\": len(episodes_val_df),\n",
    "        \"n_users\": int(episodes_val_df[\"user_id\"].nunique()),\n",
    "    })\n",
    "    \n",
    "    # Test episodes\n",
    "    print(f\"[CELL 05-06] Creating test episodes (K={K}, Q={Q})...\")\n",
    "    episodes_test = create_episodes(pairs_test, K=K, Q=Q, mode=\"test\")\n",
    "    episodes_test_df = pd.DataFrame(episodes_test)\n",
    "    \n",
    "    out_test = EPISODES_DIR / f\"episodes_test_K{K}_Q{Q}.parquet\"\n",
    "    episodes_test_df.to_parquet(out_test, index=False, compression=\"zstd\")\n",
    "    print(f\"[CELL 05-06] Test:  {len(episodes_test_df):,} episodes, {episodes_test_df['user_id'].nunique():,} users\")\n",
    "    print(f\"[CELL 05-06] Saved: {out_test.name} ({out_test.stat().st_size / 1024 / 1024:.1f} MB)\")\n",
    "    \n",
    "    all_episode_files.append((\"test\", K, Q, out_test))\n",
    "    all_episode_stats.append({\n",
    "        \"split\": \"test\",\n",
    "        \"K\": K,\n",
    "        \"Q\": Q,\n",
    "        \"n_episodes\": len(episodes_test_df),\n",
    "        \"n_users\": int(episodes_test_df[\"user_id\"].nunique()),\n",
    "    })\n",
    "\n",
    "print(f\"\\n[CELL 05-06] Created episodes for {len(K_SHOT_CONFIGS)} K-shot configs x 3 splits = {len(all_episode_files)} files\")\n",
    "\n",
    "cell_end(\"CELL 05-06\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 05-07] Register DuckDB views\n",
      "[CELL 05-07] start=2026-01-30T07:52:35\n",
      "[CELL 05-07] duckdb=/workspace/anonymous-users-mooc-session-meta/data/interim/xuetangx.duckdb\n",
      "[CELL 05-07] View xuetangx_episodes_train_K5_Q10: 30,895 rows\n",
      "[CELL 05-07] View xuetangx_episodes_val_K5_Q10: 258 rows\n",
      "[CELL 05-07] View xuetangx_episodes_test_K5_Q10: 248 rows\n",
      "[CELL 05-07] View xuetangx_episodes_train_K10_Q20: 14,383 rows\n",
      "[CELL 05-07] View xuetangx_episodes_val_K10_Q20: 72 rows\n",
      "[CELL 05-07] View xuetangx_episodes_test_K10_Q20: 85 rows\n",
      "\n",
      "[CELL 05-07] Closed DuckDB connection\n",
      "[CELL 05-07] elapsed=0.08s\n",
      "[CELL 05-07] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05-07] Register DuckDB views for all episode files\n",
    "\n",
    "t0 = cell_start(\"CELL 05-07\", \"Register DuckDB views\", duckdb=str(DUCKDB_PATH))\n",
    "\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(str(DUCKDB_PATH), read_only=False)\n",
    "\n",
    "def esc_path(p: Path) -> str:\n",
    "    return str(p).replace(\"'\", \"''\")\n",
    "\n",
    "for split, K, Q, path in all_episode_files:\n",
    "    view_name = f\"xuetangx_episodes_{split}_K{K}_Q{Q}\"\n",
    "    \n",
    "    con.execute(f\"DROP VIEW IF EXISTS {view_name};\")\n",
    "    con.execute(f\"\"\"\n",
    "    CREATE VIEW {view_name} AS\n",
    "    SELECT * FROM read_parquet('{esc_path(path)}')\n",
    "    \"\"\")\n",
    "    \n",
    "    n_rows = int(con.execute(f\"SELECT COUNT(*) FROM {view_name}\").fetchone()[0])\n",
    "    print(f\"[CELL 05-07] View {view_name}: {n_rows:,} rows\")\n",
    "\n",
    "con.close()\n",
    "print(f\"\\n[CELL 05-07] Closed DuckDB connection\")\n",
    "\n",
    "cell_end(\"CELL 05-07\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 05-08] Validate chronological ordering\n",
      "[CELL 05-08] start=2026-01-30T07:52:35\n",
      "[CELL 05-08] Checking all episodes for chronological ordering (support_max_ts < query_min_ts)...\n",
      "\n",
      "[CELL 05-08] [OK] train K= 5 Q=10: 30,895 episodes, all chronologically valid\n",
      "[CELL 05-08] [OK] val   K= 5 Q=10: 258 episodes, all chronologically valid\n",
      "[CELL 05-08] [OK] test  K= 5 Q=10: 248 episodes, all chronologically valid\n",
      "[CELL 05-08] [OK] train K=10 Q=20: 14,383 episodes, all chronologically valid\n",
      "[CELL 05-08] [OK] val   K=10 Q=20: 72 episodes, all chronologically valid\n",
      "[CELL 05-08] [OK] test  K=10 Q=20: 85 episodes, all chronologically valid\n",
      "\n",
      "[CELL 05-08] [OK] All episodes validated: support timestamps < query timestamps\n",
      "[CELL 05-08] elapsed=0.07s\n",
      "[CELL 05-08] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05-08] Validation: chronological ordering check\n",
    "\n",
    "t0 = cell_start(\"CELL 05-08\", \"Validate chronological ordering\")\n",
    "\n",
    "print(f\"[CELL 05-08] Checking all episodes for chronological ordering (support_max_ts < query_min_ts)...\\n\")\n",
    "\n",
    "for split, K, Q, path in all_episode_files:\n",
    "    episodes_df = pd.read_parquet(path)\n",
    "    \n",
    "    # Check: support_max_ts < query_min_ts for all episodes\n",
    "    violations = episodes_df[episodes_df[\"support_max_ts\"] >= episodes_df[\"query_min_ts\"]]\n",
    "    \n",
    "    if len(violations) > 0:\n",
    "        raise RuntimeError(f\"Found {len(violations)} chronology violations in {split} K={K} Q={Q}\")\n",
    "    \n",
    "    print(f\"[CELL 05-08] [OK] {split:5s} K={K:2d} Q={Q:2d}: {len(episodes_df):,} episodes, all chronologically valid\")\n",
    "\n",
    "print(f\"\\n[CELL 05-08] [OK] All episodes validated: support timestamps < query timestamps\")\n",
    "\n",
    "cell_end(\"CELL 05-08\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 05-09] Episode statistics\n",
      "[CELL 05-09] start=2026-01-30T07:52:35\n",
      "[CELL 05-09] Episode counts by K-shot config:\n",
      "\n",
      "[CELL 05-09] K=5, Q=10 (min 15 pairs):\n",
      "  train: 30,895 episodes, 1,859 users (16.6 episodes/user)\n",
      "  val  :    258 episodes,   258 users (1.0 episode/user)\n",
      "  test :    248 episodes,   248 users (1.0 episode/user)\n",
      "\n",
      "[CELL 05-09] K=10, Q=20 (min 30 pairs):\n",
      "  train: 14,383 episodes,   597 users (24.1 episodes/user)\n",
      "  val  :     72 episodes,    72 users (1.0 episode/user)\n",
      "  test :     85 episodes,    85 users (1.0 episode/user)\n",
      "\n",
      "[CELL 05-09] elapsed=0.00s\n",
      "[CELL 05-09] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05-09] Episode statistics summary\n",
    "\n",
    "t0 = cell_start(\"CELL 05-09\", \"Episode statistics\")\n",
    "\n",
    "print(f\"[CELL 05-09] Episode counts by K-shot config:\\n\")\n",
    "\n",
    "for cfg in K_SHOT_CONFIGS:\n",
    "    K = cfg[\"K\"]\n",
    "    Q = cfg[\"Q\"]\n",
    "    \n",
    "    print(f\"[CELL 05-09] K={K}, Q={Q} (min {K+Q} pairs):\")\n",
    "    \n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        stats = [s for s in all_episode_stats if s[\"split\"] == split and s[\"K\"] == K and s[\"Q\"] == Q][0]\n",
    "        n_episodes = stats[\"n_episodes\"]\n",
    "        n_users = stats[\"n_users\"]\n",
    "        \n",
    "        if split == \"train\":\n",
    "            eps_per_user = stats[\"episodes_per_user_mean\"]\n",
    "            print(f\"  {split:5s}: {n_episodes:6,} episodes, {n_users:5,} users ({eps_per_user:.1f} episodes/user)\")\n",
    "        else:\n",
    "            print(f\"  {split:5s}: {n_episodes:6,} episodes, {n_users:5,} users (1.0 episode/user)\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "cell_end(\"CELL 05-09\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 05-10] Write report + manifest\n",
      "[CELL 05-10] start=2026-01-30T07:52:35\n",
      "[CELL 05-10] Updated: /workspace/anonymous-users-mooc-session-meta/reports/05_episode_index_xuetangx/20260130_075221/report.json\n",
      "[CELL 05-10] Updated: /workspace/anonymous-users-mooc-session-meta/reports/05_episode_index_xuetangx/20260130_075221/manifest.json\n",
      "[CELL 05-10] elapsed=0.03s\n",
      "[CELL 05-10] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05-10] Update report + manifest\n",
    "\n",
    "t0 = cell_start(\"CELL 05-10\", \"Write report + manifest\")\n",
    "\n",
    "report = read_json(REPORT_PATH)\n",
    "manifest = read_json(MANIFEST_PATH)\n",
    "\n",
    "# Metrics\n",
    "report[\"metrics\"][\"k_shot_configs\"] = K_SHOT_CONFIGS\n",
    "report[\"metrics\"][\"episode_stats\"] = all_episode_stats\n",
    "\n",
    "# Key findings\n",
    "for cfg in K_SHOT_CONFIGS:\n",
    "    K = cfg[\"K\"]\n",
    "    Q = cfg[\"Q\"]\n",
    "    \n",
    "    train_stats = [s for s in all_episode_stats if s[\"split\"] == \"train\" and s[\"K\"] == K and s[\"Q\"] == Q][0]\n",
    "    val_stats = [s for s in all_episode_stats if s[\"split\"] == \"val\" and s[\"K\"] == K and s[\"Q\"] == Q][0]\n",
    "    test_stats = [s for s in all_episode_stats if s[\"split\"] == \"test\" and s[\"K\"] == K and s[\"Q\"] == Q][0]\n",
    "    \n",
    "    report[\"key_findings\"].append(\n",
    "        f\"K={K}, Q={Q}: Created {train_stats['n_episodes']:,} train episodes ({train_stats['n_users']:,} users, \"\n",
    "        f\"{train_stats['episodes_per_user_mean']:.1f} eps/user), \"\n",
    "        f\"{val_stats['n_episodes']:,} val episodes ({val_stats['n_users']:,} users), \"\n",
    "        f\"{test_stats['n_episodes']:,} test episodes ({test_stats['n_users']:,} users). \"\n",
    "        f\"All episodes chronologically valid (support_max_ts < query_min_ts).\"\n",
    "    )\n",
    "\n",
    "# Sanity samples (convert to native Python types for JSON serialization)\n",
    "sample_episodes_train = pd.read_parquet(EPISODES_DIR / f\"episodes_train_K5_Q10.parquet\")\n",
    "sample_records = []\n",
    "for _, row in sample_episodes_train.head(3).iterrows():\n",
    "    record = {\n",
    "        \"episode_id\": int(row[\"episode_id\"]),\n",
    "        \"user_id\": str(row[\"user_id\"]),\n",
    "        \"K\": int(row[\"K\"]),\n",
    "        \"Q\": int(row[\"Q\"]),\n",
    "        \"support_pair_ids\": [int(x) for x in row[\"support_pair_ids\"]],\n",
    "        \"query_pair_ids\": [int(x) for x in row[\"query_pair_ids\"]],\n",
    "        \"support_max_ts\": int(row[\"support_max_ts\"]),\n",
    "        \"query_min_ts\": int(row[\"query_min_ts\"]),\n",
    "    }\n",
    "    sample_records.append(record)\n",
    "report[\"sanity_samples\"][\"train_episodes_head3_K5_Q10\"] = sample_records\n",
    "\n",
    "# Fingerprints\n",
    "for split, K, Q, path in all_episode_files:\n",
    "    key = f\"episodes_{split}_K{K}_Q{Q}\"\n",
    "    report[\"data_fingerprints\"][key] = {\n",
    "        \"path\": str(path),\n",
    "        \"bytes\": int(path.stat().st_size),\n",
    "        \"sha256\": sha256_file(path),\n",
    "    }\n",
    "\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "# Manifest\n",
    "def add_artifact(path: Path) -> None:\n",
    "    rec = {\"path\": str(path), \"bytes\": int(path.stat().st_size), \"sha256\": None, \"sha256_error\": None}\n",
    "    try:\n",
    "        rec[\"sha256\"] = sha256_file(path)\n",
    "    except PermissionError as e:\n",
    "        rec[\"sha256_error\"] = f\"PermissionError: {e}\"\n",
    "    manifest[\"artifacts\"].append(rec)\n",
    "\n",
    "for split, K, Q, path in all_episode_files:\n",
    "    add_artifact(path)\n",
    "\n",
    "write_json_atomic(MANIFEST_PATH, manifest)\n",
    "\n",
    "print(f\"[CELL 05-10] Updated: {REPORT_PATH}\")\n",
    "print(f\"[CELL 05-10] Updated: {MANIFEST_PATH}\")\n",
    "\n",
    "cell_end(\"CELL 05-10\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 05-11] Generate plots and tables\n",
      "[CELL 05-11] start=2026-01-30T07:52:35\n",
      "[CELL 05-11] Creating visualizations in /workspace/anonymous-users-mooc-session-meta/reports/05_episode_index_xuetangx/20260130_075221/visualizations\n",
      "[CELL 05-11] Saved: fig1_episode_counts.png\n",
      "[CELL 05-11] Saved: fig2_episodes_per_user.png\n",
      "[CELL 05-11] Saved: fig3_user_eligibility.png\n",
      "[CELL 05-11] Saved: table1_episode_summary.csv\n",
      "\n",
      "[CELL 05-11] Table 1: Episode Summary\n",
      "    Config Split Episodes Users Eps/User\n",
      " K=5, Q=10 Train   30,895 1,859     16.6\n",
      " K=5, Q=10   Val      258   258      1.0\n",
      " K=5, Q=10  Test      248   248      1.0\n",
      "K=10, Q=20 Train   14,383   597     24.1\n",
      "K=10, Q=20   Val       72    72      1.0\n",
      "K=10, Q=20  Test       85    85      1.0\n",
      "[CELL 05-11] Saved: table2_user_eligibility.csv\n",
      "\n",
      "[CELL 05-11] Table 2: User Eligibility Comparison\n",
      "    Config Split Original Users Eligible Users Eligibility %\n",
      " K=5, Q=10 Train         28,633          1,859          6.5%\n",
      " K=5, Q=10   Val          3,579            258          7.2%\n",
      " K=5, Q=10  Test          3,580            248          6.9%\n",
      "K=10, Q=20 Train         28,633            597          2.1%\n",
      "K=10, Q=20   Val          3,579             72          2.0%\n",
      "K=10, Q=20  Test          3,580             85          2.4%\n",
      "\n",
      "[CELL 05-11] All visualizations saved to /workspace/anonymous-users-mooc-session-meta/reports/05_episode_index_xuetangx/20260130_075221/visualizations\n",
      "[CELL 05-11] elapsed=1.82s\n",
      "[CELL 05-11] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05-11] Visualizations: plots and tables for reporting\n",
    "\n",
    "t0 = cell_start(\"CELL 05-11\", \"Generate plots and tables\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "VIZ_DIR = OUT_DIR / \"visualizations\"\n",
    "VIZ_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"[CELL 05-11] Creating visualizations in {VIZ_DIR}\")\n",
    "\n",
    "# Load episodes for analysis\n",
    "eps_train_k5 = pd.read_parquet(EPISODES_DIR / \"episodes_train_K5_Q10.parquet\")\n",
    "eps_val_k5 = pd.read_parquet(EPISODES_DIR / \"episodes_val_K5_Q10.parquet\")\n",
    "eps_test_k5 = pd.read_parquet(EPISODES_DIR / \"episodes_test_K5_Q10.parquet\")\n",
    "eps_train_k10 = pd.read_parquet(EPISODES_DIR / \"episodes_train_K10_Q20.parquet\")\n",
    "eps_val_k10 = pd.read_parquet(EPISODES_DIR / \"episodes_val_K10_Q20.parquet\")\n",
    "eps_test_k10 = pd.read_parquet(EPISODES_DIR / \"episodes_test_K10_Q20.parquet\")\n",
    "\n",
    "# ===== PLOT 1: Episode counts by split and K-shot config =====\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "configs = ['K=5, Q=10', 'K=10, Q=20']\n",
    "train_counts = [len(eps_train_k5), len(eps_train_k10)]\n",
    "val_counts = [len(eps_val_k5), len(eps_val_k10)]\n",
    "test_counts = [len(eps_test_k5), len(eps_test_k10)]\n",
    "\n",
    "x = range(len(configs))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar([i - width for i in x], train_counts, width, label='Train', color='#2ecc71', alpha=0.7, edgecolor='black')\n",
    "ax.bar(x, val_counts, width, label='Val', color='#3498db', alpha=0.7, edgecolor='black')\n",
    "ax.bar([i + width for i in x], test_counts, width, label='Test', color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.set_ylabel('Number of Episodes', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Episode Counts by Split and K-Shot Config', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(configs, fontsize=10, fontweight='bold')\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (train, val, test) in enumerate(zip(train_counts, val_counts, test_counts)):\n",
    "    ax.text(i - width, train + 1000, f'{train:,}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "    ax.text(i, val + 10, f'{val:,}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "    ax.text(i + width, test + 10, f'{test:,}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / \"fig1_episode_counts.png\", bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"[CELL 05-11] Saved: fig1_episode_counts.png\")\n",
    "\n",
    "# ===== PLOT 2: Episodes per user distribution (K=5, Q=10 train only) =====\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "user_episode_counts = eps_train_k5.groupby('user_id').size()\n",
    "\n",
    "ax.hist(user_episode_counts, bins=50, color='#2ecc71', alpha=0.7, edgecolor='black')\n",
    "ax.axvline(user_episode_counts.median(), color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Median={user_episode_counts.median():.0f}')\n",
    "ax.axvline(user_episode_counts.quantile(0.90), color='orange', linestyle='--', linewidth=2, \n",
    "           label=f'p90={user_episode_counts.quantile(0.90):.0f}')\n",
    "ax.set_xlabel('Episodes per User', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Number of Users', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Train Episodes per User Distribution (K=5, Q=10)', fontsize=12, fontweight='bold')\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / \"fig2_episodes_per_user.png\", bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"[CELL 05-11] Saved: fig2_episodes_per_user.png\")\n",
    "\n",
    "# ===== PLOT 3: User eligibility comparison (original vs filtered) =====\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# K=5, Q=10\n",
    "original_users_k5 = [pairs_train['user_id'].nunique(), pairs_val['user_id'].nunique(), pairs_test['user_id'].nunique()]\n",
    "eligible_users_k5 = [eps_train_k5['user_id'].nunique(), eps_val_k5['user_id'].nunique(), eps_test_k5['user_id'].nunique()]\n",
    "splits = ['Train', 'Val', 'Test']\n",
    "x = range(len(splits))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar([i - width/2 for i in x], original_users_k5, width, label='Original', color='#95a5a6', alpha=0.7, edgecolor='black')\n",
    "ax1.bar([i + width/2 for i in x], eligible_users_k5, width, label='Eligible (>=15 pairs)', color='#2ecc71', alpha=0.7, edgecolor='black')\n",
    "ax1.set_ylabel('Number of Users', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('User Eligibility: K=5, Q=10', fontsize=12, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(splits)\n",
    "ax1.legend(loc='upper right', fontsize=9)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (orig, elig) in enumerate(zip(original_users_k5, eligible_users_k5)):\n",
    "    pct = elig / orig * 100 if orig > 0 else 0\n",
    "    ax1.text(i, max(orig, elig) + 500, f'{elig:,}\\n({pct:.1f}%)', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "# K=10, Q=20\n",
    "original_users_k10 = original_users_k5  # same original users\n",
    "eligible_users_k10 = [eps_train_k10['user_id'].nunique(), eps_val_k10['user_id'].nunique(), eps_test_k10['user_id'].nunique()]\n",
    "\n",
    "ax2.bar([i - width/2 for i in x], original_users_k10, width, label='Original', color='#95a5a6', alpha=0.7, edgecolor='black')\n",
    "ax2.bar([i + width/2 for i in x], eligible_users_k10, width, label='Eligible (>=30 pairs)', color='#3498db', alpha=0.7, edgecolor='black')\n",
    "ax2.set_ylabel('Number of Users', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('User Eligibility: K=10, Q=20', fontsize=12, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(splits)\n",
    "ax2.legend(loc='upper right', fontsize=9)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (orig, elig) in enumerate(zip(original_users_k10, eligible_users_k10)):\n",
    "    pct = elig / orig * 100 if orig > 0 else 0\n",
    "    ax2.text(i, max(orig, elig) + 500, f'{elig:,}\\n({pct:.1f}%)', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / \"fig3_user_eligibility.png\", bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"[CELL 05-11] Saved: fig3_user_eligibility.png\")\n",
    "\n",
    "# ===== TABLE 1: Episode statistics summary =====\n",
    "table1_records = []\n",
    "for stats in all_episode_stats:\n",
    "    rec = {\n",
    "        'Config': f\"K={stats['K']}, Q={stats['Q']}\",\n",
    "        'Split': stats['split'].capitalize(),\n",
    "        'Episodes': f\"{stats['n_episodes']:,}\",\n",
    "        'Users': f\"{stats['n_users']:,}\",\n",
    "    }\n",
    "    if 'episodes_per_user_mean' in stats:\n",
    "        rec['Eps/User'] = f\"{stats['episodes_per_user_mean']:.1f}\"\n",
    "    else:\n",
    "        rec['Eps/User'] = \"1.0\"\n",
    "    table1_records.append(rec)\n",
    "\n",
    "table1 = pd.DataFrame(table1_records)\n",
    "table1.to_csv(VIZ_DIR / \"table1_episode_summary.csv\", index=False)\n",
    "print(f\"[CELL 05-11] Saved: table1_episode_summary.csv\")\n",
    "print(f\"\\n[CELL 05-11] Table 1: Episode Summary\")\n",
    "print(table1.to_string(index=False))\n",
    "\n",
    "# ===== TABLE 2: User eligibility comparison =====\n",
    "table2 = pd.DataFrame([\n",
    "    {\n",
    "        'Config': 'K=5, Q=10',\n",
    "        'Split': 'Train',\n",
    "        'Original Users': f\"{pairs_train['user_id'].nunique():,}\",\n",
    "        'Eligible Users': f\"{eps_train_k5['user_id'].nunique():,}\",\n",
    "        'Eligibility %': f\"{eps_train_k5['user_id'].nunique() / pairs_train['user_id'].nunique() * 100:.1f}%\",\n",
    "    },\n",
    "    {\n",
    "        'Config': 'K=5, Q=10',\n",
    "        'Split': 'Val',\n",
    "        'Original Users': f\"{pairs_val['user_id'].nunique():,}\",\n",
    "        'Eligible Users': f\"{eps_val_k5['user_id'].nunique():,}\",\n",
    "        'Eligibility %': f\"{eps_val_k5['user_id'].nunique() / pairs_val['user_id'].nunique() * 100:.1f}%\",\n",
    "    },\n",
    "    {\n",
    "        'Config': 'K=5, Q=10',\n",
    "        'Split': 'Test',\n",
    "        'Original Users': f\"{pairs_test['user_id'].nunique():,}\",\n",
    "        'Eligible Users': f\"{eps_test_k5['user_id'].nunique():,}\",\n",
    "        'Eligibility %': f\"{eps_test_k5['user_id'].nunique() / pairs_test['user_id'].nunique() * 100:.1f}%\",\n",
    "    },\n",
    "    {\n",
    "        'Config': 'K=10, Q=20',\n",
    "        'Split': 'Train',\n",
    "        'Original Users': f\"{pairs_train['user_id'].nunique():,}\",\n",
    "        'Eligible Users': f\"{eps_train_k10['user_id'].nunique():,}\",\n",
    "        'Eligibility %': f\"{eps_train_k10['user_id'].nunique() / pairs_train['user_id'].nunique() * 100:.1f}%\",\n",
    "    },\n",
    "    {\n",
    "        'Config': 'K=10, Q=20',\n",
    "        'Split': 'Val',\n",
    "        'Original Users': f\"{pairs_val['user_id'].nunique():,}\",\n",
    "        'Eligible Users': f\"{eps_val_k10['user_id'].nunique():,}\",\n",
    "        'Eligibility %': f\"{eps_val_k10['user_id'].nunique() / pairs_val['user_id'].nunique() * 100:.1f}%\",\n",
    "    },\n",
    "    {\n",
    "        'Config': 'K=10, Q=20',\n",
    "        'Split': 'Test',\n",
    "        'Original Users': f\"{pairs_test['user_id'].nunique():,}\",\n",
    "        'Eligible Users': f\"{eps_test_k10['user_id'].nunique():,}\",\n",
    "        'Eligibility %': f\"{eps_test_k10['user_id'].nunique() / pairs_test['user_id'].nunique() * 100:.1f}%\",\n",
    "    },\n",
    "])\n",
    "\n",
    "table2.to_csv(VIZ_DIR / \"table2_user_eligibility.csv\", index=False)\n",
    "print(f\"[CELL 05-11] Saved: table2_user_eligibility.csv\")\n",
    "print(f\"\\n[CELL 05-11] Table 2: User Eligibility Comparison\")\n",
    "print(table2.to_string(index=False))\n",
    "\n",
    "print(f\"\\n[CELL 05-11] All visualizations saved to {VIZ_DIR}\")\n",
    "\n",
    "cell_end(\"CELL 05-11\", t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Notebook 05 Complete\n",
    "\n",
    "**Outputs:**\n",
    "- ✅ Episode files for K=5,Q=10 and K=10,Q=20 (train/val/test)\n",
    "- ✅ DuckDB views: `xuetangx_episodes_{train|val|test}_K{K}_Q{Q}`\n",
    "- ✅ `reports/05_episode_index_xuetangx/<run_tag>/report.json`\n",
    "\n",
    "**Validation Passed:**\n",
    "- ✅ Chronological ordering: support_max_ts < query_min_ts for all episodes\n",
    "- ✅ User filtering: only users with ≥K+Q pairs included\n",
    "- ✅ Train: multiple episodes per user (sliding window)\n",
    "- ✅ Val/Test: single episode per user (last K+Q pairs)\n",
    "\n",
    "**Next:** Notebook 06 (Baselines)\n",
    "- Implement baseline models: Popularity, GRU, SASRec\n",
    "- Train on train episodes, evaluate on val/test episodes\n",
    "- Metrics: Accuracy@1, Recall@{5,10}, MRR\n",
    "- Compare global (non-personalized) vs few-shot adapted models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
