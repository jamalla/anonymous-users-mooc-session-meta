{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 07f: MAML with Residual Meta-Loss and Layer-Specific Adaptation (XuetangX)\n",
    "",
    "This notebook implements **Residual MAML** with three critical fixes:\n",
    "",
    "1. **Higher Inner LR (Î±=0.05)**: Increased from 0.01 based on sweep results\n",
    "2. **Residual Meta-Loss (Î»=0.1)**: Blend adapted and unadapted query losses\n",
    "3. **Layer-Specific Adaptation**: Only adapt embedding + FC layers, freeze GRU\n",
    "",
    "**Key Changes from 07e:**\n",
    "- âœ… Inner learning rate increased to 0.05\n",
    "- âœ… Residual MAML meta-loss with Î»=0.1\n",
    "- âœ… Layer-specific adaptation (freeze GRU)\n",
    "- âœ… Config variable extraction to prevent NameError\n",
    "",
    "**Dataset**: XuetangX MOOC\n",
    "**Model**: GRU-based next-course recommendation\n",
    "**Meta-learning**: MAML (Model-Agnostic Meta-Learning) with FOMAML\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 07e-00] start=2026-01-12T15:11:22\n",
      "[CELL 07e-00] CWD: C:\\anonymous-users-mooc-session-meta\\notebooks\n",
      "[CELL 07e-00] REPO_ROOT: C:\\anonymous-users-mooc-session-meta\n",
      "[CELL 07e-00] META_REGISTRY=C:\\anonymous-users-mooc-session-meta\\meta.json\n",
      "[CELL 07e-00] DATA_INTERIM=C:\\anonymous-users-mooc-session-meta\\data\\interim\n",
      "[CELL 07e-00] DATA_PROCESSED=C:\\anonymous-users-mooc-session-meta\\data\\processed\n",
      "[CELL 07e-00] MODELS=C:\\anonymous-users-mooc-session-meta\\models\n",
      "[CELL 07e-00] RESULTS=C:\\anonymous-users-mooc-session-meta\\results\n",
      "[CELL 07e-00] REPORTS=C:\\anonymous-users-mooc-session-meta\\reports\n",
      "[CELL 07e-00] PyTorch device: cuda\n",
      "[CELL 07e-00] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07f-00] Bootstrap: repo root + paths + logger\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import pickle\n",
    "import hashlib\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "t0 = datetime.now()\n",
    "print(f\"[CELL 07f-00] start={t0.isoformat(timespec='seconds')}\")\n",
    "print(\"[CELL 07f-00] CWD:\", Path.cwd().resolve())\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists():\n",
    "            return p\n",
    "    raise RuntimeError(\"Could not find PROJECT_STATE.md. Open notebook from within the repo.\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "print(\"[CELL 07f-00] REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "PATHS = {\n",
    "    \"META_REGISTRY\": REPO_ROOT / \"meta.json\",\n",
    "    \"DATA_INTERIM\": REPO_ROOT / \"data\" / \"interim\",\n",
    "    \"DATA_PROCESSED\": REPO_ROOT / \"data\" / \"processed\",\n",
    "    \"MODELS\": REPO_ROOT / \"models\",\n",
    "    \"RESULTS\": REPO_ROOT / \"results\",\n",
    "    \"REPORTS\": REPO_ROOT / \"reports\",\n",
    "}\n",
    "for k, v in PATHS.items():\n",
    "    print(f\"[CELL 07f-00] {k}={v}\")\n",
    "\n",
    "def cell_start(cell_id: str, title: str, **kwargs: Any) -> float:\n",
    "    t = time.time()\n",
    "    print(f\"\\n[{cell_id}] {title}\")\n",
    "    print(f\"[{cell_id}] start={datetime.now().isoformat(timespec='seconds')}\")\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    return t\n",
    "\n",
    "def cell_end(cell_id: str, t0: float, **kwargs: Any) -> None:\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    print(f\"[{cell_id}] elapsed={time.time()-t0:.2f}s\")\n",
    "    print(f\"[{cell_id}] done\")\n",
    "\n",
    "# Check GPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[CELL 07f-00] PyTorch device: {DEVICE}\")\n",
    "print(\"[CELL 07f-00] done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-01] Seed everything\n",
      "[CELL 07e-01] start=2026-01-12T15:11:24\n",
      "[CELL 07e-01] seed=20260107\n",
      "[CELL 07e-01] elapsed=0.01s\n",
      "[CELL 07e-01] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07f-01] Reproducibility: seed everything\n",
    "\n",
    "t0 = cell_start(\"CELL 07f-01\", \"Seed everything\")\n",
    "\n",
    "GLOBAL_SEED = 20260107\n",
    "\n",
    "def seed_everything(seed: int) -> None:\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(GLOBAL_SEED)\n",
    "\n",
    "cell_end(\"CELL 07f-01\", t0, seed=GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-02] IO helpers\n",
      "[CELL 07e-02] start=2026-01-12T15:11:26\n",
      "[CELL 07e-02] elapsed=0.00s\n",
      "[CELL 07e-02] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07f-02] JSON/Pickle IO + hashing helpers\n",
    "\n",
    "t0 = cell_start(\"CELL 07f-02\", \"IO helpers\")\n",
    "\n",
    "def write_json_atomic(path: Path, obj: Any, indent: int = 2) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tmp = path.with_suffix(path.suffix + f\".tmp_{uuid.uuid4().hex}\")\n",
    "    with tmp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=indent)\n",
    "    tmp.replace(path)\n",
    "\n",
    "def read_json(path: Path) -> Any:\n",
    "    if not path.exists():\n",
    "        raise RuntimeError(f\"Missing JSON file: {path}\")\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_pickle(path: Path, obj: Any) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_pickle(path: Path) -> Any:\n",
    "    with path.open(\"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "cell_end(\"CELL 07f-02\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-03] Start run + init files\n",
      "[CELL 07e-03] start=2026-01-12T15:11:30\n",
      "[CELL 07e-03] K=5, Q=10\n",
      "[CELL 07e-03] MAML config: Î±=0.01, Î²=0.001, inner_steps=5, meta_batch=32\n",
      "[CELL 07e-03] â­ WARM-START: Initializing from GRU baseline\n",
      "[CELL 07e-03] GRU baseline: C:\\anonymous-users-mooc-session-meta\\models\\baselines\\gru_global.pth\n",
      "[CELL 07e-03] out_dir=C:\\anonymous-users-mooc-session-meta\\reports\\07e_maml_warmstart_xuetangx\\20260112_151130\n",
      "[CELL 07e-03] elapsed=0.02s\n",
      "[CELL 07e-03] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07f-03] Run tagging + config + meta.json\n",
    "",
    "t0 = cell_start(\"CELL 07f-03\", \"Start run + init files\")\n",
    "",
    "NOTEBOOK_NAME = \"07f_maml_residual_xuetangx\"\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_ID = uuid.uuid4().hex\n",
    "",
    "OUT_DIR = PATHS[\"REPORTS\"] / NOTEBOOK_NAME / RUN_TAG\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "",
    "REPORT_PATH = OUT_DIR / \"report.json\"\n",
    "CONFIG_PATH = OUT_DIR / \"config.json\"\n",
    "MANIFEST_PATH = OUT_DIR / \"manifest.json\"\n",
    "",
    "# Paths\n",
    "EPISODES_DIR = PATHS[\"DATA_PROCESSED\"] / \"xuetangx\" / \"episodes\"\n",
    "PAIRS_DIR = PATHS[\"DATA_PROCESSED\"] / \"xuetangx\" / \"pairs\"\n",
    "VOCAB_DIR = PATHS[\"DATA_PROCESSED\"] / \"xuetangx\" / \"vocab\"\n",
    "MODELS_DIR = PATHS[\"MODELS\"] / \"maml\"\n",
    "CHECKPOINTS_DIR = MODELS_DIR / \"checkpoints_warmstart\"\n",
    "RESULTS_DIR = PATHS[\"RESULTS\"]\n",
    "",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "",
    "# GRU baseline checkpoint path\n",
    "GRU_BASELINE_PATH = PATHS[\"MODELS\"] / \"baselines\" / \"gru_global.pth\"\n",
    "",
    "# K-shot config\n",
    "K, Q = 5, 10\n",
    "",
    "CFG = {\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"seed\": GLOBAL_SEED,\n",
    "    \"device\": str(DEVICE),\n",
    "    \"k_shot_config\": {\"K\": K, \"Q\": Q},\n",
    "    \"inputs\": {\n",
    "        \"episodes_train\": str(EPISODES_DIR / f\"episodes_train_K{K}_Q{Q}.parquet\"),\n",
    "        \"episodes_val\": str(EPISODES_DIR / f\"episodes_val_K{K}_Q{Q}.parquet\"),\n",
    "        \"episodes_test\": str(EPISODES_DIR / f\"episodes_test_K{K}_Q{Q}.parquet\"),\n",
    "        \"pairs_train\": str(PAIRS_DIR / \"pairs_train.parquet\"),\n",
    "        \"pairs_val\": str(PAIRS_DIR / \"pairs_val.parquet\"),\n",
    "        \"pairs_test\": str(PAIRS_DIR / \"pairs_test.parquet\"),\n",
    "        \"vocab\": str(VOCAB_DIR / \"course2id.json\"),\n",
    "        \"gru_baseline\": str(GRU_BASELINE_PATH),  # â† WARM-START FROM HERE\n",
    "    },\n",
    "    \"gru_config\": {\n",
    "        \"embedding_dim\": 64,\n",
    "        \"hidden_dim\": 128,\n",
    "        \"num_layers\": 1,\n",
    "        \"dropout\": 0.2,\n",
    "        \"max_seq_len\": 50,\n",
    "    },\n",
    "    \"maml_config\": {\n",
    "        \"inner_lr\": 0.05,           # Î±: INCREASED from 0.01 (Fix #1)           # Î±: learning rate for inner loop (task adaptation)\n",
    "        \"outer_lr\": 0.001,          # Î²: learning rate for outer loop (meta-update)\n",
    "        \"num_inner_steps\": 5,\n",
    "    \"lambda_residual\": 0.1,     # Î»: NEW for Residual MAML (Fix #2)       # number of gradient steps for adaptation\n",
    "        \"meta_batch_size\": 32,      # number of tasks (users) per meta-batch\n",
    "        \"num_meta_iterations\": 10000,  # total meta-training iterations\n",
    "        \"checkpoint_interval\": 1000,   # save checkpoint every N iterations\n",
    "        \"eval_interval\": 500,          # evaluate on val set every N iterations\n",
    "        \"use_second_order\": True,      # True: MAML (2nd order), False: FOMAML (1st order)\n",
    "        \"warm_start\": True,            # â† NEW: Initialize from GRU baseline\n",
    "    },\n",
    "    \"ablation_configs\": {\n",
    "        \"support_set_sizes\": [1, 3, 5, 10],\n",
    "        \"adaptation_steps\": [1, 3, 5, 10],\n",
    "    },\n",
    "    \"metrics\": [\"accuracy@1\", \"recall@5\", \"recall@10\", \"mrr\"],\n",
    "    \"outputs\": {\n",
    "        \"models_dir\": str(MODELS_DIR),\n",
    "        \"checkpoints_dir\": str(CHECKPOINTS_DIR),\n",
    "        \"results\": str(RESULTS_DIR / f\"maml_warmstart_K{K}_Q{Q}.json\"),\n",
    "        \"out_dir\": str(OUT_DIR),\n",
    "    }\n",
    "}\n",
    "",
    "write_json_atomic(CONFIG_PATH, CFG)\n",
    "",
    "report = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"repo_root\": str(REPO_ROOT),\n",
    "    \"metrics\": {},\n",
    "    \"key_findings\": [],\n",
    "    \"sanity_samples\": {},\n",
    "    \"data_fingerprints\": {},\n",
    "    \"notes\": [],\n",
    "}\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "",
    "manifest = {\"run_id\": RUN_ID, \"notebook\": NOTEBOOK_NAME, \"run_tag\": RUN_TAG, \"artifacts\": []}\n",
    "write_json_atomic(MANIFEST_PATH, manifest)\n",
    "",
    "# meta.json\n",
    "META_PATH = PATHS[\"META_REGISTRY\"]\n",
    "if not META_PATH.exists():\n",
    "    write_json_atomic(META_PATH, {\"schema_version\": 1, \"runs\": []})\n",
    "meta = read_json(META_PATH)\n",
    "meta[\"runs\"].append({\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"out_dir\": str(OUT_DIR),\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "})\n",
    "write_json_atomic(META_PATH, meta)\n",
    "",
    "print(f\"[CELL 07f-03] K={K}, Q={Q}\")\n",
    "print(f\"[CELL 07f-03] MAML config: Î±={CFG['maml_config']['inner_lr']}, Î²={CFG['maml_config']['outer_lr']}, \"\n",
    "      f\"inner_steps={CFG['maml_config']['num_inner_steps']}, meta_batch={CFG['maml_config']['meta_batch_size']}\")\n",
    "print(f\"[CELL 07f-03] â­ WARM-START: Initializing from GRU baseline\")\n",
    "print(f\"[CELL 07f-03] GRU baseline: {GRU_BASELINE_PATH}\")\n",
    "",
    "cell_end(\"CELL 07f-03\", t0, out_dir=str(OUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 07f-03b] Extract config variables to prevent NameError in evaluation cells",
    "",
    "t0 = cell_start(\"CELL 07f-03b\", \"Extract config variables\")",
    "",
    "# MAML config",
    "inner_lr = CFG['maml_config']['inner_lr']",
    "num_inner_steps = CFG['maml_config']['num_inner_steps']",
    "outer_lr = CFG['maml_config']['outer_lr']",
    "meta_batch_size = CFG['maml_config']['meta_batch_size']",
    "num_meta_iterations = CFG['maml_config']['num_meta_iterations']",
    "lambda_residual = CFG['maml_config']['lambda_residual']",
    "",
    "# GRU config",
    "embedding_dim = CFG['gru_config']['embedding_dim']",
    "hidden_dim = CFG['gru_config']['hidden_dim']",
    "num_layers = CFG['gru_config']['num_layers']",
    "dropout = CFG['gru_config']['dropout']",
    "max_seq_len = CFG['gru_config']['max_seq_len']",
    "",
    "# K-shot config",
    "K = CFG['k_shot_config']['K']",
    "Q = CFG['k_shot_config']['Q']",
    "",
    "# Loss criterion",
    "criterion = nn.CrossEntropyLoss()",
    "",
    "print(f\"[CELL 07f-03b] Extracted variables:\")",
    "print(f\"  inner_lr (Î±) = {inner_lr} (increased from 0.01)\")",
    "print(f\"  lambda_residual (Î») = {lambda_residual} (new)\")",
    "print(f\"  num_inner_steps = {num_inner_steps}\")",
    "print(f\"  hidden_dim = {hidden_dim}\")",
    "print(f\"  max_seq_len = {max_seq_len}\")",
    "print(f\"  K = {K}, Q = {Q}\")",
    "",
    "cell_end(\"CELL 07f-03b\", t0)",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-04] Load data\n",
      "[CELL 07e-04] start=2026-01-12T15:11:37\n",
      "[CELL 07e-04] Vocabulary: 343 courses\n",
      "[CELL 07e-04] Episodes train: 66,187 episodes (3,006 users)\n",
      "[CELL 07e-04] Episodes val:   340 episodes (340 users)\n",
      "[CELL 07e-04] Episodes test:  346 episodes (346 users)\n",
      "[CELL 07e-04] Pairs train: 212,923 pairs\n",
      "[CELL 07e-04] Pairs val:   24,698 pairs\n",
      "[CELL 07e-04] Pairs test:  26,608 pairs\n",
      "[CELL 07e-04] elapsed=0.39s\n",
      "[CELL 07e-04] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07f-04] Load data: episodes, pairs, vocab\n",
    "\n",
    "t0 = cell_start(\"CELL 07f-04\", \"Load data\")\n",
    "\n",
    "# Vocab\n",
    "course2id = read_json(Path(CFG[\"inputs\"][\"vocab\"]))\n",
    "id2course = {int(v): k for k, v in course2id.items()}\n",
    "n_items = len(course2id)\n",
    "print(f\"[CELL 07f-04] Vocabulary: {n_items} courses\")\n",
    "\n",
    "# Episodes\n",
    "episodes_train = pd.read_parquet(CFG[\"inputs\"][\"episodes_train\"])\n",
    "episodes_val = pd.read_parquet(CFG[\"inputs\"][\"episodes_val\"])\n",
    "episodes_test = pd.read_parquet(CFG[\"inputs\"][\"episodes_test\"])\n",
    "\n",
    "print(f\"[CELL 07f-04] Episodes train: {len(episodes_train):,} episodes ({episodes_train['user_id'].nunique():,} users)\")\n",
    "print(f\"[CELL 07f-04] Episodes val:   {len(episodes_val):,} episodes ({episodes_val['user_id'].nunique():,} users)\")\n",
    "print(f\"[CELL 07f-04] Episodes test:  {len(episodes_test):,} episodes ({episodes_test['user_id'].nunique():,} users)\")\n",
    "\n",
    "# Pairs\n",
    "pairs_train = pd.read_parquet(CFG[\"inputs\"][\"pairs_train\"])\n",
    "pairs_val = pd.read_parquet(CFG[\"inputs\"][\"pairs_val\"])\n",
    "pairs_test = pd.read_parquet(CFG[\"inputs\"][\"pairs_test\"])\n",
    "\n",
    "print(f\"[CELL 07f-04] Pairs train: {len(pairs_train):,} pairs\")\n",
    "print(f\"[CELL 07f-04] Pairs val:   {len(pairs_val):,} pairs\")\n",
    "print(f\"[CELL 07f-04] Pairs test:  {len(pairs_test):,} pairs\")\n",
    "\n",
    "cell_end(\"CELL 07f-04\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-05] Define evaluation metrics\n",
      "[CELL 07e-05] start=2026-01-12T15:11:40\n",
      "[CELL 07e-05] Metrics: accuracy@1, recall@5, recall@10, mrr\n",
      "[CELL 07e-05] elapsed=0.00s\n",
      "[CELL 07e-05] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07f-05] Evaluation metrics (same as Notebook 07)\n",
    "\n",
    "t0 = cell_start(\"CELL 07f-05\", \"Define evaluation metrics\")\n",
    "\n",
    "def compute_metrics(predictions: np.ndarray, labels: np.ndarray, k_values: List[int] = [5, 10]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute ranking metrics.\n",
    "    \n",
    "    Args:\n",
    "        predictions: (n_samples, n_items) score matrix\n",
    "        labels: (n_samples,) true item indices\n",
    "        k_values: list of k for Recall@k\n",
    "    \n",
    "    Returns:\n",
    "        dict with accuracy@1, recall@k, mrr\n",
    "    \"\"\"\n",
    "    n_samples = len(labels)\n",
    "    \n",
    "    # Get top-k predictions (indices)\n",
    "    max_k = max(k_values)\n",
    "    top_k_preds = np.argsort(-predictions, axis=1)[:, :max_k]  # descending order\n",
    "    \n",
    "    # Accuracy@1\n",
    "    top1_preds = top_k_preds[:, 0]\n",
    "    acc1 = (top1_preds == labels).mean()\n",
    "    \n",
    "    # Recall@k\n",
    "    recall_k = {}\n",
    "    for k in k_values:\n",
    "        hits = np.array([labels[i] in top_k_preds[i, :k] for i in range(n_samples)])\n",
    "        recall_k[f\"recall@{k}\"] = hits.mean()\n",
    "    \n",
    "    # MRR (Mean Reciprocal Rank)\n",
    "    ranks = []\n",
    "    for i in range(n_samples):\n",
    "        # Find rank of true label (1-indexed)\n",
    "        rank_idx = np.where(top_k_preds[i] == labels[i])[0]\n",
    "        if len(rank_idx) > 0:\n",
    "            ranks.append(1.0 / (rank_idx[0] + 1))  # reciprocal rank\n",
    "        else:\n",
    "            # Not in top-k, check full ranking\n",
    "            full_rank = np.where(np.argsort(-predictions[i]) == labels[i])[0][0]\n",
    "            ranks.append(1.0 / (full_rank + 1))\n",
    "    mrr = np.mean(ranks)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy@1\": float(acc1),\n",
    "        **{k: float(v) for k, v in recall_k.items()},\n",
    "        \"mrr\": float(mrr),\n",
    "    }\n",
    "\n",
    "print(\"[CELL 07f-05] Metrics: accuracy@1, recall@5, recall@10, mrr\")\n",
    "\n",
    "cell_end(\"CELL 07f-05\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-06] Define GRU model\n",
      "[CELL 07e-06] start=2026-01-12T15:11:45\n",
      "[CELL 07e-06] GRU model defined\n",
      "  - Embedding dim: 64\n",
      "  - Hidden dim: 128\n",
      "  - Num layers: 1\n",
      "[CELL 07e-06] elapsed=0.00s\n",
      "[CELL 07e-06] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07f-06] Define GRU model (exact same as Notebook 06 & 07)\n",
    "\n",
    "t0 = cell_start(\"CELL 07f-06\", \"Define GRU model\")\n",
    "\n",
    "class GRURecommender(nn.Module):\n",
    "    def __init__(self, n_items: int, embedding_dim: int, hidden_dim: int, num_layers: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.n_items = n_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_items, embedding_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim, n_items)\n",
    "    \n",
    "    def forward(self, seq: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            seq: (batch, max_len) padded sequences\n",
    "            lengths: (batch,) actual lengths\n",
    "        Returns:\n",
    "            logits: (batch, n_items)\n",
    "        \"\"\"\n",
    "        # Embed\n",
    "        emb = self.embedding(seq)  # (batch, max_len, embed_dim)\n",
    "        \n",
    "        # Pack for efficiency\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # GRU\n",
    "        _, hidden = self.gru(packed)  # hidden: (num_layers, batch, hidden_dim)\n",
    "        \n",
    "        # Use last layer hidden state\n",
    "        h = hidden[-1]  # (batch, hidden_dim)\n",
    "        \n",
    "        # Predict\n",
    "        logits = self.fc(h)  # (batch, n_items)\n",
    "        return logits\n",
    "\n",
    "print(\"[CELL 07f-06] GRU model defined\")\n",
    "print(f\"  - Embedding dim: {CFG['gru_config']['embedding_dim']}\")\n",
    "print(f\"  - Hidden dim: {CFG['gru_config']['hidden_dim']}\")\n",
    "print(f\"  - Num layers: {CFG['gru_config']['num_layers']}\")\n",
    "\n",
    "cell_end(\"CELL 07f-06\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-07] Initialize meta-model with warm-start\n",
      "[CELL 07e-07] start=2026-01-12T15:11:49\n",
      "[CELL 07e-07] Loading GRU baseline from: gru_global.pth\n",
      "[CELL 07e-07] Loaded directly from checkpoint\n",
      "[CELL 07e-07] GRU baseline Acc@1: 33.73% (from NB 06)\n",
      "\n",
      "[CELL 07e-07] âœ… WARM-START COMPLETE\n",
      "[CELL 07e-07] Meta-model initialized from strong GRU baseline\n",
      "[CELL 07e-07] Model parameters: 140,695\n",
      "[CELL 07e-07] Now will meta-train to make it more adaptable!\n",
      "[CELL 07e-07] elapsed=0.16s\n",
      "[CELL 07e-07] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07f-07] â­ Initialize meta-model with GRU baseline (WARM-START)\n",
    "\n",
    "t0 = cell_start(\"CELL 07f-07\", \"Initialize meta-model with warm-start\")\n",
    "\n",
    "# Create meta-model\n",
    "meta_model = GRURecommender(\n",
    "    n_items=n_items,\n",
    "    embedding_dim=CFG[\"gru_config\"][\"embedding_dim\"],\n",
    "    hidden_dim=CFG[\"gru_config\"][\"hidden_dim\"],\n",
    "    num_layers=CFG[\"gru_config\"][\"num_layers\"],\n",
    "    dropout=CFG[\"gru_config\"][\"dropout\"],\n",
    ").to(DEVICE)\n",
    "\n",
    "# â­ WARM-START: Load GRU baseline checkpoint\n",
    "if not GRU_BASELINE_PATH.exists():\n",
    "    raise FileNotFoundError(f\"GRU baseline not found: {GRU_BASELINE_PATH}\")\n",
    "\n",
    "print(f\"[CELL 07f-07] Loading GRU baseline from: {GRU_BASELINE_PATH.name}\")\n",
    "baseline_checkpoint = torch.load(GRU_BASELINE_PATH, map_location=DEVICE)\n",
    "\n",
    "# Load state dict\n",
    "if \"model_state_dict\" in baseline_checkpoint:\n",
    "    meta_model.load_state_dict(baseline_checkpoint[\"model_state_dict\"])\n",
    "    print(\"[CELL 07f-07] Loaded from 'model_state_dict' key\")\n",
    "else:\n",
    "    meta_model.load_state_dict(baseline_checkpoint)\n",
    "    print(\"[CELL 07f-07] Loaded directly from checkpoint\")\n",
    "\n",
    "# Verify baseline performance\n",
    "if \"metrics\" in baseline_checkpoint:\n",
    "    baseline_acc = baseline_checkpoint[\"metrics\"].get(\"test_accuracy@1\", \"N/A\")\n",
    "    print(f\"[CELL 07f-07] GRU baseline Acc@1: {baseline_acc}\")\n",
    "else:\n",
    "    print(\"[CELL 07f-07] GRU baseline Acc@1: 33.73% (from NB 06)\")\n",
    "\n",
    "print(f\"\\n[CELL 07f-07] âœ… WARM-START COMPLETE\")\n",
    "print(f\"[CELL 07f-07] Meta-model initialized from strong GRU baseline\")\n",
    "print(f\"[CELL 07f-07] Model parameters: {sum(p.numel() for p in meta_model.parameters()):,}\")\n",
    "print(f\"[CELL 07f-07] Now will meta-train to make it more adaptable!\")\n",
    "\n",
    "cell_end(\"CELL 07f-07\", t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¥ Key Difference from Notebook 07\n",
    "\n",
    "**Notebook 07 (Random Init)**:\n",
    "```python\n",
    "meta_model = GRURecommender(...)  # Random initialization\n",
    "# Meta-train from scratch â†’ 30.52% Acc@1\n",
    "```\n",
    "\n",
    "**Notebook 07e (Warm-Start)**:\n",
    "```python\n",
    "meta_model = GRURecommender(...)\n",
    "meta_model.load_state_dict(gru_baseline)  # â† Load GRU baseline (33.73%)\n",
    "# Meta-train from here â†’ Expected: 35-38% Acc@1 âœ…\n",
    "```\n",
    "\n",
    "**Why this works**:\n",
    "- GRU baseline already knows how to recommend courses (33.73%)\n",
    "- MAML meta-training refines it to adapt better to new users\n",
    "- Combines: Strong task initialization + Meta-learned adaptation\n",
    "\n",
    "**From here onwards, the rest of the notebook is IDENTICAL to Notebook 07:**\n",
    "- Same MAML training loop\n",
    "- Same meta-batch sampling\n",
    "- Same inner/outer loop updates\n",
    "- Same evaluation\n",
    "\n",
    "**Only difference**: We started from a better place!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-08] Define helper functions\n",
      "[CELL 07e-08] start=2026-01-12T15:11:54\n",
      "[CELL 07e-08] Helper functions defined\n",
      "[CELL 07e-08] elapsed=0.00s\n",
      "[CELL 07e-08] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07f-08] Helper functions (same as Notebook 07)\n",
    "\n",
    "t0 = cell_start(\"CELL 07f-08\", \"Define helper functions\")\n",
    "\n",
    "def get_episode_data(episode_row, pairs_df):\n",
    "    \"\"\"Extract support and query pairs for an episode.\"\"\"\n",
    "    support_pair_ids = episode_row[\"support_pair_ids\"]\n",
    "    query_pair_ids = episode_row[\"query_pair_ids\"]\n",
    "\n",
    "    support_pairs = pairs_df[pairs_df[\"pair_id\"].isin(support_pair_ids)].sort_values(\"label_ts_epoch\")\n",
    "    query_pairs = pairs_df[pairs_df[\"pair_id\"].isin(query_pair_ids)].sort_values(\"label_ts_epoch\")\n",
    "\n",
    "    return support_pairs, query_pairs\n",
    "\n",
    "def pairs_to_batch(pairs_df, max_len):\n",
    "    \"\"\"Convert pairs to batched tensors.\"\"\"\n",
    "    prefixes = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "\n",
    "    for _, row in pairs_df.iterrows():\n",
    "        prefix = row[\"prefix\"]\n",
    "        if len(prefix) > max_len:\n",
    "            prefix = prefix[-max_len:]\n",
    "        prefixes.append(prefix)\n",
    "        labels.append(row[\"label\"])\n",
    "        lengths.append(len(prefix))\n",
    "\n",
    "    # Pad sequences\n",
    "    max_l = max(lengths)\n",
    "    padded = []\n",
    "    for seq in prefixes:\n",
    "        padded.append(list(seq) + [0] * (max_l - len(seq)))\n",
    "\n",
    "    return (\n",
    "        torch.LongTensor(padded).to(DEVICE),\n",
    "        torch.LongTensor(labels).to(DEVICE),\n",
    "        torch.LongTensor(lengths).to(DEVICE),\n",
    "    )\n",
    "\n",
    "def functional_forward(seq, lengths, params, hidden_dim, n_items):\n",
    "    \"\"\"Functional forward pass using explicit parameters.\"\"\"\n",
    "    batch_size = seq.size(0)\n",
    "    \n",
    "    # 1. Embedding\n",
    "    emb = F.embedding(seq, params[\"embedding.weight\"], padding_idx=0)\n",
    "    \n",
    "    # 2. GRU (manual implementation)\n",
    "    h = torch.zeros(batch_size, hidden_dim, device=seq.device)\n",
    "    w_ih = params[\"gru.weight_ih_l0\"]\n",
    "    w_hh = params[\"gru.weight_hh_l0\"]\n",
    "    b_ih = params[\"gru.bias_ih_l0\"]\n",
    "    b_hh = params[\"gru.bias_hh_l0\"]\n",
    "    \n",
    "    for t in range(emb.size(1)):\n",
    "        x_t = emb[:, t, :]\n",
    "        gi = F.linear(x_t, w_ih, b_ih)\n",
    "        gh = F.linear(h, w_hh, b_hh)\n",
    "        i_r, i_z, i_n = gi.chunk(3, 1)\n",
    "        h_r, h_z, h_n = gh.chunk(3, 1)\n",
    "        \n",
    "        r = torch.sigmoid(i_r + h_r)\n",
    "        z = torch.sigmoid(i_z + h_z)\n",
    "        n = torch.tanh(i_n + r * h_n)\n",
    "        h_new = (1 - z) * n + z * h\n",
    "        \n",
    "        mask = (lengths > t).unsqueeze(1).float()\n",
    "        h = mask * h_new + (1 - mask) * h\n",
    "    \n",
    "    # 3. FC layer\n",
    "    logits = F.linear(h, params[\"fc.weight\"], params[\"fc.bias\"])\n",
    "    return logits\n",
    "\n",
    "print(\"[CELL 07f-08] Helper functions defined\")\n",
    "\n",
    "cell_end(\"CELL 07f-08\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 07f-08b] MAML Evaluation Function (with Layer-Specific Adaptation Support)\n",
    "\n",
    "t0 = cell_start(\"CELL 07f-08b\", \"MAML Evaluation Function\")\n",
    "\n",
    "def evaluate_maml(meta_model, support_loaders, query_loaders, users, device,\n",
    "                  hidden_dim, n_items, inner_lr, num_inner_steps, criterion,\n",
    "                  adaptable_param_names=None):\n",
    "    \"\"\"\n",
    "    Evaluate MAML on validation or test set with optional layer-specific adaptation.\n",
    "\n",
    "    Args:\n",
    "        meta_model: The meta-learned model\n",
    "        support_loaders: Dict of support set loaders per user\n",
    "        query_loaders: Dict of query set loaders per user\n",
    "        users: List of user IDs to evaluate\n",
    "        device: torch device\n",
    "        hidden_dim: GRU hidden dimension\n",
    "        n_items: Number of items (courses)\n",
    "        inner_lr: Inner loop learning rate\n",
    "        num_inner_steps: Number of inner loop gradient steps\n",
    "        criterion: Loss criterion\n",
    "        adaptable_param_names: List of parameter names to adapt (layer-specific).\n",
    "                               If None, adapt all parameters.\n",
    "\n",
    "    Returns:\n",
    "        avg_loss: Average query loss across all users\n",
    "        avg_acc: Average query accuracy across all users\n",
    "    \"\"\"\n",
    "    meta_model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    num_tasks = len(users)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for user_id in users:\n",
    "            # Get support and query sets\n",
    "            support_seq, support_lengths, support_labels = support_loaders[user_id].dataset.tensors\n",
    "            query_seq, query_lengths, query_labels = query_loaders[user_id].dataset.tensors\n",
    "\n",
    "            support_seq = support_seq.to(device)\n",
    "            support_lengths = support_lengths.to(device)\n",
    "            support_labels = support_labels.to(device)\n",
    "            query_seq = query_seq.to(device)\n",
    "            query_lengths = query_lengths.to(device)\n",
    "            query_labels = query_labels.to(device)\n",
    "\n",
    "            # Initialize fast weights\n",
    "            fast_weights = OrderedDict(meta_model.named_parameters())\n",
    "\n",
    "            # Inner loop adaptation\n",
    "            for step in range(num_inner_steps):\n",
    "                support_logits = functional_forward(\n",
    "                    support_seq, support_lengths, fast_weights, hidden_dim, n_items\n",
    "                )\n",
    "                support_loss = criterion(support_logits, support_labels)\n",
    "\n",
    "                # Compute gradients (layer-specific if specified)\n",
    "                if adaptable_param_names is not None:\n",
    "                    # Layer-specific: only adapt specified parameters\n",
    "                    adaptable_params = [fast_weights[name] for name in adaptable_param_names]\n",
    "                else:\n",
    "                    # Adapt all parameters\n",
    "                    adaptable_params = list(fast_weights.values())\n",
    "\n",
    "                grads = torch.autograd.grad(\n",
    "                    support_loss,\n",
    "                    adaptable_params,\n",
    "                    create_graph=False\n",
    "                )\n",
    "\n",
    "                # Update parameters\n",
    "                new_fast_weights = OrderedDict()\n",
    "                grad_idx = 0\n",
    "                for name, param in fast_weights.items():\n",
    "                    if adaptable_param_names is None or name in adaptable_param_names:\n",
    "                        new_fast_weights[name] = param - inner_lr * grads[grad_idx]\n",
    "                        grad_idx += 1\n",
    "                    else:\n",
    "                        new_fast_weights[name] = param\n",
    "                fast_weights = new_fast_weights\n",
    "\n",
    "            # Evaluate on query set\n",
    "            query_logits = functional_forward(\n",
    "                query_seq, query_lengths, fast_weights, hidden_dim, n_items\n",
    "            )\n",
    "            query_loss = criterion(query_logits, query_labels)\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(query_logits, 1)\n",
    "            acc = (predicted == query_labels).float().mean()\n",
    "\n",
    "            total_loss += query_loss.item()\n",
    "            total_acc += acc.item()\n",
    "\n",
    "    avg_loss = total_loss / num_tasks\n",
    "    avg_acc = total_acc / num_tasks\n",
    "\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "print(\"[CELL 07f-08b] MAML evaluation function defined\")\n",
    "print(\"  â€¢ Supports layer-specific adaptation via adaptable_param_names parameter\")\n",
    "print(\"  â€¢ Used during training for validation\")\n",
    "\n",
    "cell_end(\"CELL 07f-08b\", t0)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 07f-09] MAML Training Loop with Residual Meta-Loss and Layer-Specific Adaptation\n",
    "",
    "t0 = cell_start(\"CELL 07f-09\", \"MAML Training Loop (Residual + Layer-Specific)\")\n",
    "",
    "print(f\"[CELL 07f-09] Starting MAML training with THREE FIXES:\")\n",
    "print(f\"  â€¢ [Fix #1] Inner LR (Î±) = {inner_lr} (increased from 0.01)\")\n",
    "print(f\"  â€¢ [Fix #2] Lambda residual (Î») = {lambda_residual} (Residual MAML)\")\n",
    "print(f\"  â€¢ [Fix #3] Layer-specific adaptation (freeze GRU)\")\n",
    "print(f\"  â€¢ Meta batch size = {meta_batch_size}\")\n",
    "print(f\"  â€¢ Meta iterations = {num_meta_iterations}\")\n",
    "print(f\"  â€¢ Num inner steps = {num_inner_steps}\")\n",
    "print()\n",
    "",
    "# [Fix #3] Layer-specific adaptation: Only adapt embedding + FC, freeze GRU\n",
    "adaptable_param_names = ['embedding.weight', 'fc.weight', 'fc.bias']\n",
    "frozen_param_names = ['gru.weight_ih_l0', 'gru.weight_hh_l0', 'gru.bias_ih_l0', 'gru.bias_hh_l0']\n",
    "",
    "print(f\"[Fix #3] Layer-specific adaptation:\")\n",
    "print(f\"  â€¢ Adaptable: {adaptable_param_names}\")\n",
    "print(f\"  â€¢ Frozen (GRU): {frozen_param_names}\")\n",
    "print()\n",
    "",
    "meta_model.train()\n",
    "meta_optimizer = torch.optim.Adam(meta_model.parameters(), lr=outer_lr)\n",
    "",
    "# Training history\n",
    "history = {\n",
    "    'meta_loss': [],\n",
    "    'meta_loss_adapted': [],\n",
    "    'meta_loss_unadapted': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "",
    "for meta_iter in range(num_meta_iterations):\n",
    "    meta_optimizer.zero_grad()\n",
    "    meta_loss_total = 0.0\n",
    "    meta_loss_adapted_total = 0.0\n",
    "    meta_loss_unadapted_total = 0.0\n",
    "",
    "    # Sample meta-batch of tasks\n",
    "    for task_idx in range(meta_batch_size):\n",
    "        # Sample a task (user)\n",
    "        user_id = cold_users[torch.randint(0, len(cold_users), (1,)).item()]\n",
    "",
    "        # Get support and query sets\n",
    "        support_seq, support_lengths, support_labels = cold_support_loaders[user_id].dataset.tensors\n",
    "        query_seq, query_lengths, query_labels = cold_query_loaders[user_id].dataset.tensors\n",
    "",
    "        support_seq = support_seq.to(device)\n",
    "        support_lengths = support_lengths.to(device)\n",
    "        support_labels = support_labels.to(device)\n",
    "        query_seq = query_seq.to(device)\n",
    "        query_lengths = query_lengths.to(device)\n",
    "        query_labels = query_labels.to(device)\n",
    "",
    "        # Initialize fast weights from meta-model\n",
    "        fast_weights = OrderedDict(meta_model.named_parameters())\n",
    "",
    "        # Inner loop: Only adapt embedding + FC layers (Fix #3)\n",
    "        for step in range(num_inner_steps):\n",
    "            support_logits = functional_forward(\n",
    "                support_seq, support_lengths, fast_weights, hidden_dim, n_items\n",
    "            )\n",
    "            support_loss = criterion(support_logits, support_labels)\n",
    "",
    "            # [Fix #3] Only compute gradients for adaptable parameters\n",
    "            adaptable_params = [fast_weights[name] for name in adaptable_param_names]\n",
    "            grads = torch.autograd.grad(\n",
    "                support_loss,\n",
    "                adaptable_params,\n",
    "                create_graph=False  # FOMAML\n",
    "            )\n",
    "",
    "            # Update only adaptable parameters\n",
    "            new_fast_weights = OrderedDict()\n",
    "            grad_idx = 0\n",
    "            for name, param in fast_weights.items():\n",
    "                if name in adaptable_param_names:\n",
    "                    new_fast_weights[name] = param - inner_lr * grads[grad_idx]\n",
    "                    grad_idx += 1\n",
    "                else:\n",
    "                    new_fast_weights[name] = param  # Keep frozen params unchanged\n",
    "            fast_weights = new_fast_weights\n",
    "",
    "        # Outer loop: Compute meta-loss on query set\n",
    "",
    "        # Compute adapted query loss (with fast weights)\n",
    "        query_logits_adapted = functional_forward(\n",
    "            query_seq, query_lengths, fast_weights, hidden_dim, n_items\n",
    "        )\n",
    "        query_loss_adapted = criterion(query_logits_adapted, query_labels)\n",
    "",
    "        # [Fix #2] Residual MAML: Compute unadapted query loss (with original meta-model)\n",
    "        original_params = OrderedDict(meta_model.named_parameters())\n",
    "        query_logits_unadapted = functional_forward(\n",
    "            query_seq, query_lengths, original_params, hidden_dim, n_items\n",
    "        )\n",
    "        query_loss_unadapted = criterion(query_logits_unadapted, query_labels)\n",
    "",
    "        # [Fix #2] Residual MAML meta-loss: (1-Î») * L_adapted + Î» * L_unadapted\n",
    "        task_meta_loss = (1 - lambda_residual) * query_loss_adapted + lambda_residual * query_loss_unadapted\n",
    "",
    "        meta_loss_total = meta_loss_total + task_meta_loss\n",
    "        meta_loss_adapted_total = meta_loss_adapted_total + query_loss_adapted.item()\n",
    "        meta_loss_unadapted_total = meta_loss_unadapted_total + query_loss_unadapted.item()\n",
    "",
    "    # Average meta-loss over meta-batch\n",
    "    meta_loss_avg = meta_loss_total / meta_batch_size\n",
    "",
    "    # Meta-update\n",
    "    meta_loss_avg.backward()\n",
    "    meta_optimizer.step()\n",
    "",
    "    # Record history\n",
    "    history['meta_loss'].append(meta_loss_avg.item())\n",
    "    history['meta_loss_adapted'].append(meta_loss_adapted_total / meta_batch_size)\n",
    "    history['meta_loss_unadapted'].append(meta_loss_unadapted_total / meta_batch_size)\n",
    "",
    "    # Validation every 100 iterations\n",
    "    if (meta_iter + 1) % 100 == 0:\n",
    "        meta_model.eval()\n",
    "        val_loss, val_acc = evaluate_maml(\n",
    "            meta_model, cold_val_support_loaders, cold_val_query_loaders,\n",
    "            cold_val_users, device, hidden_dim, n_items, inner_lr, num_inner_steps,\n",
    "            criterion, adaptable_param_names  # Pass adaptable params for layer-specific eval\n",
    "        )\n",
    "        meta_model.train()\n",
    "",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "",
    "        print(f\"Meta-iter {meta_iter + 1}/{num_meta_iterations} | \"\n",
    "              f\"Meta-loss: {meta_loss_avg.item():.4f} | \"\n",
    "              f\"Adapted: {meta_loss_adapted_total/meta_batch_size:.4f} | \"\n",
    "              f\"Unadapted: {meta_loss_unadapted_total/meta_batch_size:.4f} | \"\n",
    "              f\"Val-loss: {val_loss:.4f} | Val-acc: {val_acc:.4f}\")\n",
    "",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(meta_model.state_dict(), RUN_DIR / 'best_meta_model.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= 20:  # 2000 iterations\n",
    "                print(f\"Early stopping at iteration {meta_iter + 1}\")\n",
    "                break\n",
    "",
    "# Save final model\n",
    "torch.save(meta_model.state_dict(), RUN_DIR / 'final_meta_model.pt')\n",
    "",
    "# Save history\n",
    "with open(RUN_DIR / 'training_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "",
    "print(f\"\\n[CELL 07f-09] Training completed!\")\n",
    "print(f\"  â€¢ Best val loss: {best_val_loss:.4f}\")\n",
    "print(f\"  â€¢ Models saved to {RUN_DIR}\")\n",
    "",
    "cell_end(\"CELL 07f-09\", t0)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained warmstart model from: ../models/maml/maml_warmstart_gru_K5.pth\n",
      "Model has 140,695 parameters\n"
     ]
    }
   ],
   "source": [
    "# Load the trained warmstart model\n",
    "import torch\n",
    "max_seq_len = 50\n",
    "\n",
    "MODEL_PATH = \"../models/maml/maml_warmstart_gru_K5.pth\"\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "meta_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "meta_model.eval()\n",
    "\n",
    "print(f\"Loaded trained warmstart model from: {MODEL_PATH}\")\n",
    "print(f\"Model has {sum(p.numel() for p in meta_model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted config variables:\n",
      "  inner_lr = 0.01\n",
      "  num_inner_steps = 5\n",
      "  hidden_dim = 128\n",
      "  max_seq_len = 50\n",
      "  K = 5, Q = 10\n"
     ]
    }
   ],
   "source": [
    "# Extract config variables from CFG for convenience\n",
    "\n",
    "# MAML config\n",
    "inner_lr = CFG['maml_config']['inner_lr']\n",
    "num_inner_steps = CFG['maml_config']['num_inner_steps']\n",
    "outer_lr = CFG['maml_config']['outer_lr']\n",
    "meta_batch_size = CFG['maml_config']['meta_batch_size']\n",
    "num_meta_iterations = CFG['maml_config']['num_meta_iterations']\n",
    "\n",
    "# GRU config\n",
    "embedding_dim = CFG['gru_config']['embedding_dim']\n",
    "hidden_dim = CFG['gru_config']['hidden_dim']\n",
    "num_layers = CFG['gru_config']['num_layers']\n",
    "dropout = CFG['gru_config']['dropout']\n",
    "max_seq_len = CFG['gru_config']['max_seq_len']\n",
    "\n",
    "# K-shot config\n",
    "K = CFG['k_shot_config']['K']\n",
    "Q = CFG['k_shot_config']['Q']\n",
    "\n",
    "print(f\"Extracted config variables:\")\n",
    "print(f\"  inner_lr = {inner_lr}\")\n",
    "print(f\"  num_inner_steps = {num_inner_steps}\")\n",
    "print(f\"  hidden_dim = {hidden_dim}\")\n",
    "print(f\"  max_seq_len = {max_seq_len}\")\n",
    "print(f\"  K = {K}, Q = {Q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "114cbe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-10] Zero-shot evaluation (K=0)\n",
      "[CELL 07e-10] start=2026-01-12T15:12:17\n",
      "[CELL 07e-10] Evaluating meta-learned model WITHOUT adaptation (zero-shot)...\n",
      "\n",
      "[CELL 07e-10] Zero-shot Results (No Adaptation):\n",
      "  Accuracy@1:  0.2494\n",
      "  Recall@5:    0.4598\n",
      "  Recall@10:   0.5685\n",
      "  MRR:         0.3553\n",
      "[CELL 07e-10] elapsed=2.32s\n",
      "[CELL 07e-10] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07f-10] Meta-testing: Zero-shot (K=0) - no adaptation\n",
    "\n",
    "t0 = cell_start(\"CELL 07f-10\", \"Zero-shot evaluation (K=0)\")\n",
    "max_seq_len = 50\n",
    "\n",
    "print(\"[CELL 07f-10] Evaluating meta-learned model WITHOUT adaptation (zero-shot)...\")\n",
    "\n",
    "meta_model.eval()\n",
    "zeroshot_predictions = []\n",
    "zeroshot_labels = []\n",
    "\n",
    "with torch.no_grad():  # Pure inference, no gradients needed\n",
    "    for _, episode in episodes_test.iterrows():\n",
    "        support_pairs, query_pairs = get_episode_data(episode, pairs_test)\n",
    "\n",
    "        if len(query_pairs) == 0:\n",
    "            continue\n",
    "\n",
    "        # Only use query set (no support set adaptation)\n",
    "        query_seq, query_labels_test, query_lengths = pairs_to_batch(query_pairs, max_seq_len)\n",
    "\n",
    "        # Use original meta-learned model (no adaptation)\n",
    "        query_logits = meta_model(query_seq, query_lengths)\n",
    "        query_probs = torch.softmax(query_logits, dim=-1).cpu().numpy()\n",
    "\n",
    "        zeroshot_predictions.append(query_probs)\n",
    "        zeroshot_labels.extend(query_labels_test.cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "if len(zeroshot_predictions) > 0:\n",
    "    zeroshot_predictions = np.vstack(zeroshot_predictions)\n",
    "    zeroshot_labels = np.array(zeroshot_labels)\n",
    "    zeroshot_metrics = compute_metrics(zeroshot_predictions, zeroshot_labels)\n",
    "\n",
    "    print(f\"\\n[CELL 07f-10] Zero-shot Results (No Adaptation):\")\n",
    "    print(f\"  Accuracy@1:  {zeroshot_metrics['accuracy@1']:.4f}\")\n",
    "    print(f\"  Recall@5:    {zeroshot_metrics['recall@5']:.4f}\")\n",
    "    print(f\"  Recall@10:   {zeroshot_metrics['recall@10']:.4f}\")\n",
    "    print(f\"  MRR:         {zeroshot_metrics['mrr']:.4f}\")\n",
    "else:\n",
    "    print(\"[CELL 07f-10] WARNING: No predictions generated\")\n",
    "    zeroshot_metrics = {}\n",
    "\n",
    "cell_end(\"CELL 07f-10\", t0)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRURecommender(\n",
       "  (embedding): Embedding(343, 64, padding_idx=0)\n",
       "  (gru): GRU(64, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=343, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "checkpoint = torch.load('../models/maml/maml_warmstart_gru_K5.pth', map_location=DEVICE)\n",
    "meta_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "meta_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted config variables:\n",
      "  inner_lr = 0.01\n",
      "  num_inner_steps = 5\n",
      "  hidden_dim = 128\n",
      "  max_seq_len = 50\n",
      "  K = 5, Q = 10\n",
      "  criterion = CrossEntropyLoss\n"
     ]
    }
   ],
   "source": [
    "# Extract config variables and define missing objects\n",
    "\n",
    "# MAML config\n",
    "inner_lr = CFG['maml_config']['inner_lr']\n",
    "num_inner_steps = CFG['maml_config']['num_inner_steps']\n",
    "outer_lr = CFG['maml_config']['outer_lr']\n",
    "meta_batch_size = CFG['maml_config']['meta_batch_size']\n",
    "num_meta_iterations = CFG['maml_config']['num_meta_iterations']\n",
    "\n",
    "# GRU config\n",
    "embedding_dim = CFG['gru_config']['embedding_dim']\n",
    "hidden_dim = CFG['gru_config']['hidden_dim']\n",
    "num_layers = CFG['gru_config']['num_layers']\n",
    "dropout = CFG['gru_config']['dropout']\n",
    "max_seq_len = CFG['gru_config']['max_seq_len']\n",
    "\n",
    "# K-shot config\n",
    "K = CFG['k_shot_config']['K']\n",
    "Q = CFG['k_shot_config']['Q']\n",
    "\n",
    "# Loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Extracted config variables:\")\n",
    "print(f\"  inner_lr = {inner_lr}\")\n",
    "print(f\"  num_inner_steps = {num_inner_steps}\")\n",
    "print(f\"  hidden_dim = {hidden_dim}\")\n",
    "print(f\"  max_seq_len = {max_seq_len}\")\n",
    "print(f\"  K = {K}, Q = {Q}\")\n",
    "print(f\"  criterion = CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-11] Few-shot evaluation (K=5)\n",
      "[CELL 07e-11] start=2026-01-12T16:27:01\n",
      "[CELL 07e-11] Evaluating meta-learned model WITH adaptation (few-shot K=5)...\n",
      "\n",
      "[CELL 07e-11] Few-shot Results (K=5 adaptation):\n",
      "  - Accuracy@1:  0.3165\n",
      "  - Recall@5:    0.5254\n",
      "  - Recall@10:   0.6136\n",
      "  - MRR:         0.4184\n",
      "[CELL 07e-11] elapsed=48.57s\n",
      "[CELL 07e-11] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07f-11] Meta-testing: Few-shot K=5 (with adaptation using functional forward)\n",
    "\n",
    "t0 = cell_start(\"CELL 07f-11\", \"Few-shot evaluation (K=5)\")\n",
    "\n",
    "print(\"[CELL 07f-11] Evaluating meta-learned model WITH adaptation (few-shot K=5)...\")\n",
    "\n",
    "meta_model.eval()\n",
    "fewshot_predictions = []\n",
    "fewshot_labels = []\n",
    "\n",
    "for _, episode in episodes_test.iterrows():\n",
    "    support_pairs, query_pairs = get_episode_data(episode, pairs_test)\n",
    "    \n",
    "    if len(support_pairs) == 0 or len(query_pairs) == 0:\n",
    "        continue\n",
    "    \n",
    "    support_seq, support_labels_test, support_lengths = pairs_to_batch(support_pairs, max_seq_len)\n",
    "    query_seq, query_labels_test, query_lengths = pairs_to_batch(query_pairs, max_seq_len)\n",
    "    \n",
    "    # Adapt using functional forward (consistent with training)\n",
    "    with torch.enable_grad():\n",
    "        # Clone parameters for adaptation\n",
    "        fast_weights_test = OrderedDict()\n",
    "        for name, param in meta_model.named_parameters():\n",
    "            fast_weights_test[name] = param.clone().requires_grad_()\n",
    "        \n",
    "        # Inner loop adaptation\n",
    "        for _ in range(num_inner_steps):\n",
    "            support_logits_test = functional_forward(\n",
    "                support_seq, support_lengths, fast_weights_test, hidden_dim, n_items\n",
    "            )\n",
    "            support_loss_test = criterion(support_logits_test, support_labels_test)\n",
    "            \n",
    "            grads_test = torch.autograd.grad(\n",
    "                support_loss_test,\n",
    "                fast_weights_test.values(),\n",
    "                create_graph=False  # No second-order needed for testing\n",
    "            )\n",
    "            \n",
    "            fast_weights_test = OrderedDict(\n",
    "                (name, param - inner_lr * grad)\n",
    "                for ((name, param), grad) in zip(fast_weights_test.items(), grads_test)\n",
    "            )\n",
    "    \n",
    "    # Evaluate on query (no gradients)\n",
    "    with torch.no_grad():\n",
    "        query_logits_test = functional_forward(\n",
    "            query_seq, query_lengths, fast_weights_test, hidden_dim, n_items\n",
    "        )\n",
    "        probs = torch.softmax(query_logits_test, dim=-1).cpu().numpy()\n",
    "        \n",
    "        fewshot_predictions.append(probs)\n",
    "        fewshot_labels.extend(query_labels_test.cpu().numpy())\n",
    "\n",
    "fewshot_predictions = np.vstack(fewshot_predictions)\n",
    "fewshot_labels = np.array(fewshot_labels)\n",
    "\n",
    "fewshot_metrics = compute_metrics(fewshot_predictions, fewshot_labels)\n",
    "\n",
    "print(f\"\\n[CELL 07f-11] Few-shot Results (K=5 adaptation):\")\n",
    "print(f\"  - Accuracy@1:  {fewshot_metrics['accuracy@1']:.4f}\")\n",
    "print(f\"  - Recall@5:    {fewshot_metrics['recall@5']:.4f}\")\n",
    "print(f\"  - Recall@10:   {fewshot_metrics['recall@10']:.4f}\")\n",
    "print(f\"  - MRR:         {fewshot_metrics['mrr']:.4f}\")\n",
    "\n",
    "cell_end(\"CELL 07f-11\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-12] Ablation: support set size\n",
      "[CELL 07e-12] start=2026-01-12T16:32:22\n",
      "[CELL 07e-12] Ablation Study: Varying support set size K...\n",
      "\n",
      "[CELL 07e-12] Testing with K=1...\n",
      "[CELL 07e-12] K=1: Acc@1=0.2633, Recall@5=0.4818, MRR=0.3713\n",
      "\n",
      "[CELL 07e-12] Testing with K=3...\n",
      "[CELL 07e-12] K=3: Acc@1=0.3029, Recall@5=0.5153, MRR=0.4072\n",
      "\n",
      "[CELL 07e-12] Testing with K=5...\n",
      "[CELL 07e-12] K=5: Acc@1=0.3165, Recall@5=0.5254, MRR=0.4184\n",
      "\n",
      "[CELL 07e-12] Testing with K=10...\n",
      "\n",
      "[CELL 07e-12] Ablation complete: tested K âˆˆ [1, 3, 5, 10]\n",
      "[CELL 07e-12] elapsed=128.32s\n",
      "[CELL 07e-12] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07f-12] Ablation Study 1: Support set size (K=1,3,5,10) - functional forward\n",
    "\n",
    "t0 = cell_start(\"CELL 07f-12\", \"Ablation: support set size\")\n",
    "\n",
    "print(\"[CELL 07f-12] Ablation Study: Varying support set size K...\")\n",
    "\n",
    "support_sizes = CFG[\"ablation_configs\"][\"support_set_sizes\"]\n",
    "ablation_support_results = {}\n",
    "\n",
    "meta_model.eval()\n",
    "\n",
    "for K_test in support_sizes:\n",
    "    print(f\"\\n[CELL 07f-12] Testing with K={K_test}...\")\n",
    "    \n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, episode in episodes_test.iterrows():\n",
    "        support_pairs, query_pairs = get_episode_data(episode, pairs_test)\n",
    "        \n",
    "        if len(support_pairs) < K_test or len(query_pairs) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Use only K_test support pairs\n",
    "        support_pairs_k = support_pairs.head(K_test)\n",
    "        \n",
    "        support_seq, support_labels_abl, support_lengths = pairs_to_batch(support_pairs_k, max_seq_len)\n",
    "        query_seq, query_labels_abl, query_lengths = pairs_to_batch(query_pairs, max_seq_len)\n",
    "        \n",
    "        # Adapt using functional forward\n",
    "        with torch.enable_grad():\n",
    "            fast_weights_abl = OrderedDict()\n",
    "            for name, param in meta_model.named_parameters():\n",
    "                fast_weights_abl[name] = param.clone().requires_grad_()\n",
    "            \n",
    "            for _ in range(num_inner_steps):\n",
    "                support_logits_abl = functional_forward(\n",
    "                    support_seq, support_lengths, fast_weights_abl, hidden_dim, n_items\n",
    "                )\n",
    "                support_loss_abl = criterion(support_logits_abl, support_labels_abl)\n",
    "                \n",
    "                grads_abl = torch.autograd.grad(\n",
    "                    support_loss_abl,\n",
    "                    fast_weights_abl.values(),\n",
    "                    create_graph=False\n",
    "                )\n",
    "                \n",
    "                fast_weights_abl = OrderedDict(\n",
    "                    (name, param - inner_lr * grad)\n",
    "                    for ((name, param), grad) in zip(fast_weights_abl.items(), grads_abl)\n",
    "                )\n",
    "        \n",
    "        # Evaluate on query\n",
    "        with torch.no_grad():\n",
    "            query_logits_abl = functional_forward(\n",
    "                query_seq, query_lengths, fast_weights_abl, hidden_dim, n_items\n",
    "            )\n",
    "            probs = torch.softmax(query_logits_abl, dim=-1).cpu().numpy()\n",
    "            \n",
    "            predictions.append(probs)\n",
    "            labels.extend(query_labels_abl.cpu().numpy())\n",
    "    \n",
    "    if len(predictions) > 0:\n",
    "        predictions = np.vstack(predictions)\n",
    "        labels = np.array(labels)\n",
    "        metrics = compute_metrics(predictions, labels)\n",
    "        ablation_support_results[K_test] = metrics\n",
    "        \n",
    "        print(f\"[CELL 07f-12] K={K_test}: Acc@1={metrics['accuracy@1']:.4f}, \"\n",
    "              f\"Recall@5={metrics['recall@5']:.4f}, MRR={metrics['mrr']:.4f}\")\n",
    "\n",
    "print(f\"\\n[CELL 07f-12] Ablation complete: tested K âˆˆ {support_sizes}\")\n",
    "\n",
    "cell_end(\"CELL 07f-12\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-13] Ablation: adaptation steps\n",
      "[CELL 07e-13] start=2026-01-12T16:35:24\n",
      "[CELL 07e-13] Ablation Study: Varying adaptation steps...\n",
      "\n",
      "[CELL 07e-13] Testing with 1 adaptation steps...\n",
      "[CELL 07e-13] Steps=1: Acc@1=0.2884, Recall@5=0.4916, MRR=0.3885\n",
      "\n",
      "[CELL 07e-13] Testing with 3 adaptation steps...\n",
      "[CELL 07e-13] Steps=3: Acc@1=0.3064, Recall@5=0.5110, MRR=0.4084\n",
      "\n",
      "[CELL 07e-13] Testing with 5 adaptation steps...\n",
      "[CELL 07e-13] Steps=5: Acc@1=0.3165, Recall@5=0.5254, MRR=0.4184\n",
      "\n",
      "[CELL 07e-13] Testing with 10 adaptation steps...\n",
      "[CELL 07e-13] Steps=10: Acc@1=0.3246, Recall@5=0.5358, MRR=0.4301\n",
      "\n",
      "[CELL 07e-13] Ablation complete: tested adaptation steps âˆˆ [1, 3, 5, 10]\n",
      "[CELL 07e-13] elapsed=178.84s\n",
      "[CELL 07e-13] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07f-13] Ablation Study 2: Adaptation steps (1,3,5,10) - functional forward\n",
    "\n",
    "t0 = cell_start(\"CELL 07f-13\", \"Ablation: adaptation steps\")\n",
    "\n",
    "print(\"[CELL 07f-13] Ablation Study: Varying adaptation steps...\")\n",
    "\n",
    "adaptation_steps = CFG[\"ablation_configs\"][\"adaptation_steps\"]\n",
    "ablation_steps_results = {}\n",
    "\n",
    "meta_model.eval()\n",
    "\n",
    "for num_steps in adaptation_steps:\n",
    "    print(f\"\\n[CELL 07f-13] Testing with {num_steps} adaptation steps...\")\n",
    "    \n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, episode in episodes_test.iterrows():\n",
    "        support_pairs, query_pairs = get_episode_data(episode, pairs_test)\n",
    "        \n",
    "        if len(support_pairs) == 0 or len(query_pairs) == 0:\n",
    "            continue\n",
    "        \n",
    "        support_seq, support_labels_steps, support_lengths = pairs_to_batch(support_pairs, max_seq_len)\n",
    "        query_seq, query_labels_steps, query_lengths = pairs_to_batch(query_pairs, max_seq_len)\n",
    "        \n",
    "        # Adapt using functional forward with varying steps\n",
    "        with torch.enable_grad():\n",
    "            fast_weights_steps = OrderedDict()\n",
    "            for name, param in meta_model.named_parameters():\n",
    "                fast_weights_steps[name] = param.clone().requires_grad_()\n",
    "            \n",
    "            for _ in range(num_steps):  # Use num_steps instead of num_inner_steps\n",
    "                support_logits_steps = functional_forward(\n",
    "                    support_seq, support_lengths, fast_weights_steps, hidden_dim, n_items\n",
    "                )\n",
    "                support_loss_steps = criterion(support_logits_steps, support_labels_steps)\n",
    "                \n",
    "                grads_steps = torch.autograd.grad(\n",
    "                    support_loss_steps,\n",
    "                    fast_weights_steps.values(),\n",
    "                    create_graph=False\n",
    "                )\n",
    "                \n",
    "                fast_weights_steps = OrderedDict(\n",
    "                    (name, param - inner_lr * grad)\n",
    "                    for ((name, param), grad) in zip(fast_weights_steps.items(), grads_steps)\n",
    "                )\n",
    "        \n",
    "        # Evaluate on query\n",
    "        with torch.no_grad():\n",
    "            query_logits_steps = functional_forward(\n",
    "                query_seq, query_lengths, fast_weights_steps, hidden_dim, n_items\n",
    "            )\n",
    "            probs = torch.softmax(query_logits_steps, dim=-1).cpu().numpy()\n",
    "            \n",
    "            predictions.append(probs)\n",
    "            labels.extend(query_labels_steps.cpu().numpy())\n",
    "    \n",
    "    if len(predictions) > 0:\n",
    "        predictions = np.vstack(predictions)\n",
    "        labels = np.array(labels)\n",
    "        metrics = compute_metrics(predictions, labels)\n",
    "        ablation_steps_results[num_steps] = metrics\n",
    "        \n",
    "        print(f\"[CELL 07f-13] Steps={num_steps}: Acc@1={metrics['accuracy@1']:.4f}, \"\n",
    "              f\"Recall@5={metrics['recall@5']:.4f}, MRR={metrics['mrr']:.4f}\")\n",
    "\n",
    "print(f\"\\n[CELL 07f-13] Ablation complete: tested adaptation steps âˆˆ {adaptation_steps}\")\n",
    "\n",
    "cell_end(\"CELL 07f-13\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-14] Parameter update analysis\n",
      "[CELL 07e-14] start=2026-01-12T16:42:44\n",
      "[CELL 07e-14] Analyzing parameter updates during adaptation...\n",
      "[CELL 07e-14] Parameter changes after 5 adaptation steps:\n",
      "Parameter                            Before        After       Change   Change %\n",
      "--------------------------------------------------------------------------------\n",
      "embedding.weight                   162.5457     162.5457       0.0000      0.00%\n",
      "gru.weight_ih_l0                    34.0514      34.0516       0.0003      0.00%\n",
      "gru.weight_hh_l0                    43.4369      43.4365      -0.0004     -0.00%\n",
      "gru.bias_ih_l0                      11.2469      11.2472       0.0003      0.00%\n",
      "gru.bias_hh_l0                      11.9787      11.9790       0.0003      0.00%\n",
      "fc.weight                           52.0300      52.0309       0.0010      0.00%\n",
      "fc.bias                              3.6749       3.6751       0.0002      0.00%\n",
      "[CELL 07e-14] elapsed=0.21s\n",
      "[CELL 07e-14] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07f-14] Parameter update analysis\n",
    "\n",
    "t0 = cell_start(\"CELL 07f-14\", \"Parameter update analysis\")\n",
    "\n",
    "# Set to train mode for gradient computation\n",
    "meta_model.train()\n",
    "\n",
    "print(\"[CELL 07f-14] Analyzing parameter updates during adaptation...\")\n",
    "\n",
    "# Select one test episode for analysis\n",
    "sample_episode = episodes_test.iloc[0]\n",
    "support_pairs, query_pairs = get_episode_data(sample_episode, pairs_test)\n",
    "\n",
    "if len(support_pairs) > 0:\n",
    "    support_seq, support_labels_viz, support_lengths = pairs_to_batch(support_pairs, max_seq_len)\n",
    "    \n",
    "    # Get original parameters (before adaptation)\n",
    "    param_norms_before = {}\n",
    "    for name, param in meta_model.named_parameters():\n",
    "        param_norms_before[name] = param.data.norm().item()\n",
    "    \n",
    "    # Adapt using functional forward (no in-place operations)\n",
    "    with torch.enable_grad():\n",
    "        fast_weights_viz = OrderedDict()\n",
    "        for name, param in meta_model.named_parameters():\n",
    "            fast_weights_viz[name] = param.clone().requires_grad_(True)\n",
    "        \n",
    "        for _ in range(num_inner_steps):\n",
    "            support_logits_viz = functional_forward(\n",
    "                support_seq, support_lengths, fast_weights_viz, hidden_dim, n_items\n",
    "            )\n",
    "            support_loss_viz = criterion(support_logits_viz, support_labels_viz)\n",
    "            \n",
    "            grads_viz = torch.autograd.grad(\n",
    "                support_loss_viz,\n",
    "                fast_weights_viz.values(),\n",
    "                create_graph=False\n",
    "            )\n",
    "            \n",
    "            # FIXED: Create new dict instead of modifying in-place\n",
    "            new_fast_weights = OrderedDict()\n",
    "            for (name, param), grad in zip(fast_weights_viz.items(), grads_viz):\n",
    "                new_fast_weights[name] = param - inner_lr * grad\n",
    "            fast_weights_viz = new_fast_weights\n",
    "    \n",
    "    # Compute parameter changes\n",
    "    param_norms_after = {}\n",
    "    param_changes = {}\n",
    "    \n",
    "    for name in fast_weights_viz.keys():\n",
    "        adapted_norm = fast_weights_viz[name].data.norm().item()\n",
    "        original_norm = param_norms_before[name]\n",
    "        change = adapted_norm - original_norm\n",
    "        \n",
    "        param_norms_after[name] = adapted_norm\n",
    "        param_changes[name] = {\n",
    "            \"before\": original_norm,\n",
    "            \"after\": adapted_norm,\n",
    "            \"change\": change,\n",
    "            \"change_pct\": (change / original_norm * 100) if original_norm > 0 else 0,\n",
    "        }\n",
    "    \n",
    "    print(f\"[CELL 07f-14] Parameter changes after {num_inner_steps} adaptation steps:\")\n",
    "    print(f\"{'Parameter':<30} {'Before':>12} {'After':>12} {'Change':>12} {'Change %':>10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for name, stats in list(param_changes.items())[:10]:  # Show first 10\n",
    "        print(f\"{name:<30} {stats['before']:>12.4f} {stats['after']:>12.4f} \"\n",
    "              f\"{stats['change']:>12.4f} {stats['change_pct']:>9.2f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"[CELL 07f-14] WARNING: No support pairs for visualization\")\n",
    "\n",
    "\n",
    "# Reset to eval mode\n",
    "meta_model.eval()\n",
    "cell_end(\"CELL 07f-14\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created placeholder training_history for results summary\n"
     ]
    }
   ],
   "source": [
    "# Create minimal training_history for results summary\n",
    "# (The actual training was already completed, this is just for the final report)\n",
    "training_history = {\n",
    "    \"meta_iterations\": [],\n",
    "    \"meta_train_loss\": [],\n",
    "    \"val_accuracy\": [],\n",
    "    \"val_iterations\": [],\n",
    "}\n",
    "\n",
    "print(\"Created placeholder training_history for results summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-15] Results summary\n",
      "[CELL 07e-15] start=2026-01-12T16:44:02\n",
      "\n",
      "[CELL 07e-15] ========== RESULTS SUMMARY (Test Set) ==========\n",
      "K=5, Q=10 | Test Episodes: 346\n",
      "\n",
      "Model                               Acc@1   Recall@5  Recall@10        MRR\n",
      "------------------------------------------------------------------------\n",
      "GRU (Baseline - 06)                0.3373     0.5590     0.6575     0.4438\n",
      "MAML Zero-shot                     0.2494     0.4598     0.5685     0.3553\n",
      "MAML Few-shot (K=5)                0.3165     0.5254     0.6136     0.4184\n",
      "\n",
      "[CELL 07e-15] MAML Few-shot improvement over GRU baseline: -6.17%\n",
      "\n",
      "[CELL 07e-15] ========== ABLATION STUDY 1: Support Set Size ==========\n",
      "K (Support Size)          Acc@1   Recall@5  Recall@10        MRR\n",
      "--------------------------------------------------------------\n",
      "1                        0.2633     0.4818     0.5829     0.3713\n",
      "3                        0.3029     0.5153     0.6092     0.4072\n",
      "5                        0.3165     0.5254     0.6136     0.4184\n",
      "\n",
      "[CELL 07e-15] ========== ABLATION STUDY 2: Adaptation Steps ==========\n",
      "Adaptation Steps          Acc@1   Recall@5  Recall@10        MRR\n",
      "--------------------------------------------------------------\n",
      "1                        0.2884     0.4916     0.5899     0.3885\n",
      "3                        0.3064     0.5110     0.6040     0.4084\n",
      "5                        0.3165     0.5254     0.6136     0.4184\n",
      "10                       0.3246     0.5358     0.6303     0.4301\n",
      "\n",
      "[CELL 07e-15] Saved: maml_warmstart_K5_Q10.json\n",
      "[CELL 07e-15] elapsed=0.02s\n",
      "[CELL 07e-15] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07f-15] Results summary table + comparison with baselines\n",
    "\n",
    "t0 = cell_start(\"CELL 07f-15\", \"Results summary\")\n",
    "\n",
    "print(\"\\n[CELL 07f-15] ========== RESULTS SUMMARY (Test Set) ==========\")\n",
    "print(f\"K={K}, Q={Q} | Test Episodes: {len(episodes_test):,}\\n\")\n",
    "\n",
    "# Load GRU baseline results for comparison\n",
    "baseline_results_path = RESULTS_DIR / f\"baselines_K{K}_Q{Q}.json\"\n",
    "if baseline_results_path.exists():\n",
    "    baseline_results = read_json(baseline_results_path)\n",
    "    gru_baseline_metrics = baseline_results[\"baselines\"][\"gru_global\"]\n",
    "else:\n",
    "    gru_baseline_metrics = {\"accuracy@1\": 0.3373, \"recall@5\": 0.5590, \"recall@10\": 0.6575, \"mrr\": 0.4437}\n",
    "\n",
    "# Create comparison table\n",
    "print(f\"{'Model':<30} {'Acc@1':>10} {'Recall@5':>10} {'Recall@10':>10} {'MRR':>10}\")\n",
    "print(\"-\" * 72)\n",
    "\n",
    "# Baselines\n",
    "print(f\"{'GRU (Baseline - 06)':<30} {gru_baseline_metrics['accuracy@1']:>10.4f} \"\n",
    "      f\"{gru_baseline_metrics['recall@5']:>10.4f} {gru_baseline_metrics['recall@10']:>10.4f} \"\n",
    "      f\"{gru_baseline_metrics['mrr']:>10.4f}\")\n",
    "\n",
    "# MAML results\n",
    "print(f\"{'MAML Zero-shot':<30} {zeroshot_metrics['accuracy@1']:>10.4f} \"\n",
    "      f\"{zeroshot_metrics['recall@5']:>10.4f} {zeroshot_metrics['recall@10']:>10.4f} \"\n",
    "      f\"{zeroshot_metrics['mrr']:>10.4f}\")\n",
    "\n",
    "print(f\"{'MAML Few-shot (K=5)':<30} {fewshot_metrics['accuracy@1']:>10.4f} \"\n",
    "      f\"{fewshot_metrics['recall@5']:>10.4f} {fewshot_metrics['recall@10']:>10.4f} \"\n",
    "      f\"{fewshot_metrics['mrr']:>10.4f}\")\n",
    "\n",
    "# Improvement over baseline\n",
    "improvement = (fewshot_metrics['accuracy@1'] - gru_baseline_metrics['accuracy@1']) / gru_baseline_metrics['accuracy@1'] * 100\n",
    "print(f\"\\n[CELL 07f-15] MAML Few-shot improvement over GRU baseline: {improvement:+.2f}%\")\n",
    "\n",
    "# Ablation results\n",
    "print(f\"\\n[CELL 07f-15] ========== ABLATION STUDY 1: Support Set Size ==========\")\n",
    "print(f\"{'K (Support Size)':<20} {'Acc@1':>10} {'Recall@5':>10} {'Recall@10':>10} {'MRR':>10}\")\n",
    "print(\"-\" * 62)\n",
    "for K_test, metrics in ablation_support_results.items():\n",
    "    print(f\"{K_test:<20} {metrics['accuracy@1']:>10.4f} {metrics['recall@5']:>10.4f} \"\n",
    "          f\"{metrics['recall@10']:>10.4f} {metrics['mrr']:>10.4f}\")\n",
    "\n",
    "print(f\"\\n[CELL 07f-15] ========== ABLATION STUDY 2: Adaptation Steps ==========\")\n",
    "print(f\"{'Adaptation Steps':<20} {'Acc@1':>10} {'Recall@5':>10} {'Recall@10':>10} {'MRR':>10}\")\n",
    "print(\"-\" * 62)\n",
    "for num_steps, metrics in ablation_steps_results.items():\n",
    "    print(f\"{num_steps:<20} {metrics['accuracy@1']:>10.4f} {metrics['recall@5']:>10.4f} \"\n",
    "          f\"{metrics['recall@10']:>10.4f} {metrics['mrr']:>10.4f}\")\n",
    "\n",
    "# Save all results\n",
    "all_results = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"k_shot_config\": {\"K\": K, \"Q\": Q},\n",
    "    \"n_test_episodes\": len(episodes_test),\n",
    "    \"baseline\": {\n",
    "        \"gru_global\": gru_baseline_metrics,\n",
    "    },\n",
    "    \"maml\": {\n",
    "        \"zero_shot\": zeroshot_metrics,\n",
    "        \"few_shot_K5\": fewshot_metrics,\n",
    "    },\n",
    "    \"ablation_support_size\": ablation_support_results,\n",
    "    \"ablation_adaptation_steps\": ablation_steps_results,\n",
    "    \"improvement_over_baseline_pct\": improvement,\n",
    "    \"training_history\": training_history,\n",
    "}\n",
    "\n",
    "results_path = Path(CFG[\"outputs\"][\"results\"])\n",
    "write_json_atomic(results_path, all_results)\n",
    "print(f\"\\n[CELL 07f-15] Saved: {results_path.name}\")\n",
    "\n",
    "cell_end(\"CELL 07f-15\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_model_path: C:\\anonymous-users-mooc-session-meta\\models\\maml\\maml_warmstart_gru_K5.pth\n"
     ]
    }
   ],
   "source": [
    "# Define final_model_path (points to the trained warmstart model)\n",
    "final_model_path = MODELS_DIR / f\"maml_warmstart_gru_K{K}.pth\"\n",
    "\n",
    "print(f\"final_model_path: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 07e-16] Write report + manifest\n",
      "[CELL 07e-16] start=2026-01-12T16:45:02\n",
      "[CELL 07e-16] Updated: C:\\anonymous-users-mooc-session-meta\\reports\\07e_maml_warmstart_xuetangx\\20260112_151130\\report.json\n",
      "[CELL 07e-16] Updated: C:\\anonymous-users-mooc-session-meta\\reports\\07e_maml_warmstart_xuetangx\\20260112_151130\\manifest.json\n",
      "[CELL 07e-16] elapsed=0.52s\n",
      "[CELL 07e-16] done\n",
      "\n",
      "================================================================================\n",
      "âœ… NOTEBOOK 07 COMPLETE\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Key Results:\n",
      "  - GRU Baseline (06):        0.3373 Acc@1\n",
      "  - MAML Zero-shot:           0.2494 Acc@1\n",
      "  - MAML Few-shot (K=5):      0.3165 Acc@1\n",
      "  - Improvement:              -6.17%\n",
      "\n",
      "ðŸ“ Outputs:\n",
      "  - Meta-model: C:\\anonymous-users-mooc-session-meta\\models\\maml\\maml_warmstart_gru_K5.pth\n",
      "  - Results:    C:\\anonymous-users-mooc-session-meta\\results\\maml_warmstart_K5_Q10.json\n",
      "  - Report:     C:\\anonymous-users-mooc-session-meta\\reports\\07e_maml_warmstart_xuetangx\\20260112_151130\\report.json\n",
      "\n",
      "ðŸŽ¯ Next Steps:\n",
      "  - Fine-tune hyperparameters (Î±, Î², inner steps)\n",
      "  - Try different architectures (Transformer, GNN)\n",
      "  - Compare with other meta-learning methods (ProtoNet, Matching Networks)\n"
     ]
    }
   ],
   "source": [
    "# [CELL 07f-16] Update report + manifest\n",
    "\n",
    "t0 = cell_start(\"CELL 07f-16\", \"Write report + manifest\")\n",
    "\n",
    "report = read_json(REPORT_PATH)\n",
    "manifest = read_json(MANIFEST_PATH)\n",
    "\n",
    "# Metrics\n",
    "report[\"metrics\"] = {\n",
    "    \"n_test_episodes\": len(episodes_test),\n",
    "    \"gru_baseline_acc1\": gru_baseline_metrics['accuracy@1'],\n",
    "    \"maml_zero_shot_acc1\": zeroshot_metrics['accuracy@1'],\n",
    "    \"maml_few_shot_K5_acc1\": fewshot_metrics['accuracy@1'],\n",
    "    \"improvement_over_baseline_pct\": improvement,\n",
    "    \"training_iterations\": num_meta_iterations,\n",
    "}\n",
    "\n",
    "# Key findings\n",
    "report[\"key_findings\"].extend([\n",
    "    f\"MAML meta-training: {num_meta_iterations:,} iterations with {meta_batch_size} tasks/batch\",\n",
    "    f\"Zero-shot performance (no adaptation): Acc@1={zeroshot_metrics['accuracy@1']:.4f}\",\n",
    "    f\"Few-shot performance (K=5 adaptation): Acc@1={fewshot_metrics['accuracy@1']:.4f}\",\n",
    "    f\"Improvement over GRU baseline: {improvement:+.2f}% ({fewshot_metrics['accuracy@1']:.4f} vs {gru_baseline_metrics['accuracy@1']:.4f})\",\n",
    "    f\"Ablation: Best K={max(ablation_support_results, key=lambda k: ablation_support_results[k]['accuracy@1'])} \"\n",
    "    f\"(Acc@1={max(ablation_support_results.values(), key=lambda m: m['accuracy@1'])['accuracy@1']:.4f})\",\n",
    "    f\"Ablation: Best adaptation steps={max(ablation_steps_results, key=lambda k: ablation_steps_results[k]['accuracy@1'])} \"\n",
    "    f\"(Acc@1={max(ablation_steps_results.values(), key=lambda m: m['accuracy@1'])['accuracy@1']:.4f})\",\n",
    "])\n",
    "\n",
    "# Sanity samples\n",
    "report[\"sanity_samples\"][\"maml_config\"] = CFG[\"maml_config\"]\n",
    "report[\"sanity_samples\"][\"sample_episode\"] = {\n",
    "    \"episode_id\": int(episodes_test.iloc[0][\"episode_id\"]),\n",
    "    \"user_id\": str(episodes_test.iloc[0][\"user_id\"]),\n",
    "    \"n_support_pairs\": len(episodes_test.iloc[0][\"support_pair_ids\"]),\n",
    "    \"n_query_pairs\": len(episodes_test.iloc[0][\"query_pair_ids\"]),\n",
    "}\n",
    "\n",
    "# Fingerprints\n",
    "report[\"data_fingerprints\"][\"meta_model\"] = {\n",
    "    \"path\": str(final_model_path),\n",
    "    \"bytes\": int(final_model_path.stat().st_size),\n",
    "    \"sha256\": sha256_file(final_model_path),\n",
    "}\n",
    "\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "# Manifest\n",
    "def add_artifact(path: Path) -> None:\n",
    "    rec = {\"path\": str(path), \"bytes\": int(path.stat().st_size), \"sha256\": None, \"sha256_error\": None}\n",
    "    try:\n",
    "        rec[\"sha256\"] = sha256_file(path)\n",
    "    except Exception as e:\n",
    "        rec[\"sha256_error\"] = str(e)\n",
    "    manifest[\"artifacts\"].append(rec)\n",
    "\n",
    "add_artifact(final_model_path)\n",
    "add_artifact(results_path)\n",
    "\n",
    "# Add checkpoints\n",
    "for checkpoint_file in sorted(CHECKPOINTS_DIR.glob(\"checkpoint_iter*.pth\")):\n",
    "    add_artifact(checkpoint_file)\n",
    "\n",
    "write_json_atomic(MANIFEST_PATH, manifest)\n",
    "\n",
    "print(f\"[CELL 07f-16] Updated: {REPORT_PATH}\")\n",
    "print(f\"[CELL 07f-16] Updated: {MANIFEST_PATH}\")\n",
    "\n",
    "cell_end(\"CELL 07f-16\", t0)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"âœ… NOTEBOOK 07 COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nðŸ“Š Key Results:\")\n",
    "print(f\"  - GRU Baseline (06):        {gru_baseline_metrics['accuracy@1']:.4f} Acc@1\")\n",
    "print(f\"  - MAML Zero-shot:           {zeroshot_metrics['accuracy@1']:.4f} Acc@1\")\n",
    "print(f\"  - MAML Few-shot (K=5):      {fewshot_metrics['accuracy@1']:.4f} Acc@1\")\n",
    "print(f\"  - Improvement:              {improvement:+.2f}%\")\n",
    "print(f\"\\nðŸ“ Outputs:\")\n",
    "print(f\"  - Meta-model: {final_model_path}\")\n",
    "print(f\"  - Results:    {results_path}\")\n",
    "print(f\"  - Report:     {REPORT_PATH}\")\n",
    "print(f\"\\nðŸŽ¯ Next Steps:\")\n",
    "print(f\"  - Fine-tune hyperparameters (Î±, Î², inner steps)\")\n",
    "print(f\"  - Try different architectures (Transformer, GNN)\")\n",
    "print(f\"  - Compare with other meta-learning methods (ProtoNet, Matching Networks)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on VALIDATION set (same as training used)...\n",
      "\n",
      "Validation Set Results:\n",
      "  Accuracy@1:  0.3062 (expected ~37.80%)\n",
      "  Recall@5:    0.4962\n",
      "  MRR:         0.4003\n"
     ]
    }
   ],
   "source": [
    "# Test on VALIDATION set instead of test set (to verify the 37.80% claim)\n",
    "print(\"Testing on VALIDATION set (same as training used)...\")\n",
    "\n",
    "meta_model.eval()\n",
    "val_predictions = []\n",
    "val_labels = []\n",
    "\n",
    "for _, episode in episodes_val.iterrows():\n",
    "    support_pairs, query_pairs = get_episode_data(episode, pairs_val)\n",
    "    \n",
    "    if len(support_pairs) == 0 or len(query_pairs) == 0:\n",
    "        continue\n",
    "    \n",
    "    support_seq, support_labels_val, support_lengths = pairs_to_batch(support_pairs, max_seq_len)\n",
    "    query_seq, query_labels_val, query_lengths = pairs_to_batch(query_pairs, max_seq_len)\n",
    "    \n",
    "    # Adapt using functional forward\n",
    "    with torch.enable_grad():\n",
    "        fast_weights_val = OrderedDict()\n",
    "        for name, param in meta_model.named_parameters():\n",
    "            fast_weights_val[name] = param.clone().requires_grad_()\n",
    "        \n",
    "        for _ in range(num_inner_steps):\n",
    "            support_logits_val = functional_forward(\n",
    "                support_seq, support_lengths, fast_weights_val, hidden_dim, n_items\n",
    "            )\n",
    "            support_loss_val = criterion(support_logits_val, support_labels_val)\n",
    "            \n",
    "            grads_val = torch.autograd.grad(\n",
    "                support_loss_val,\n",
    "                fast_weights_val.values(),\n",
    "                create_graph=False\n",
    "            )\n",
    "            \n",
    "            fast_weights_val = OrderedDict(\n",
    "                (name, param - inner_lr * grad)\n",
    "                for ((name, param), grad) in zip(fast_weights_val.items(), grads_val)\n",
    "            )\n",
    "    \n",
    "    # Evaluate on query\n",
    "    with torch.no_grad():\n",
    "        query_logits_val = functional_forward(\n",
    "            query_seq, query_lengths, fast_weights_val, hidden_dim, n_items\n",
    "        )\n",
    "        probs = torch.softmax(query_logits_val, dim=-1).cpu().numpy()\n",
    "        \n",
    "        val_predictions.append(probs)\n",
    "        val_labels.extend(query_labels_val.cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "val_predictions = np.vstack(val_predictions)\n",
    "val_labels = np.array(val_labels)\n",
    "val_metrics = compute_metrics(val_predictions, val_labels)\n",
    "\n",
    "print(f\"\\nValidation Set Results:\")\n",
    "print(f\"  Accuracy@1:  {val_metrics['accuracy@1']:.4f} (expected ~37.80%)\")\n",
    "print(f\"  Recall@5:    {val_metrics['recall@5']:.4f}\")\n",
    "print(f\"  MRR:         {val_metrics['mrr']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CELL 07f-17] SAVE TRAINED MODEL\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Model save path\n",
    "MODEL_SAVE_PATH = Path(\"models/maml/maml_warmstart_gru_K5.pth\")\n",
    "MODEL_SAVE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': meta_model.state_dict(),\n",
    "    'config': CFG,\n",
    "    'final_metrics': {\n",
    "        'zero_shot_acc1': zeroshot_metrics['accuracy@1'],\n",
    "        'few_shot_K5_acc1': fewshot_metrics['accuracy@1'],\n",
    "    },\n",
    "    'training_iterations': num_meta_iterations,\n",
    "}, MODEL_SAVE_PATH)\n",
    "\n",
    "print(f\"[CELL 07f-17] Model saved to: {MODEL_SAVE_PATH}\")\n",
    "print(f\"[CELL 07f-17] File size: {MODEL_SAVE_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"[CELL 07f-17] Model can now be loaded for inner_lr sweep tests!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Notebook 07e Complete: MAML with Warm-Start\\n\n",
    "\\n\n",
    "**Key Innovation:** Initialized MAML from pre-trained GRU baseline (33.73%)\\n\n",
    "\\n\n",
    "**Results:**\\n\n",
    "- See CELL 07f-10 for zero-shot results\\n\n",
    "- See CELL 07f-11 for few-shot results (K=5)\\n\n",
    "- See CELL 07f-15 for complete comparison\\n\n",
    "\\n\n",
    "**Expected Outcome:**\\n\n",
    "- MAML warm-start should beat both:\\n\n",
    "  - MAML random init (30.52%)\\n\n",
    "  - GRU baseline (33.73%)\\n\n",
    "- Target: 35-38% Acc@1\\n\n",
    "\\n\n",
    "**Why This Works:**\\n\n",
    "1. Strong initialization (GRU) + Meta-learned adaptation (MAML)\\n\n",
    "2. Best of both worlds\\n\n",
    "3. Well-established approach in meta-learning literature\\n\n",
    "\\n\n",
    "**Next Steps:**\\n\n",
    "- If beats baseline â†’ Thesis complete! ðŸŽ‰\\n\n",
    "- If close but not quite â†’ Try bigger model (Fix #4)\\n\n",
    "- Analyze learned adaptations vs random init MAML"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}