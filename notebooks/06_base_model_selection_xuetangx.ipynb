{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 06: Base Model Selection (XuetangX)\n\n**Purpose:** Implement and evaluate base models for next-course prediction.\n\n**Cold-Start Focus:**\n- **Global models**: Non-personalized models (Popularity, Random)\n- **Sequential models**: GRU, SASRec, Session-KNN trained on global data\n- **Evaluation**: Test on cold-start users (no training data for these users)\n\n**Models Implemented:**\n1. **Random**: Uniform random prediction from vocabulary (sanity check)\n2. **Popularity**: Recommend most popular courses from training set\n3. **GRU (Global)**: GRU trained on all training pairs, zero-shot evaluation\n4. **SASRec**: Self-Attention Sequential Recommendation model\n5. **Session-KNN (V-SKNN)**: Vector-based session k-nearest neighbors\n\n**Actual Test Results (from CELL 06-11 output):**\n- Random: **0.06%** Acc@1\n- Popularity: **1.73%** Acc@1\n- **GRU (Global): 33.55% Acc@1** ‚Üê Best base model\n- SASRec: **21.98%** Acc@1\n- Session-KNN: **14.60%** Acc@1\n\n**Inputs:**\n- `data/processed/xuetangx/episodes/episodes_train_K5_Q10.parquet`\n- `data/processed/xuetangx/episodes/episodes_val_K5_Q10.parquet`\n- `data/processed/xuetangx/episodes/episodes_test_K5_Q10.parquet`\n- `data/processed/xuetangx/pairs/pairs_train.parquet`\n- `data/processed/xuetangx/vocab/course2id.json`\n\n**Outputs:**\n- Trained models: `models/baselines/*.pkl`, `models/baselines/*.pth`\n- Evaluation results: `results/baselines_K5_Q10.json`\n- Report: `reports/06_base_model_selection_xuetangx/<run_tag>/report.json`\n\n**Metrics:**\n- Accuracy@1 (exact match)\n- Recall@5, Recall@10 (label in top-k)\n- MRR (Mean Reciprocal Rank)\n\n**Strategy:**\n1. Load episodes and pairs\n2. Implement base models\n3. Train on train episodes\n4. Evaluate on val/test episodes\n5. Report metrics + save results"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 06-00] start=2026-02-01T02:50:04\n",
      "[CELL 06-00] CWD: /workspace/anonymous-users-mooc-session-meta/notebooks\n",
      "[CELL 06-00] REPO_ROOT: /workspace/anonymous-users-mooc-session-meta\n",
      "[CELL 06-00] META_REGISTRY=/workspace/anonymous-users-mooc-session-meta/meta.json\n",
      "[CELL 06-00] DATA_INTERIM=/workspace/anonymous-users-mooc-session-meta/data/interim\n",
      "[CELL 06-00] DATA_PROCESSED=/workspace/anonymous-users-mooc-session-meta/data/processed\n",
      "[CELL 06-00] MODELS=/workspace/anonymous-users-mooc-session-meta/models\n",
      "[CELL 06-00] RESULTS=/workspace/anonymous-users-mooc-session-meta/results\n",
      "[CELL 06-00] REPORTS=/workspace/anonymous-users-mooc-session-meta/reports\n",
      "[CELL 06-00] PyTorch device: cuda\n",
      "[CELL 06-00] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-00] Bootstrap: repo root + paths + logger\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import pickle\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "t0 = datetime.now()\n",
    "print(f\"[CELL 06-00] start={t0.isoformat(timespec='seconds')}\")\n",
    "print(\"[CELL 06-00] CWD:\", Path.cwd().resolve())\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists():\n",
    "            return p\n",
    "    raise RuntimeError(\"Could not find PROJECT_STATE.md. Open notebook from within the repo.\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "print(\"[CELL 06-00] REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "PATHS = {\n",
    "    \"META_REGISTRY\": REPO_ROOT / \"meta.json\",\n",
    "    \"DATA_INTERIM\": REPO_ROOT / \"data\" / \"interim\",\n",
    "    \"DATA_PROCESSED\": REPO_ROOT / \"data\" / \"processed\",\n",
    "    \"MODELS\": REPO_ROOT / \"models\",\n",
    "    \"RESULTS\": REPO_ROOT / \"results\",\n",
    "    \"REPORTS\": REPO_ROOT / \"reports\",\n",
    "}\n",
    "for k, v in PATHS.items():\n",
    "    print(f\"[CELL 06-00] {k}={v}\")\n",
    "\n",
    "def cell_start(cell_id: str, title: str, **kwargs: Any) -> float:\n",
    "    t = time.time()\n",
    "    print(f\"\\n[{cell_id}] {title}\")\n",
    "    print(f\"[{cell_id}] start={datetime.now().isoformat(timespec='seconds')}\")\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    return t\n",
    "\n",
    "def cell_end(cell_id: str, t0: float, **kwargs: Any) -> None:\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    print(f\"[{cell_id}] elapsed={time.time()-t0:.2f}s\")\n",
    "    print(f\"[{cell_id}] done\")\n",
    "\n",
    "# Check GPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[CELL 06-00] PyTorch device: {DEVICE}\")\n",
    "print(\"[CELL 06-00] done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-01] Seed everything\n",
      "[CELL 06-01] start=2026-02-01T02:50:04\n",
      "[CELL 06-01] seed=20260107\n",
      "[CELL 06-01] elapsed=0.00s\n",
      "[CELL 06-01] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-01] Reproducibility: seed everything\n",
    "\n",
    "t0 = cell_start(\"CELL 06-01\", \"Seed everything\")\n",
    "\n",
    "GLOBAL_SEED = 20260107\n",
    "\n",
    "def seed_everything(seed: int) -> None:\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(GLOBAL_SEED)\n",
    "\n",
    "cell_end(\"CELL 06-01\", t0, seed=GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-02] IO helpers\n",
      "[CELL 06-02] start=2026-02-01T02:50:04\n",
      "[CELL 06-02] elapsed=0.00s\n",
      "[CELL 06-02] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-02] JSON/Pickle IO + hashing helpers\n",
    "\n",
    "t0 = cell_start(\"CELL 06-02\", \"IO helpers\")\n",
    "\n",
    "def write_json_atomic(path: Path, obj: Any, indent: int = 2) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tmp = path.with_suffix(path.suffix + f\".tmp_{uuid.uuid4().hex}\")\n",
    "    with tmp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=indent)\n",
    "    tmp.replace(path)\n",
    "\n",
    "def read_json(path: Path) -> Any:\n",
    "    if not path.exists():\n",
    "        raise RuntimeError(f\"Missing JSON file: {path}\")\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_pickle(path: Path, obj: Any) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_pickle(path: Path) -> Any:\n",
    "    with path.open(\"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "cell_end(\"CELL 06-02\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-03] Start run + init files\n",
      "[CELL 06-03] start=2026-02-01T02:50:04\n",
      "[CELL 06-03] K=5, Q=10\n",
      "[CELL 06-03] Baselines: ['random', 'popularity', 'gru_global', 'sasrec', 'sessionknn']\n",
      "[CELL 06-03] out_dir=/workspace/anonymous-users-mooc-session-meta/reports/06_base_model_selection_xuetangx/20260201_025004\n",
      "[CELL 06-03] elapsed=0.00s\n",
      "[CELL 06-03] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-03] Run tagging + config + meta.json\n",
    "\n",
    "t0 = cell_start(\"CELL 06-03\", \"Start run + init files\")\n",
    "\n",
    "NOTEBOOK_NAME = \"06_base_model_selection_xuetangx\"\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_ID = uuid.uuid4().hex\n",
    "\n",
    "OUT_DIR = PATHS[\"REPORTS\"] / NOTEBOOK_NAME / RUN_TAG\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REPORT_PATH = OUT_DIR / \"report.json\"\n",
    "CONFIG_PATH = OUT_DIR / \"config.json\"\n",
    "MANIFEST_PATH = OUT_DIR / \"manifest.json\"\n",
    "\n",
    "# Paths\n",
    "EPISODES_DIR = PATHS[\"DATA_PROCESSED\"] / \"xuetangx\" / \"episodes\"\n",
    "PAIRS_DIR = PATHS[\"DATA_PROCESSED\"] / \"xuetangx\" / \"pairs\"\n",
    "VOCAB_DIR = PATHS[\"DATA_PROCESSED\"] / \"xuetangx\" / \"vocab\"\n",
    "MODELS_DIR = PATHS[\"MODELS\"] / \"baselines\"\n",
    "RESULTS_DIR = PATHS[\"RESULTS\"]\n",
    "\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Focus on K=5, Q=10 for now\n",
    "K, Q = 5, 10\n",
    "\n",
    "CFG = {\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"seed\": GLOBAL_SEED,\n",
    "    \"device\": str(DEVICE),\n",
    "    \"k_shot_config\": {\"K\": K, \"Q\": Q},\n",
    "    \"inputs\": {\n",
    "        \"episodes_train\": str(EPISODES_DIR / f\"episodes_train_K{K}_Q{Q}.parquet\"),\n",
    "        \"episodes_val\": str(EPISODES_DIR / f\"episodes_val_K{K}_Q{Q}.parquet\"),\n",
    "        \"episodes_test\": str(EPISODES_DIR / f\"episodes_test_K{K}_Q{Q}.parquet\"),\n",
    "        \"pairs_train\": str(PAIRS_DIR / \"pairs_train.parquet\"),\n",
    "        \"pairs_val\": str(PAIRS_DIR / \"pairs_val.parquet\"),\n",
    "        \"pairs_test\": str(PAIRS_DIR / \"pairs_test.parquet\"),\n",
    "        \"vocab\": str(VOCAB_DIR / \"course2id.json\"),\n",
    "    },\n",
    "    \"baselines\": [\n",
    "        \"random\",\n",
    "        \"popularity\",\n",
    "        \"gru_global\",\n",
    "        \"sasrec\",\n",
    "        \"sessionknn\",\n",
    "    ],\n",
    "    \"gru_config\": {\n",
    "        \"embedding_dim\": 64,\n",
    "        \"hidden_dim\": 128,\n",
    "        \"num_layers\": 1,\n",
    "        \"dropout\": 0.2,\n",
    "        \"batch_size\": 256,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"num_epochs\": 10,\n",
    "        \"max_seq_len\": 50,  # truncate long sequences\n",
    "    },\n",
    "    \"metrics\": [\"accuracy@1\", \"recall@5\", \"recall@10\", \"mrr\"],\n",
    "    \"outputs\": {\n",
    "        \"models_dir\": str(MODELS_DIR),\n",
    "        \"results\": str(RESULTS_DIR / f\"baselines_K{K}_Q{Q}.json\"),\n",
    "        \"out_dir\": str(OUT_DIR),\n",
    "    }\n",
    "}\n",
    "\n",
    "write_json_atomic(CONFIG_PATH, CFG)\n",
    "\n",
    "report = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"repo_root\": str(REPO_ROOT),\n",
    "    \"metrics\": {},\n",
    "    \"key_findings\": [],\n",
    "    \"sanity_samples\": {},\n",
    "    \"data_fingerprints\": {},\n",
    "    \"notes\": [],\n",
    "}\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "manifest = {\"run_id\": RUN_ID, \"notebook\": NOTEBOOK_NAME, \"run_tag\": RUN_TAG, \"artifacts\": []}\n",
    "write_json_atomic(MANIFEST_PATH, manifest)\n",
    "\n",
    "# meta.json\n",
    "META_PATH = PATHS[\"META_REGISTRY\"]\n",
    "if not META_PATH.exists():\n",
    "    write_json_atomic(META_PATH, {\"schema_version\": 1, \"runs\": []})\n",
    "meta = read_json(META_PATH)\n",
    "meta[\"runs\"].append({\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"out_dir\": str(OUT_DIR),\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "})\n",
    "write_json_atomic(META_PATH, meta)\n",
    "\n",
    "print(f\"[CELL 06-03] K={K}, Q={Q}\")\n",
    "print(f\"[CELL 06-03] Baselines: {CFG['baselines']}\")\n",
    "\n",
    "cell_end(\"CELL 06-03\", t0, out_dir=str(OUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-04] Load data\n",
      "[CELL 06-04] start=2026-02-01T02:50:04\n",
      "[CELL 06-04] Vocabulary: 1518 courses\n",
      "[CELL 06-04] Episodes train: 47,357 episodes\n",
      "[CELL 06-04] Episodes val:   341 episodes\n",
      "[CELL 06-04] Episodes test:  313 episodes\n",
      "[CELL 06-04] Pairs train: 225,168 pairs\n",
      "[CELL 06-04] Pairs val:   28,559 pairs\n",
      "[CELL 06-04] Pairs test:  28,252 pairs\n",
      "[CELL 06-04] elapsed=0.19s\n",
      "[CELL 06-04] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-04] Load vocab, episodes, and pairs\n",
    "\n",
    "t0 = cell_start(\"CELL 06-04\", \"Load data\")\n",
    "\n",
    "# Vocab\n",
    "course2id = read_json(Path(CFG[\"inputs\"][\"vocab\"]))\n",
    "id2course = {int(v): k for k, v in course2id.items()}\n",
    "n_items = len(course2id)\n",
    "print(f\"[CELL 06-04] Vocabulary: {n_items} courses\")\n",
    "\n",
    "# Episodes\n",
    "episodes_train = pd.read_parquet(CFG[\"inputs\"][\"episodes_train\"])\n",
    "episodes_val = pd.read_parquet(CFG[\"inputs\"][\"episodes_val\"])\n",
    "episodes_test = pd.read_parquet(CFG[\"inputs\"][\"episodes_test\"])\n",
    "\n",
    "print(f\"[CELL 06-04] Episodes train: {len(episodes_train):,} episodes\")\n",
    "print(f\"[CELL 06-04] Episodes val:   {len(episodes_val):,} episodes\")\n",
    "print(f\"[CELL 06-04] Episodes test:  {len(episodes_test):,} episodes\")\n",
    "\n",
    "# Pairs (for GRU training)\n",
    "pairs_train = pd.read_parquet(CFG[\"inputs\"][\"pairs_train\"])\n",
    "pairs_val = pd.read_parquet(CFG[\"inputs\"][\"pairs_val\"])\n",
    "pairs_test = pd.read_parquet(CFG[\"inputs\"][\"pairs_test\"])\n",
    "print(f\"[CELL 06-04] Pairs train: {len(pairs_train):,} pairs\")\n",
    "print(f\"[CELL 06-04] Pairs val:   {len(pairs_val):,} pairs\")\n",
    "print(f\"[CELL 06-04] Pairs test:  {len(pairs_test):,} pairs\")\n",
    "\n",
    "cell_end(\"CELL 06-04\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-05] Define evaluation metrics\n",
      "[CELL 06-05] start=2026-02-01T02:50:04\n",
      "[CELL 06-05] Metrics: accuracy@1, recall@5, recall@10, mrr\n",
      "[CELL 06-05] elapsed=0.00s\n",
      "[CELL 06-05] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-05] Evaluation metrics\n",
    "\n",
    "t0 = cell_start(\"CELL 06-05\", \"Define evaluation metrics\")\n",
    "\n",
    "def compute_metrics(predictions: np.ndarray, labels: np.ndarray, k_values: List[int] = [5, 10]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute ranking metrics.\n",
    "    \n",
    "    Args:\n",
    "        predictions: (n_samples, n_items) score matrix\n",
    "        labels: (n_samples,) true item indices\n",
    "        k_values: list of k for Recall@k\n",
    "    \n",
    "    Returns:\n",
    "        dict with accuracy@1, recall@k, mrr\n",
    "    \"\"\"\n",
    "    n_samples = len(labels)\n",
    "    \n",
    "    # Get top-k predictions (indices)\n",
    "    max_k = max(k_values)\n",
    "    top_k_preds = np.argsort(-predictions, axis=1)[:, :max_k]  # descending order\n",
    "    \n",
    "    # Accuracy@1\n",
    "    top1_preds = top_k_preds[:, 0]\n",
    "    acc1 = (top1_preds == labels).mean()\n",
    "    \n",
    "    # Recall@k\n",
    "    recall_k = {}\n",
    "    for k in k_values:\n",
    "        hits = np.array([labels[i] in top_k_preds[i, :k] for i in range(n_samples)])\n",
    "        recall_k[f\"recall@{k}\"] = hits.mean()\n",
    "    \n",
    "    # MRR (Mean Reciprocal Rank)\n",
    "    ranks = []\n",
    "    for i in range(n_samples):\n",
    "        # Find rank of true label (1-indexed)\n",
    "        rank_idx = np.where(top_k_preds[i] == labels[i])[0]\n",
    "        if len(rank_idx) > 0:\n",
    "            ranks.append(1.0 / (rank_idx[0] + 1))  # reciprocal rank\n",
    "        else:\n",
    "            # Not in top-k, check full ranking\n",
    "            full_rank = np.where(np.argsort(-predictions[i]) == labels[i])[0][0]\n",
    "            ranks.append(1.0 / (full_rank + 1))\n",
    "    mrr = np.mean(ranks)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy@1\": float(acc1),\n",
    "        **{k: float(v) for k, v in recall_k.items()},\n",
    "        \"mrr\": float(mrr),\n",
    "    }\n",
    "\n",
    "print(\"[CELL 06-05] Metrics: accuracy@1, recall@5, recall@10, mrr\")\n",
    "\n",
    "cell_end(\"CELL 06-05\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-06] Random baseline\n",
      "[CELL 06-06] start=2026-02-01T02:50:04\n",
      "[CELL 06-06] Saved: /workspace/anonymous-users-mooc-session-meta/models/baselines/random.pkl\n",
      "[CELL 06-06] elapsed=0.00s\n",
      "[CELL 06-06] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-06] Baseline 1: Random predictor\n",
    "\n",
    "t0 = cell_start(\"CELL 06-06\", \"Random baseline\")\n",
    "\n",
    "class RandomPredictor:\n",
    "    def __init__(self, n_items: int, seed: int = 42):\n",
    "        self.n_items = n_items\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "    \n",
    "    def predict(self, n_samples: int) -> np.ndarray:\n",
    "        \"\"\"Return uniform random scores for each item.\"\"\"\n",
    "        return self.rng.rand(n_samples, self.n_items)\n",
    "\n",
    "# Initialize\n",
    "random_model = RandomPredictor(n_items=n_items, seed=GLOBAL_SEED)\n",
    "\n",
    "# Save\n",
    "save_pickle(MODELS_DIR / \"random.pkl\", random_model)\n",
    "print(f\"[CELL 06-06] Saved: {MODELS_DIR / 'random.pkl'}\")\n",
    "\n",
    "cell_end(\"CELL 06-06\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-07] Popularity baseline\n",
      "[CELL 06-07] start=2026-02-01T02:50:04\n",
      "[CELL 06-07] Top-5 popular courses:\n",
      "  1. course-v1:TsinghuaX+30240184+sp (score=0.0227)\n",
      "  2. course-v1:TsinghuaX+10610193X+2017_T1 (score=0.0155)\n",
      "  3. course-v1:TsinghuaX+30640014X+2017_T1 (score=0.0155)\n",
      "  4. course-v1:TsinghuaX+30240243X+sp (score=0.0121)\n",
      "  5. course-v1:MITx+6_00_1x+sp (score=0.0116)\n",
      "\n",
      "[CELL 06-07] Saved: /workspace/anonymous-users-mooc-session-meta/models/baselines/popularity.pkl\n",
      "[CELL 06-07] elapsed=0.03s\n",
      "[CELL 06-07] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-07] Baseline 2: Popularity predictor\n",
    "\n",
    "t0 = cell_start(\"CELL 06-07\", \"Popularity baseline\")\n",
    "\n",
    "class PopularityPredictor:\n",
    "    def __init__(self, n_items: int):\n",
    "        self.n_items = n_items\n",
    "        self.popularity_scores = np.zeros(n_items)\n",
    "    \n",
    "    def fit(self, labels: List[int]):\n",
    "        \"\"\"Compute popularity from training labels.\"\"\"\n",
    "        counts = Counter(labels)\n",
    "        for item_id, count in counts.items():\n",
    "            self.popularity_scores[item_id] = count\n",
    "        # Normalize\n",
    "        self.popularity_scores /= self.popularity_scores.sum()\n",
    "    \n",
    "    def predict(self, n_samples: int) -> np.ndarray:\n",
    "        \"\"\"Return popularity scores repeated for each sample.\"\"\"\n",
    "        return np.tile(self.popularity_scores, (n_samples, 1))\n",
    "\n",
    "# Train: extract labels from training pairs\n",
    "train_labels = pairs_train[\"label\"].tolist()\n",
    "\n",
    "popularity_model = PopularityPredictor(n_items=n_items)\n",
    "popularity_model.fit(train_labels)\n",
    "\n",
    "# Top-5 most popular courses\n",
    "top5_idx = np.argsort(-popularity_model.popularity_scores)[:5]\n",
    "print(f\"[CELL 06-07] Top-5 popular courses:\")\n",
    "for rank, idx in enumerate(top5_idx, 1):\n",
    "    course = id2course[idx]\n",
    "    score = popularity_model.popularity_scores[idx]\n",
    "    print(f\"  {rank}. {course} (score={score:.4f})\")\n",
    "\n",
    "# Save\n",
    "save_pickle(MODELS_DIR / \"popularity.pkl\", popularity_model)\n",
    "print(f\"\\n[CELL 06-07] Saved: {MODELS_DIR / 'popularity.pkl'}\")\n",
    "\n",
    "cell_end(\"CELL 06-07\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-08] Define GRU model\n",
      "[CELL 06-08] start=2026-02-01T02:50:05\n",
      "[CELL 06-08] GRU model defined\n",
      "  - Embedding dim: 64\n",
      "  - Hidden dim: 128\n",
      "  - Num layers: 1\n",
      "[CELL 06-08] elapsed=0.00s\n",
      "[CELL 06-08] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-08] Baseline 3: GRU model definition\n",
    "\n",
    "t0 = cell_start(\"CELL 06-08\", \"Define GRU model\")\n",
    "\n",
    "class GRURecommender(nn.Module):\n",
    "    def __init__(self, n_items: int, embedding_dim: int, hidden_dim: int, num_layers: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.n_items = n_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_items, embedding_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim, n_items)\n",
    "    \n",
    "    def forward(self, seq: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            seq: (batch, max_len) padded sequences\n",
    "            lengths: (batch,) actual lengths\n",
    "        Returns:\n",
    "            logits: (batch, n_items)\n",
    "        \"\"\"\n",
    "        # Embed\n",
    "        emb = self.embedding(seq)  # (batch, max_len, embed_dim)\n",
    "        \n",
    "        # Pack for efficiency\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # GRU\n",
    "        _, hidden = self.gru(packed)  # hidden: (num_layers, batch, hidden_dim)\n",
    "        \n",
    "        # Use last layer hidden state\n",
    "        h = hidden[-1]  # (batch, hidden_dim)\n",
    "        \n",
    "        # Predict\n",
    "        logits = self.fc(h)  # (batch, n_items)\n",
    "        return logits\n",
    "\n",
    "print(\"[CELL 06-08] GRU model defined\")\n",
    "print(f\"  - Embedding dim: {CFG['gru_config']['embedding_dim']}\")\n",
    "print(f\"  - Hidden dim: {CFG['gru_config']['hidden_dim']}\")\n",
    "print(f\"  - Num layers: {CFG['gru_config']['num_layers']}\")\n",
    "\n",
    "cell_end(\"CELL 06-08\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-08B] Define SASRec model\n",
      "[CELL 06-08B] start=2026-02-01T02:50:05\n",
      "[CELL 06-08B] SASRec model defined\n",
      "  - Hidden dim: 64\n",
      "  - Num heads: 2\n",
      "  - Num blocks: 2\n",
      "  - Max len: 50\n",
      "[CELL 06-08B] elapsed=0.00s\n",
      "[CELL 06-08B] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-08B] Baseline 4: SASRec model definition\n",
    "\n",
    "t0 = cell_start(\"CELL 06-08B\", \"Define SASRec model\")\n",
    "\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(self, n_items: int, hidden_dim: int, num_heads: int, num_blocks: int, max_len: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.n_items = n_items\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Item embedding + positional embedding\n",
    "        self.item_emb = nn.Embedding(n_items + 1, hidden_dim, padding_idx=0)  # +1 for padding\n",
    "        self.pos_emb = nn.Embedding(max_len, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Multi-head self-attention blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=hidden_dim * 4,\n",
    "                dropout=dropout,\n",
    "                activation='relu',\n",
    "                batch_first=True,\n",
    "            )\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # Prediction head\n",
    "        self.fc = nn.Linear(hidden_dim, n_items)\n",
    "    \n",
    "    def forward(self, seq: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            seq: (batch, seq_len) padded sequences\n",
    "        Returns:\n",
    "            logits: (batch, n_items)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = seq.size()\n",
    "        \n",
    "        # Create position indices\n",
    "        positions = torch.arange(seq_len, device=seq.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        # Embed items and positions\n",
    "        item_emb = self.item_emb(seq)  # (batch, seq_len, hidden_dim)\n",
    "        pos_emb = self.pos_emb(positions)  # (batch, seq_len, hidden_dim)\n",
    "        x = self.dropout(item_emb + pos_emb)\n",
    "        \n",
    "        # Create attention mask (causal mask: can only attend to past)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=seq.device) * float('-inf'), diagonal=1)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, src_mask=attn_mask)\n",
    "        \n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        # Use last position for prediction\n",
    "        x = x[:, -1, :]  # (batch, hidden_dim)\n",
    "        \n",
    "        logits = self.fc(x)  # (batch, n_items)\n",
    "        return logits\n",
    "\n",
    "print(\"[CELL 06-08B] SASRec model defined\")\n",
    "print(f\"  - Hidden dim: 64\")\n",
    "print(f\"  - Num heads: 2\")\n",
    "print(f\"  - Num blocks: 2\")\n",
    "print(f\"  - Max len: 50\")\n",
    "\n",
    "cell_end(\"CELL 06-08B\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-09] Create GRU dataset\n",
      "[CELL 06-09] start=2026-02-01T02:50:05\n",
      "[CELL 06-09] Train dataset: 225,168 pairs\n",
      "[CELL 06-09] Train loader: 880 batches\n",
      "[CELL 06-09] elapsed=0.00s\n",
      "[CELL 06-09] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-09] GRU dataset and dataloader\n",
    "\n",
    "t0 = cell_start(\"CELL 06-09\", \"Create GRU dataset\")\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, pairs_df: pd.DataFrame, max_seq_len: int):\n",
    "        self.pairs = pairs_df.reset_index(drop=True)\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.pairs.iloc[idx]\n",
    "        prefix = row[\"prefix\"]  # list of item IDs\n",
    "        label = row[\"label\"]    # int\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(prefix) > self.max_seq_len:\n",
    "            prefix = prefix[-self.max_seq_len:]\n",
    "        \n",
    "        return {\n",
    "            \"prefix\": prefix,\n",
    "            \"label\": label,\n",
    "            \"length\": len(prefix),\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate batch with padding.\"\"\"\n",
    "    prefixes = [item[\"prefix\"] for item in batch]\n",
    "    labels = [item[\"label\"] for item in batch]\n",
    "    lengths = [item[\"length\"] for item in batch]\n",
    "    \n",
    "    # Pad sequences\n",
    "    max_len = max(lengths)\n",
    "    padded = []\n",
    "    for seq in prefixes:\n",
    "        padded.append(list(seq) + [0] * (max_len - len(seq)))  # convert to list if numpy array\n",
    "    \n",
    "    return {\n",
    "        \"prefix\": torch.LongTensor(padded),\n",
    "        \"label\": torch.LongTensor(labels),\n",
    "        \"length\": torch.LongTensor(lengths),\n",
    "    }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = PairDataset(pairs_train, max_seq_len=CFG[\"gru_config\"][\"max_seq_len\"])\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CFG[\"gru_config\"][\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,  # Windows compatibility\n",
    ")\n",
    "\n",
    "print(f\"[CELL 06-09] Train dataset: {len(train_dataset):,} pairs\")\n",
    "print(f\"[CELL 06-09] Train loader: {len(train_loader):,} batches\")\n",
    "\n",
    "cell_end(\"CELL 06-09\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-10] Train GRU\n",
      "[CELL 06-10] start=2026-02-01T02:50:05\n",
      "[CELL 06-10] Model parameters: 367,470\n",
      "[CELL 06-10] Epoch 1/10: loss=5.0410\n",
      "[CELL 06-10] Epoch 2/10: loss=4.0517\n",
      "[CELL 06-10] Epoch 3/10: loss=3.7687\n",
      "[CELL 06-10] Epoch 4/10: loss=3.6152\n",
      "[CELL 06-10] Epoch 5/10: loss=3.5106\n",
      "[CELL 06-10] Epoch 6/10: loss=3.4314\n",
      "[CELL 06-10] Epoch 7/10: loss=3.3664\n",
      "[CELL 06-10] Epoch 8/10: loss=3.3106\n",
      "[CELL 06-10] Epoch 9/10: loss=3.2618\n",
      "[CELL 06-10] Epoch 10/10: loss=3.2184\n",
      "\n",
      "[CELL 06-10] Saved: /workspace/anonymous-users-mooc-session-meta/models/baselines/gru_global.pth\n",
      "[CELL 06-10] elapsed=107.88s\n",
      "[CELL 06-10] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-10] Train GRU model\n",
    "\n",
    "t0 = cell_start(\"CELL 06-10\", \"Train GRU\")\n",
    "\n",
    "# Initialize model\n",
    "gru_model = GRURecommender(\n",
    "    n_items=n_items,\n",
    "    embedding_dim=CFG[\"gru_config\"][\"embedding_dim\"],\n",
    "    hidden_dim=CFG[\"gru_config\"][\"hidden_dim\"],\n",
    "    num_layers=CFG[\"gru_config\"][\"num_layers\"],\n",
    "    dropout=CFG[\"gru_config\"][\"dropout\"],\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(gru_model.parameters(), lr=CFG[\"gru_config\"][\"learning_rate\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"[CELL 06-10] Model parameters: {sum(p.numel() for p in gru_model.parameters()):,}\")\n",
    "\n",
    "# Training loop\n",
    "gru_model.train()\n",
    "for epoch in range(CFG[\"gru_config\"][\"num_epochs\"]):\n",
    "    epoch_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        prefix = batch[\"prefix\"].to(DEVICE)\n",
    "        label = batch[\"label\"].to(DEVICE)\n",
    "        length = batch[\"length\"].to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = gru_model(prefix, length)\n",
    "        loss = criterion(logits, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"[CELL 06-10] Epoch {epoch+1}/{CFG['gru_config']['num_epochs']}: loss={avg_loss:.4f}\")\n",
    "\n",
    "# Save model\n",
    "torch.save(gru_model.state_dict(), MODELS_DIR / \"gru_global.pth\")\n",
    "print(f\"\\n[CELL 06-10] Saved: {MODELS_DIR / 'gru_global.pth'}\")\n",
    "\n",
    "cell_end(\"CELL 06-10\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-10B] Train SASRec\n",
      "[CELL 06-10B] start=2026-02-01T02:51:52\n",
      "[CELL 06-10B] SASRec parameters: 299,182\n",
      "[CELL 06-10B] Epoch 1/10: loss=5.5806\n",
      "[CELL 06-10B] Epoch 2/10: loss=4.8620\n",
      "[CELL 06-10B] Epoch 3/10: loss=4.6107\n",
      "[CELL 06-10B] Epoch 4/10: loss=4.4687\n",
      "[CELL 06-10B] Epoch 5/10: loss=4.3610\n",
      "[CELL 06-10B] Epoch 6/10: loss=4.2846\n",
      "[CELL 06-10B] Epoch 7/10: loss=4.2217\n",
      "[CELL 06-10B] Epoch 8/10: loss=4.1714\n",
      "[CELL 06-10B] Epoch 9/10: loss=4.1265\n",
      "[CELL 06-10B] Epoch 10/10: loss=4.0929\n",
      "\n",
      "[CELL 06-10B] Saved: /workspace/anonymous-users-mooc-session-meta/models/baselines/sasrec.pth\n",
      "[CELL 06-10B] elapsed=111.98s\n",
      "[CELL 06-10B] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-10B] Train SASRec model\n",
    "\n",
    "t0 = cell_start(\"CELL 06-10B\", \"Train SASRec\")\n",
    "\n",
    "# SASRec config\n",
    "sasrec_config = {\n",
    "    'hidden_dim': 64,\n",
    "    'num_heads': 2,\n",
    "    'num_blocks': 2,\n",
    "    'max_len': 50,\n",
    "    'dropout': 0.2,\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 0.001,\n",
    "    'num_epochs': 10,\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "sasrec_model = SASRec(\n",
    "    n_items=n_items,\n",
    "    hidden_dim=sasrec_config['hidden_dim'],\n",
    "    num_heads=sasrec_config['num_heads'],\n",
    "    num_blocks=sasrec_config['num_blocks'],\n",
    "    max_len=sasrec_config['max_len'],\n",
    "    dropout=sasrec_config['dropout'],\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(sasrec_model.parameters(), lr=sasrec_config['learning_rate'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"[CELL 06-10B] SASRec parameters: {sum(p.numel() for p in sasrec_model.parameters()):,}\")\n",
    "\n",
    "# Create SASRec dataloader (same as GRU)\n",
    "def collate_fn_sasrec(batch):\n",
    "    prefixes = [item['prefix'] for item in batch]\n",
    "    labels = [item['label'] for item in batch]\n",
    "    \n",
    "    # Pad to same length\n",
    "    max_len = max(len(p) for p in prefixes)\n",
    "    padded = []\n",
    "    for seq in prefixes:\n",
    "        if len(seq) > sasrec_config['max_len']:\n",
    "            seq = seq[-sasrec_config['max_len']:]\n",
    "        padded.append(list(seq) + [0] * (max_len - len(seq)))\n",
    "    \n",
    "    return {\n",
    "        'prefix': torch.LongTensor(padded),\n",
    "        'label': torch.LongTensor(labels),\n",
    "    }\n",
    "\n",
    "sasrec_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=sasrec_config['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_sasrec,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "sasrec_model.train()\n",
    "for epoch in range(sasrec_config['num_epochs']):\n",
    "    epoch_loss = 0.0\n",
    "    for batch in sasrec_loader:\n",
    "        prefix = batch['prefix'].to(DEVICE)\n",
    "        label = batch['label'].to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = sasrec_model(prefix)\n",
    "        loss = criterion(logits, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(sasrec_loader)\n",
    "    print(f\"[CELL 06-10B] Epoch {epoch+1}/{sasrec_config['num_epochs']}: loss={avg_loss:.4f}\")\n",
    "\n",
    "# Save\n",
    "torch.save(sasrec_model.state_dict(), MODELS_DIR / 'sasrec.pth')\n",
    "print(f\"\\n[CELL 06-10B] Saved: {MODELS_DIR / 'sasrec.pth'}\")\n",
    "\n",
    "cell_end(\"CELL 06-10B\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-10C] Session-KNN baseline\n",
      "[CELL 06-10C] start=2026-02-01T02:53:44\n",
      "[CELL 06-10C] Training Session-KNN (k=100)...\n",
      "[CELL 06-10C] Built session database: 49,643 sessions\n",
      "[CELL 06-10C] Saved: /workspace/anonymous-users-mooc-session-meta/models/baselines/sessionknn.pkl\n",
      "[CELL 06-10C] elapsed=13.60s\n",
      "[CELL 06-10C] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-10C] Baseline 5: Session-KNN (V-SKNN)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-10C\", \"Session-KNN baseline\")\n",
    "\n",
    "class SessionKNN:\n",
    "    \"\"\"\n",
    "    Session-based k-Nearest Neighbors (V-SKNN variant).\n",
    "\n",
    "    Finds similar past sessions and recommends items from those sessions.\n",
    "    Similarity based on cosine similarity of session item sets.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_items: int, k: int = 100, sample_size: int = 500):\n",
    "        self.n_items = n_items\n",
    "        self.k = k  # number of nearest neighbor sessions\n",
    "        self.sample_size = sample_size  # max sessions to consider (for efficiency)\n",
    "        self.sessions = []  # list of (session_id, item_list)\n",
    "\n",
    "    def fit(self, pairs_df: pd.DataFrame):\n",
    "        \"\"\"Build session database from training data.\"\"\"\n",
    "        # Group pairs by user to create sessions\n",
    "        sessions_list = []\n",
    "        for user_id, user_pairs in pairs_df.groupby('user_id'):\n",
    "            # Sort by timestamp\n",
    "            user_pairs_sorted = user_pairs.sort_values('label_ts_epoch')\n",
    "\n",
    "            # Extract session as list of items (prefix + label)\n",
    "            session_items = []\n",
    "            for _, row in user_pairs_sorted.iterrows():\n",
    "                prefix = row['prefix']\n",
    "                label = row['label']\n",
    "                # Add items from prefix\n",
    "                if isinstance(prefix, (list, np.ndarray)):\n",
    "                    session_items.extend(prefix)\n",
    "                # Add label\n",
    "                session_items.append(label)\n",
    "\n",
    "            # Store unique items in session\n",
    "            session_items_unique = list(dict.fromkeys(session_items))  # preserve order, remove duplicates\n",
    "            sessions_list.append((user_id, session_items_unique))\n",
    "\n",
    "        self.sessions = sessions_list\n",
    "        print(f\"[CELL 06-10C] Built session database: {len(self.sessions):,} sessions\")\n",
    "\n",
    "    def _session_similarity(self, session1: List[int], session2: List[int]) -> float:\n",
    "        \"\"\"Compute cosine similarity between two sessions (item sets).\"\"\"\n",
    "        set1 = set(session1)\n",
    "        set2 = set(session2)\n",
    "\n",
    "        if len(set1) == 0 or len(set2) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        # Jaccard similarity (intersection over union) as approximation\n",
    "        # More efficient than full cosine for sets\n",
    "        intersection = len(set1 & set2)\n",
    "        union = len(set1 | set2)\n",
    "\n",
    "        return intersection / union if union > 0 else 0.0\n",
    "\n",
    "    def predict(self, current_session: List[int]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict item scores based on k most similar past sessions.\n",
    "\n",
    "        Args:\n",
    "            current_session: List of items in current session (prefix)\n",
    "\n",
    "        Returns:\n",
    "            scores: (n_items,) array of prediction scores\n",
    "        \"\"\"\n",
    "        if len(current_session) == 0 or len(self.sessions) == 0:\n",
    "            return np.zeros(self.n_items)\n",
    "\n",
    "        # Sample sessions for efficiency if too many\n",
    "        sessions_to_consider = self.sessions\n",
    "        if len(self.sessions) > self.sample_size:\n",
    "            import random\n",
    "            sessions_to_consider = random.sample(self.sessions, self.sample_size)\n",
    "\n",
    "        # Compute similarities to all sessions\n",
    "        similarities = []\n",
    "        for session_id, session_items in sessions_to_consider:\n",
    "            sim = self._session_similarity(current_session, session_items)\n",
    "            similarities.append((session_id, session_items, sim))\n",
    "\n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        # Take top-k similar sessions\n",
    "        top_k_sessions = similarities[:self.k]\n",
    "\n",
    "        # Aggregate scores from top-k sessions\n",
    "        scores = np.zeros(self.n_items)\n",
    "        total_sim = sum(sim for _, _, sim in top_k_sessions)\n",
    "\n",
    "        if total_sim == 0:\n",
    "            return scores\n",
    "\n",
    "        for session_id, session_items, sim in top_k_sessions:\n",
    "            # Weight by similarity and recency (last items have higher weight)\n",
    "            for i, item in enumerate(session_items):\n",
    "                if 0 <= item < self.n_items:\n",
    "                    # Recency weight: more recent items (later in list) get higher weight\n",
    "                    recency_weight = (i + 1) / len(session_items)\n",
    "                    scores[item] += sim * recency_weight\n",
    "\n",
    "        # Normalize by total similarity\n",
    "        scores /= total_sim\n",
    "\n",
    "        return scores\n",
    "\n",
    "# Train Session-KNN\n",
    "print(\"[CELL 06-10C] Training Session-KNN (k=100)...\")\n",
    "sessionknn_model = SessionKNN(n_items=n_items, k=100, sample_size=500)\n",
    "sessionknn_model.fit(pairs_train)\n",
    "\n",
    "# Save\n",
    "save_pickle(MODELS_DIR / 'sessionknn.pkl', sessionknn_model)\n",
    "print(f\"[CELL 06-10C] Saved: {MODELS_DIR / 'sessionknn.pkl'}\")\n",
    "\n",
    "cell_end(\"CELL 06-10C\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-10C] Session-KNN baseline\n",
      "[CELL 06-10C] start=2026-02-01T02:53:58\n",
      "[CELL 06-10C] Training Session-KNN (k=100)...\n",
      "[CELL 06-10C] Built session database: 49,643 sessions\n",
      "[CELL 06-10C] Saved: /workspace/anonymous-users-mooc-session-meta/models/baselines/sessionknn.pkl\n",
      "[CELL 06-10C] elapsed=13.46s\n",
      "[CELL 06-10C] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-10C] Baseline 5: Session-KNN (V-SKNN)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-10C\", \"Session-KNN baseline\")\n",
    "\n",
    "class SessionKNN:\n",
    "    \"\"\"\n",
    "    Session-based k-Nearest Neighbors (V-SKNN variant).\n",
    "\n",
    "    Finds similar past sessions and recommends items from those sessions.\n",
    "    Similarity based on cosine similarity of session item sets.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_items: int, k: int = 100, sample_size: int = 500):\n",
    "        self.n_items = n_items\n",
    "        self.k = k  # number of nearest neighbor sessions\n",
    "        self.sample_size = sample_size  # max sessions to consider (for efficiency)\n",
    "        self.sessions = []  # list of (session_id, item_list)\n",
    "\n",
    "    def fit(self, pairs_df: pd.DataFrame):\n",
    "        \"\"\"Build session database from training data.\"\"\"\n",
    "        # Group pairs by user to create sessions\n",
    "        sessions_list = []\n",
    "        for user_id, user_pairs in pairs_df.groupby('user_id'):\n",
    "            # Sort by timestamp\n",
    "            user_pairs_sorted = user_pairs.sort_values('label_ts_epoch')\n",
    "\n",
    "            # Extract session as list of items (prefix + label)\n",
    "            session_items = []\n",
    "            for _, row in user_pairs_sorted.iterrows():\n",
    "                prefix = row['prefix']\n",
    "                label = row['label']\n",
    "                # Add items from prefix\n",
    "                if isinstance(prefix, (list, np.ndarray)):\n",
    "                    session_items.extend(prefix)\n",
    "                # Add label\n",
    "                session_items.append(label)\n",
    "\n",
    "            # Store unique items in session\n",
    "            session_items_unique = list(dict.fromkeys(session_items))  # preserve order, remove duplicates\n",
    "            sessions_list.append((user_id, session_items_unique))\n",
    "\n",
    "        self.sessions = sessions_list\n",
    "        print(f\"[CELL 06-10C] Built session database: {len(self.sessions):,} sessions\")\n",
    "\n",
    "    def _session_similarity(self, session1: List[int], session2: List[int]) -> float:\n",
    "        \"\"\"Compute cosine similarity between two sessions (item sets).\"\"\"\n",
    "        set1 = set(session1)\n",
    "        set2 = set(session2)\n",
    "\n",
    "        if len(set1) == 0 or len(set2) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        # Jaccard similarity (intersection over union) as approximation\n",
    "        # More efficient than full cosine for sets\n",
    "        intersection = len(set1 & set2)\n",
    "        union = len(set1 | set2)\n",
    "\n",
    "        return intersection / union if union > 0 else 0.0\n",
    "\n",
    "    def predict(self, current_session: List[int]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict item scores based on k most similar past sessions.\n",
    "\n",
    "        Args:\n",
    "            current_session: List of items in current session (prefix)\n",
    "\n",
    "        Returns:\n",
    "            scores: (n_items,) array of prediction scores\n",
    "        \"\"\"\n",
    "        if len(current_session) == 0 or len(self.sessions) == 0:\n",
    "            return np.zeros(self.n_items)\n",
    "\n",
    "        # Sample sessions for efficiency if too many\n",
    "        sessions_to_consider = self.sessions\n",
    "        if len(self.sessions) > self.sample_size:\n",
    "            import random\n",
    "            sessions_to_consider = random.sample(self.sessions, self.sample_size)\n",
    "\n",
    "        # Compute similarities to all sessions\n",
    "        similarities = []\n",
    "        for session_id, session_items in sessions_to_consider:\n",
    "            sim = self._session_similarity(current_session, session_items)\n",
    "            similarities.append((session_id, session_items, sim))\n",
    "\n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        # Take top-k similar sessions\n",
    "        top_k_sessions = similarities[:self.k]\n",
    "\n",
    "        # Aggregate scores from top-k sessions\n",
    "        scores = np.zeros(self.n_items)\n",
    "        total_sim = sum(sim for _, _, sim in top_k_sessions)\n",
    "\n",
    "        if total_sim == 0:\n",
    "            return scores\n",
    "\n",
    "        for session_id, session_items, sim in top_k_sessions:\n",
    "            # Weight by similarity and recency (last items have higher weight)\n",
    "            for i, item in enumerate(session_items):\n",
    "                if 0 <= item < self.n_items:\n",
    "                    # Recency weight: more recent items (later in list) get higher weight\n",
    "                    recency_weight = (i + 1) / len(session_items)\n",
    "                    scores[item] += sim * recency_weight\n",
    "\n",
    "        # Normalize by total similarity\n",
    "        scores /= total_sim\n",
    "\n",
    "        return scores\n",
    "\n",
    "# Train Session-KNN\n",
    "print(\"[CELL 06-10C] Training Session-KNN (k=100)...\")\n",
    "sessionknn_model = SessionKNN(n_items=n_items, k=100, sample_size=500)\n",
    "sessionknn_model.fit(pairs_train)\n",
    "\n",
    "# Save\n",
    "save_pickle(MODELS_DIR / 'sessionknn.pkl', sessionknn_model)\n",
    "print(f\"[CELL 06-10C] Saved: {MODELS_DIR / 'sessionknn.pkl'}\")\n",
    "\n",
    "cell_end(\"CELL 06-10C\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-11] Evaluate baselines\n",
      "[CELL 06-11] start=2026-02-01T02:54:11\n",
      "[CELL 06-11] Evaluating Random...\n",
      "  Accuracy@1: 0.0006\n",
      "\n",
      "[CELL 06-11] Evaluating Popularity...\n",
      "  Accuracy@1: 0.0173\n",
      "\n",
      "[CELL 06-11] Evaluating GRU (global)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2425/1657306488.py:45: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
      "  seq = torch.LongTensor([prefix]).to(DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy@1: 0.3355\n",
      "\n",
      "[CELL 06-11] Evaluating SASRec...\n",
      "  Accuracy@1: 0.2198\n",
      "\n",
      "[CELL 06-11] Evaluating Session-KNN...\n",
      "  Accuracy@1: 0.1460\n",
      "\n",
      "[CELL 06-11] Saved: /workspace/anonymous-users-mooc-session-meta/results/baselines_K5_Q10.json\n",
      "[CELL 06-11] elapsed=10.19s\n",
      "[CELL 06-11] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-11] Evaluate all baselines on test episodes\n",
    "\n",
    "t0 = cell_start(\"CELL 06-11\", \"Evaluate baselines\")\n",
    "\n",
    "def evaluate_on_episodes(model, episodes_df: pd.DataFrame, pairs_df: pd.DataFrame, model_type: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate a model on episodes.\n",
    "\n",
    "    For each episode:\n",
    "    - Use query pairs (ignore support for now, this is zero-shot)\n",
    "    - Predict next course for each query pair\n",
    "    - Aggregate metrics\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for _, episode in episodes_df.iterrows():\n",
    "        query_pair_ids = episode[\"query_pair_ids\"]\n",
    "\n",
    "        # Get query pairs\n",
    "        query_pairs = pairs_df[pairs_df[\"pair_id\"].isin(query_pair_ids)].sort_values(\"label_ts_epoch\")\n",
    "\n",
    "        if len(query_pairs) == 0:\n",
    "            continue\n",
    "\n",
    "        labels = query_pairs[\"label\"].values\n",
    "\n",
    "        if model_type == \"random\":\n",
    "            preds = model.predict(len(query_pairs))\n",
    "\n",
    "        elif model_type == \"popularity\":\n",
    "            preds = model.predict(len(query_pairs))\n",
    "\n",
    "        elif model_type == \"gru\":\n",
    "            # Use prefix from each query pair\n",
    "            prefixes = query_pairs[\"prefix\"].tolist()\n",
    "            preds = []\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for prefix in prefixes:\n",
    "                    if len(prefix) > CFG[\"gru_config\"][\"max_seq_len\"]:\n",
    "                        prefix = prefix[-CFG[\"gru_config\"][\"max_seq_len\"]:]\n",
    "\n",
    "                    seq = torch.LongTensor([prefix]).to(DEVICE)\n",
    "                    length = torch.LongTensor([len(prefix)]).to(DEVICE)\n",
    "                    logits = model(seq, length)\n",
    "                    scores = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "                    preds.append(scores)\n",
    "\n",
    "            preds = np.array(preds)\n",
    "\n",
    "        elif model_type == \"sasrec\":\n",
    "            # Use prefix from each query pair\n",
    "            prefixes = query_pairs[\"prefix\"].tolist()\n",
    "            preds = []\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for prefix in prefixes:\n",
    "                    if len(prefix) > 50:  # max_len for SASRec\n",
    "                        prefix = prefix[-50:]\n",
    "\n",
    "                    seq = torch.LongTensor([prefix]).to(DEVICE)\n",
    "                    logits = model(seq)\n",
    "                    scores = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "                    preds.append(scores)\n",
    "\n",
    "            preds = np.array(preds)\n",
    "\n",
    "        elif model_type == \"sessionknn\":\n",
    "            # Use prefix from each query pair\n",
    "            prefixes = query_pairs[\"prefix\"].tolist()\n",
    "            preds = []\n",
    "\n",
    "            for prefix in prefixes:\n",
    "                scores = model.predict(prefix)\n",
    "                preds.append(scores)\n",
    "\n",
    "            preds = np.array(preds)\n",
    "\n",
    "        all_predictions.append(preds)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    # Concatenate all predictions\n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = compute_metrics(all_predictions, all_labels, k_values=[5, 10])\n",
    "    return metrics\n",
    "\n",
    "# Evaluate on test set\n",
    "results = {}\n",
    "\n",
    "print(\"[CELL 06-11] Evaluating Random...\")\n",
    "results[\"random\"] = evaluate_on_episodes(random_model, episodes_test, pairs_test, \"random\")\n",
    "print(f\"  Accuracy@1: {results['random']['accuracy@1']:.4f}\")\n",
    "\n",
    "print(\"\\n[CELL 06-11] Evaluating Popularity...\")\n",
    "results[\"popularity\"] = evaluate_on_episodes(popularity_model, episodes_test, pairs_test, \"popularity\")\n",
    "print(f\"  Accuracy@1: {results['popularity']['accuracy@1']:.4f}\")\n",
    "\n",
    "print(\"\\n[CELL 06-11] Evaluating GRU (global)...\")\n",
    "results[\"gru_global\"] = evaluate_on_episodes(gru_model, episodes_test, pairs_test, \"gru\")\n",
    "print(f\"  Accuracy@1: {results['gru_global']['accuracy@1']:.4f}\")\n",
    "\n",
    "print(\"\\n[CELL 06-11] Evaluating SASRec...\")\n",
    "results[\"sasrec\"] = evaluate_on_episodes(sasrec_model, episodes_test, pairs_test, \"sasrec\")\n",
    "print(f\"  Accuracy@1: {results['sasrec']['accuracy@1']:.4f}\")\n",
    "\n",
    "print(\"\\n[CELL 06-11] Evaluating Session-KNN...\")\n",
    "results[\"sessionknn\"] = evaluate_on_episodes(sessionknn_model, episodes_test, pairs_test, \"sessionknn\")\n",
    "print(f\"  Accuracy@1: {results['sessionknn']['accuracy@1']:.4f}\")\n",
    "\n",
    "# Save results\n",
    "results_with_meta = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"k_shot_config\": {\"K\": K, \"Q\": Q},\n",
    "    \"n_test_episodes\": len(episodes_test),\n",
    "    \"baselines\": results,\n",
    "}\n",
    "write_json_atomic(Path(CFG[\"outputs\"][\"results\"]), results_with_meta)\n",
    "print(f\"\\n[CELL 06-11] Saved: {CFG['outputs']['results']}\")\n",
    "\n",
    "cell_end(\"CELL 06-11\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-12] Results summary\n",
      "[CELL 06-12] start=2026-02-01T02:54:22\n",
      "\n",
      "[CELL 06-12] ========== BASELINE RESULTS (Test Set) ==========\n",
      "K=5, Q=10 | Test Episodes: 313\n",
      "\n",
      "Model                     Acc@1   Recall@5  Recall@10        MRR\n",
      "--------------------------------------------------------------\n",
      "random                   0.0006     0.0022     0.0051     0.0047\n",
      "popularity               0.0173     0.0597     0.0958     0.0498\n",
      "gru_global               0.3355     0.4856     0.5505     0.4111\n",
      "sasrec                   0.2198     0.4042     0.4773     0.3124\n",
      "sessionknn               0.1460     0.3633     0.4326     0.2503\n",
      "[CELL 06-12] elapsed=0.00s\n",
      "[CELL 06-12] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-12] Results summary table\n",
    "\n",
    "t0 = cell_start(\"CELL 06-12\", \"Results summary\")\n",
    "\n",
    "print(\"\\n[CELL 06-12] ========== BASELINE RESULTS (Test Set) ==========\")\n",
    "print(f\"K={K}, Q={Q} | Test Episodes: {len(episodes_test):,}\\n\")\n",
    "\n",
    "# Table header\n",
    "print(f\"{'Model':<20} {'Acc@1':>10} {'Recall@5':>10} {'Recall@10':>10} {'MRR':>10}\")\n",
    "print(\"-\" * 62)\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name:<20} {metrics['accuracy@1']:>10.4f} {metrics['recall@5']:>10.4f} {metrics['recall@10']:>10.4f} {metrics['mrr']:>10.4f}\")\n",
    "\n",
    "cell_end(\"CELL 06-12\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-10C] Session-KNN baseline\n",
      "[CELL 06-10C] start=2026-02-01T02:54:22\n",
      "[CELL 06-10C] Training Session-KNN (k=100)...\n",
      "[CELL 06-10C] Built session database: 49,643 sessions\n",
      "[CELL 06-10C] Saved: /workspace/anonymous-users-mooc-session-meta/models/baselines/sessionknn.pkl\n",
      "[CELL 06-10C] elapsed=13.59s\n",
      "[CELL 06-10C] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-10C] Baseline 5: Session-KNN (V-SKNN)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-10C\", \"Session-KNN baseline\")\n",
    "\n",
    "class SessionKNN:\n",
    "    \"\"\"\n",
    "    Session-based k-Nearest Neighbors (V-SKNN variant).\n",
    "\n",
    "    Finds similar past sessions and recommends items from those sessions.\n",
    "    Similarity based on cosine similarity of session item sets.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_items: int, k: int = 100, sample_size: int = 500):\n",
    "        self.n_items = n_items\n",
    "        self.k = k  # number of nearest neighbor sessions\n",
    "        self.sample_size = sample_size  # max sessions to consider (for efficiency)\n",
    "        self.sessions = []  # list of (session_id, item_list)\n",
    "\n",
    "    def fit(self, pairs_df: pd.DataFrame):\n",
    "        \"\"\"Build session database from training data.\"\"\"\n",
    "        # Group pairs by user to create sessions\n",
    "        sessions_list = []\n",
    "        for user_id, user_pairs in pairs_df.groupby('user_id'):\n",
    "            # Sort by timestamp\n",
    "            user_pairs_sorted = user_pairs.sort_values('label_ts_epoch')\n",
    "\n",
    "            # Extract session as list of items (prefix + label)\n",
    "            session_items = []\n",
    "            for _, row in user_pairs_sorted.iterrows():\n",
    "                prefix = row['prefix']\n",
    "                label = row['label']\n",
    "                # Add items from prefix\n",
    "                if isinstance(prefix, (list, np.ndarray)):\n",
    "                    session_items.extend(prefix)\n",
    "                # Add label\n",
    "                session_items.append(label)\n",
    "\n",
    "            # Store unique items in session\n",
    "            session_items_unique = list(dict.fromkeys(session_items))  # preserve order, remove duplicates\n",
    "            sessions_list.append((user_id, session_items_unique))\n",
    "\n",
    "        self.sessions = sessions_list\n",
    "        print(f\"[CELL 06-10C] Built session database: {len(self.sessions):,} sessions\")\n",
    "\n",
    "    def _session_similarity(self, session1: List[int], session2: List[int]) -> float:\n",
    "        \"\"\"Compute cosine similarity between two sessions (item sets).\"\"\"\n",
    "        set1 = set(session1)\n",
    "        set2 = set(session2)\n",
    "\n",
    "        if len(set1) == 0 or len(set2) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        # Jaccard similarity (intersection over union) as approximation\n",
    "        # More efficient than full cosine for sets\n",
    "        intersection = len(set1 & set2)\n",
    "        union = len(set1 | set2)\n",
    "\n",
    "        return intersection / union if union > 0 else 0.0\n",
    "\n",
    "    def predict(self, current_session: List[int]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict item scores based on k most similar past sessions.\n",
    "\n",
    "        Args:\n",
    "            current_session: List of items in current session (prefix)\n",
    "\n",
    "        Returns:\n",
    "            scores: (n_items,) array of prediction scores\n",
    "        \"\"\"\n",
    "        if len(current_session) == 0 or len(self.sessions) == 0:\n",
    "            return np.zeros(self.n_items)\n",
    "\n",
    "        # Sample sessions for efficiency if too many\n",
    "        sessions_to_consider = self.sessions\n",
    "        if len(self.sessions) > self.sample_size:\n",
    "            import random\n",
    "            sessions_to_consider = random.sample(self.sessions, self.sample_size)\n",
    "\n",
    "        # Compute similarities to all sessions\n",
    "        similarities = []\n",
    "        for session_id, session_items in sessions_to_consider:\n",
    "            sim = self._session_similarity(current_session, session_items)\n",
    "            similarities.append((session_id, session_items, sim))\n",
    "\n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        # Take top-k similar sessions\n",
    "        top_k_sessions = similarities[:self.k]\n",
    "\n",
    "        # Aggregate scores from top-k sessions\n",
    "        scores = np.zeros(self.n_items)\n",
    "        total_sim = sum(sim for _, _, sim in top_k_sessions)\n",
    "\n",
    "        if total_sim == 0:\n",
    "            return scores\n",
    "\n",
    "        for session_id, session_items, sim in top_k_sessions:\n",
    "            # Weight by similarity and recency (last items have higher weight)\n",
    "            for i, item in enumerate(session_items):\n",
    "                if 0 <= item < self.n_items:\n",
    "                    # Recency weight: more recent items (later in list) get higher weight\n",
    "                    recency_weight = (i + 1) / len(session_items)\n",
    "                    scores[item] += sim * recency_weight\n",
    "\n",
    "        # Normalize by total similarity\n",
    "        scores /= total_sim\n",
    "\n",
    "        return scores\n",
    "\n",
    "# Train Session-KNN\n",
    "print(\"[CELL 06-10C] Training Session-KNN (k=100)...\")\n",
    "sessionknn_model = SessionKNN(n_items=n_items, k=100, sample_size=500)\n",
    "sessionknn_model.fit(pairs_train)\n",
    "\n",
    "# Save\n",
    "save_pickle(MODELS_DIR / 'sessionknn.pkl', sessionknn_model)\n",
    "print(f\"[CELL 06-10C] Saved: {MODELS_DIR / 'sessionknn.pkl'}\")\n",
    "\n",
    "cell_end(\"CELL 06-10C\", t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Notebook 06 Complete: Base Model Selection Summary\n\n**Run Information:**\n- Configuration: K=5, Q=10 (5 support pairs, 10 query pairs per episode)\n- Test Episodes: **313 cold-start users**\n- Vocabulary: **1,518 courses**\n\n---\n\n### Base Model Performance on Test Set (Actual Results from CELL 06-12)\n\n| Model | Acc@1 | Recall@5 | Recall@10 | MRR |\n|-------|-------|----------|-----------|-----|\n| **GRU (Global)** | **33.55%** | 48.56% | 55.05% | 0.4111 |\n| **SASRec** | **21.98%** | 40.42% | 47.73% | 0.3124 |\n| **Session-KNN** | **14.60%** | 36.33% | 43.26% | 0.2503 |\n| **Popularity** | **1.73%** | 5.97% | 9.58% | 0.0498 |\n| **Random** | **0.06%** | 0.22% | 0.51% | 0.0047 |\n\n---\n\n### Key Findings\n\n1. **GRU (Global) is the strongest base model** with 33.55% Acc@1\n   - This is the architecture we use for MAML meta-learning\n   - Trained on all 225,168 training pairs\n   - Zero-shot evaluation on cold-start users (no personalization)\n\n2. **Transformer-based SASRec underperforms GRU** by ~11.6 percentage points\n   - Self-attention may require more training data or tuning\n\n3. **Session-KNN is competitive** at 14.60% despite being non-parametric\n   - Uses Jaccard similarity on session item sets\n   - 49,643 training sessions database\n\n4. **Non-personalized models are weak**\n   - Popularity: Only 1.73% (recommends most popular courses)\n   - Random: 0.06% (sanity check ‚âà 1/1518 courses)\n\n---\n\n### Saved Artifacts\n\n**Models:**\n- `models/baselines/random.pkl` (RandomPredictor)\n- `models/baselines/popularity.pkl` (PopularityPredictor)\n- `models/baselines/gru_global.pth` (GRURecommender state_dict)\n- `models/baselines/sasrec.pth` (SASRec state_dict)\n- `models/baselines/sessionknn.pkl` (SessionKNN)\n\n**Results:**\n- `results/baselines_K5_Q10.json` (complete metrics for all models)\n\n---\n\n### Next Steps: Notebook 07 (MAML Meta-Learning)\n\n1. **Implement MAML using GRU as base model**\n   - Meta-train GRU to quickly adapt to new users\n   - Inner loop: Adapt on K=5 support pairs\n   - Outer loop: Optimize meta-initialization across users\n\n2. **Evaluation Protocol**\n   - Zero-shot: Same as GRU base model (no support set)\n   - Few-shot: Adapt using K=5 support pairs, evaluate on query pairs\n\n3. **Success Criteria**\n   - MAML few-shot should improve over zero-shot baseline\n   - Support set should enable effective task-specific adaptation"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (VENV)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}