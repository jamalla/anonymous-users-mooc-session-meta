{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Build Vocab + Pairs (XuetangX)\n",
    "\n",
    "**Purpose:** Build course vocabulary + create prefix→label pairs for next-course prediction.\n",
    "\n",
    "**Cold-Start Focus:**\n",
    "- Vocabulary is **fixed** (343 courses from XuetangX, not cold-start)\n",
    "- Pairs are **user-specific** (support K-shot learning for cold-start users)\n",
    "- Chronological ordering **critical** (no future leakage)\n",
    "\n",
    "**Inputs:**\n",
    "- `data/interim/xuetangx.duckdb` (view: `xuetangx_events_sessionized`)\n",
    "\n",
    "**Outputs:**\n",
    "- `data/processed/xuetangx/vocab/course2id.json` (course_id → int)\n",
    "- `data/processed/xuetangx/vocab/id2course.json` (int → course_id)\n",
    "- `data/processed/xuetangx/pairs/pairs.parquet` (prefix→label pairs)\n",
    "- DuckDB view: `xuetangx_pairs`\n",
    "- `reports/03_build_vocab_pairs_xuetangx/<run_tag>/report.json`\n",
    "\n",
    "**Strategy:**\n",
    "1. Build course vocabulary (alphabetical ordering for determinism)\n",
    "2. Extract course sequences from sessions (chronological)\n",
    "3. Create pairs: prefix=[c0, c1, ..., c_{t-1}], label=c_t\n",
    "4. Validate: no future leakage, all courses in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 03-00] start=2026-01-07T10:25:33\n",
      "[CELL 03-00] CWD: C:\\anonymous-users-mooc-session-meta\\notebooks\n",
      "[CELL 03-00] REPO_ROOT: C:\\anonymous-users-mooc-session-meta\n",
      "[CELL 03-00] META_REGISTRY=C:\\anonymous-users-mooc-session-meta\\meta.json\n",
      "[CELL 03-00] DATA_INTERIM=C:\\anonymous-users-mooc-session-meta\\data\\interim\n",
      "[CELL 03-00] DATA_PROCESSED=C:\\anonymous-users-mooc-session-meta\\data\\processed\n",
      "[CELL 03-00] REPORTS=C:\\anonymous-users-mooc-session-meta\\reports\n",
      "[CELL 03-00] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 03-00] Bootstrap: repo root + paths + logger\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "t0 = datetime.now()\n",
    "print(f\"[CELL 03-00] start={t0.isoformat(timespec='seconds')}\")\n",
    "print(\"[CELL 03-00] CWD:\", Path.cwd().resolve())\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists():\n",
    "            return p\n",
    "    raise RuntimeError(\"Could not find PROJECT_STATE.md. Open notebook from within the repo.\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "print(\"[CELL 03-00] REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "PATHS = {\n",
    "    \"META_REGISTRY\": REPO_ROOT / \"meta.json\",\n",
    "    \"DATA_INTERIM\": REPO_ROOT / \"data\" / \"interim\",\n",
    "    \"DATA_PROCESSED\": REPO_ROOT / \"data\" / \"processed\",\n",
    "    \"REPORTS\": REPO_ROOT / \"reports\",\n",
    "}\n",
    "for k, v in PATHS.items():\n",
    "    print(f\"[CELL 03-00] {k}={v}\")\n",
    "\n",
    "def cell_start(cell_id: str, title: str, **kwargs: Any) -> float:\n",
    "    t = time.time()\n",
    "    print(f\"\\n[{cell_id}] {title}\")\n",
    "    print(f\"[{cell_id}] start={datetime.now().isoformat(timespec='seconds')}\")\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    return t\n",
    "\n",
    "def cell_end(cell_id: str, t0: float, **kwargs: Any) -> None:\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    print(f\"[{cell_id}] elapsed={time.time()-t0:.2f}s\")\n",
    "    print(f\"[{cell_id}] done\")\n",
    "\n",
    "print(\"[CELL 03-00] done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 03-01] Seed everything\n",
      "[CELL 03-01] start=2026-01-07T10:25:33\n",
      "[CELL 03-01] seed=20260107\n",
      "[CELL 03-01] elapsed=0.00s\n",
      "[CELL 03-01] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 03-01] Reproducibility: seed everything\n",
    "\n",
    "t0 = cell_start(\"CELL 03-01\", \"Seed everything\")\n",
    "\n",
    "GLOBAL_SEED = 20260107\n",
    "\n",
    "def seed_everything(seed: int) -> None:\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(GLOBAL_SEED)\n",
    "\n",
    "cell_end(\"CELL 03-01\", t0, seed=GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 03-02] JSON IO + hashing\n",
      "[CELL 03-02] start=2026-01-07T10:25:33\n",
      "[CELL 03-02] elapsed=0.00s\n",
      "[CELL 03-02] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 03-02] JSON IO + hashing helpers\n",
    "\n",
    "t0 = cell_start(\"CELL 03-02\", \"JSON IO + hashing\")\n",
    "\n",
    "def write_json_atomic(path: Path, obj: Any, indent: int = 2) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tmp = path.with_suffix(path.suffix + f\".tmp_{uuid.uuid4().hex}\")\n",
    "    with tmp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=indent)\n",
    "    tmp.replace(path)\n",
    "\n",
    "def read_json(path: Path) -> Any:\n",
    "    if not path.exists():\n",
    "        raise RuntimeError(f\"Missing JSON file: {path}\")\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "cell_end(\"CELL 03-02\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 03-03] Start run + init files + meta.json\n",
      "[CELL 03-03] start=2026-01-07T10:25:33\n",
      "[CELL 03-03] out_dir=C:\\anonymous-users-mooc-session-meta\\reports\\03_build_vocab_pairs_xuetangx\\20260107_102533\n",
      "[CELL 03-03] elapsed=0.02s\n",
      "[CELL 03-03] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 03-03] Run tagging + report/config/manifest + meta.json\n",
    "\n",
    "t0 = cell_start(\"CELL 03-03\", \"Start run + init files + meta.json\")\n",
    "\n",
    "NOTEBOOK_NAME = \"03_build_vocab_pairs_xuetangx\"\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_ID = uuid.uuid4().hex\n",
    "\n",
    "OUT_DIR = PATHS[\"REPORTS\"] / NOTEBOOK_NAME / RUN_TAG\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REPORT_PATH = OUT_DIR / \"report.json\"\n",
    "CONFIG_PATH = OUT_DIR / \"config.json\"\n",
    "MANIFEST_PATH = OUT_DIR / \"manifest.json\"\n",
    "\n",
    "DUCKDB_PATH = PATHS[\"DATA_INTERIM\"] / \"xuetangx.duckdb\"\n",
    "EVENTS_VIEW = \"xuetangx_events_sessionized\"\n",
    "\n",
    "VOCAB_DIR = PATHS[\"DATA_PROCESSED\"] / \"xuetangx\" / \"vocab\"\n",
    "PAIRS_DIR = PATHS[\"DATA_PROCESSED\"] / \"xuetangx\" / \"pairs\"\n",
    "VOCAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PAIRS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_COURSE2ID = VOCAB_DIR / \"course2id.json\"\n",
    "OUT_ID2COURSE = VOCAB_DIR / \"id2course.json\"\n",
    "OUT_PAIRS_PARQUET = PAIRS_DIR / \"pairs.parquet\"\n",
    "\n",
    "CFG = {\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"seed\": GLOBAL_SEED,\n",
    "    \"inputs\": {\n",
    "        \"duckdb_path\": str(DUCKDB_PATH),\n",
    "        \"events_view\": EVENTS_VIEW,\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"course2id\": str(OUT_COURSE2ID),\n",
    "        \"id2course\": str(OUT_ID2COURSE),\n",
    "        \"pairs\": str(OUT_PAIRS_PARQUET),\n",
    "        \"out_dir\": str(OUT_DIR),\n",
    "    },\n",
    "    \"vocab\": {\n",
    "        \"ordering\": \"alphabetical\",  # deterministic\n",
    "        \"start_index\": 0,  # course_id 0, 1, 2, ...\n",
    "    },\n",
    "    \"pairs\": {\n",
    "        \"min_prefix_len\": 1,  # at least 1 course before predicting next\n",
    "        \"max_prefix_len\": None,  # no limit (variable length)\n",
    "        \"deduplicate_consecutive\": True,  # remove consecutive repeats (e.g., [A,A,B] → [A,B])\n",
    "    }\n",
    "}\n",
    "\n",
    "write_json_atomic(CONFIG_PATH, CFG)\n",
    "\n",
    "report = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"repo_root\": str(REPO_ROOT),\n",
    "    \"metrics\": {},\n",
    "    \"key_findings\": [],\n",
    "    \"sanity_samples\": {},\n",
    "    \"data_fingerprints\": {},\n",
    "    \"notes\": [],\n",
    "}\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "manifest = {\"run_id\": RUN_ID, \"notebook\": NOTEBOOK_NAME, \"run_tag\": RUN_TAG, \"artifacts\": []}\n",
    "write_json_atomic(MANIFEST_PATH, manifest)\n",
    "\n",
    "# meta.json append-only\n",
    "META_PATH = PATHS[\"META_REGISTRY\"]\n",
    "if not META_PATH.exists():\n",
    "    write_json_atomic(META_PATH, {\"schema_version\": 1, \"runs\": []})\n",
    "meta = read_json(META_PATH)\n",
    "meta[\"runs\"].append({\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"out_dir\": str(OUT_DIR),\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "})\n",
    "write_json_atomic(META_PATH, meta)\n",
    "\n",
    "cell_end(\"CELL 03-03\", t0, out_dir=str(OUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 03-04] Load sessionized events\n",
      "[CELL 03-04] start=2026-01-07T10:25:33\n",
      "[CELL 03-04] duckdb=C:\\anonymous-users-mooc-session-meta\\data\\interim\\xuetangx.duckdb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f131a5ffbfb46d2ab89f272e6af8ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 03-04] Loaded events shape: (19239571, 5)\n",
      "[CELL 03-04] Columns: ['user_id', 'course_id', 'session_id', 'ts_epoch', 'pos_in_sess']\n",
      "\n",
      "[CELL 03-04] Head(5):\n",
      "user_id                             course_id                             session_id   ts_epoch  pos_in_sess\n",
      "      1                       UQx/Think101x/_     1_ed614e7cbc66a577d760911c5c1684d4 1441241706            1\n",
      "      1                       UQx/Think101x/_     1_ed614e7cbc66a577d760911c5c1684d4 1441241713            2\n",
      "  10000 course-v1:TsinghuaX+40040152X+2015_T2 10000_e20c166625ba46d905c81429dd713c97 1442264613            1\n",
      "  10000 course-v1:TsinghuaX+40040152X+2015_T2 10000_e20c166625ba46d905c81429dd713c97 1442264934            2\n",
      "  10000 course-v1:TsinghuaX+40040152X+2015_T2 10000_e20c166625ba46d905c81429dd713c97 1442264994            3\n",
      "[CELL 03-04] n_events=19239571\n",
      "[CELL 03-04] elapsed=9.22s\n",
      "[CELL 03-04] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 03-04] Load sessionized events from DuckDB\n",
    "\n",
    "t0 = cell_start(\"CELL 03-04\", \"Load sessionized events\", duckdb=str(DUCKDB_PATH))\n",
    "\n",
    "import duckdb\n",
    "\n",
    "if not DUCKDB_PATH.exists():\n",
    "    raise RuntimeError(f\"Missing DuckDB: {DUCKDB_PATH}. Run Notebook 02 first.\")\n",
    "\n",
    "con = duckdb.connect(str(DUCKDB_PATH), read_only=True)\n",
    "\n",
    "# Load events with session + timestamp info\n",
    "events = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        user_id,\n",
    "        course_id,\n",
    "        session_id,\n",
    "        ts_epoch,\n",
    "        pos_in_sess\n",
    "    FROM {EVENTS_VIEW}\n",
    "    ORDER BY user_id, session_id, ts_epoch, pos_in_sess\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "con.close()\n",
    "\n",
    "print(f\"[CELL 03-04] Loaded events shape: {events.shape}\")\n",
    "print(f\"[CELL 03-04] Columns: {list(events.columns)}\")\n",
    "print(f\"\\n[CELL 03-04] Head(5):\")\n",
    "print(events.head(5).to_string(index=False))\n",
    "\n",
    "cell_end(\"CELL 03-04\", t0, n_events=int(events.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 03-05] Build course vocabulary\n",
      "[CELL 03-05] start=2026-01-07T10:25:42\n",
      "[CELL 03-05] Found 343 unique courses\n",
      "[CELL 03-05] First 5 courses: ['AdelaideX/humbio101x/_', 'BIT/ELC05198/2014_T2', 'BIT/PHY1701701/2015_T1', 'BIT/PHY1701702/2015_T1', 'BerkeleyX/CS169_1x/_']\n",
      "[CELL 03-05] Last 5 courses: ['course-v1:test+test+test', 'course-v1:test+test0001+2015_test', 'course-v1:ustcX+LB05203a+2015_T2', 'edX/BlendedX/_', 'ustcX/LB05203a/2014_T2']\n",
      "\n",
      "[CELL 03-05] Vocabulary size (n_items): 343\n",
      "\n",
      "[CELL 03-05] Saved: course2id.json (SHA256: 8c65e26abef28d74...)\n",
      "[CELL 03-05] Saved: id2course.json (SHA256: 04fad75365624bc7...)\n",
      "[CELL 03-05] n_items=343\n",
      "[CELL 03-05] elapsed=1.47s\n",
      "[CELL 03-05] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 03-05] Build course vocabulary (alphabetical, deterministic)\n",
    "\n",
    "t0 = cell_start(\"CELL 03-05\", \"Build course vocabulary\")\n",
    "\n",
    "# Extract unique courses and sort alphabetically\n",
    "unique_courses = sorted(events[\"course_id\"].unique())\n",
    "\n",
    "print(f\"[CELL 03-05] Found {len(unique_courses)} unique courses\")\n",
    "print(f\"[CELL 03-05] First 5 courses: {unique_courses[:5]}\")\n",
    "print(f\"[CELL 03-05] Last 5 courses: {unique_courses[-5:]}\")\n",
    "\n",
    "# Create bidirectional mappings\n",
    "course2id = {course: idx for idx, course in enumerate(unique_courses)}\n",
    "id2course = {idx: course for course, idx in course2id.items()}\n",
    "\n",
    "n_items = len(course2id)\n",
    "print(f\"\\n[CELL 03-05] Vocabulary size (n_items): {n_items}\")\n",
    "\n",
    "# Save vocabularies\n",
    "write_json_atomic(OUT_COURSE2ID, course2id)\n",
    "write_json_atomic(OUT_ID2COURSE, id2course)\n",
    "\n",
    "course2id_sha = sha256_file(OUT_COURSE2ID)\n",
    "id2course_sha = sha256_file(OUT_ID2COURSE)\n",
    "\n",
    "print(f\"\\n[CELL 03-05] Saved: {OUT_COURSE2ID.name} (SHA256: {course2id_sha[:16]}...)\")\n",
    "print(f\"[CELL 03-05] Saved: {OUT_ID2COURSE.name} (SHA256: {id2course_sha[:16]}...)\")\n",
    "\n",
    "cell_end(\"CELL 03-05\", t0, n_items=n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 03-06] Map courses to item IDs\n",
      "[CELL 03-06] start=2026-01-07T10:25:44\n",
      "[CELL 03-06] Mapped all courses to item_id [0, 342]\n",
      "\n",
      "[CELL 03-06] Head(5) with item_id:\n",
      "user_id                             session_id                             course_id  item_id   ts_epoch\n",
      "      1     1_ed614e7cbc66a577d760911c5c1684d4                       UQx/Think101x/_      203 1441241706\n",
      "      1     1_ed614e7cbc66a577d760911c5c1684d4                       UQx/Think101x/_      203 1441241713\n",
      "  10000 10000_e20c166625ba46d905c81429dd713c97 course-v1:TsinghuaX+40040152X+2015_T2      307 1442264613\n",
      "  10000 10000_e20c166625ba46d905c81429dd713c97 course-v1:TsinghuaX+40040152X+2015_T2      307 1442264934\n",
      "  10000 10000_e20c166625ba46d905c81429dd713c97 course-v1:TsinghuaX+40040152X+2015_T2      307 1442264994\n",
      "[CELL 03-06] elapsed=2.70s\n",
      "[CELL 03-06] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 03-06] Map course_id to item_id (integer encoding)\n",
    "\n",
    "t0 = cell_start(\"CELL 03-06\", \"Map courses to item IDs\")\n",
    "\n",
    "events[\"item_id\"] = events[\"course_id\"].map(course2id)\n",
    "\n",
    "# Sanity check: no unmapped courses\n",
    "n_missing = events[\"item_id\"].isna().sum()\n",
    "if n_missing > 0:\n",
    "    raise RuntimeError(f\"Found {n_missing} events with unmapped courses (should be impossible)\")\n",
    "\n",
    "events[\"item_id\"] = events[\"item_id\"].astype(int)\n",
    "\n",
    "print(f\"[CELL 03-06] Mapped all courses to item_id [0, {n_items-1}]\")\n",
    "print(f\"\\n[CELL 03-06] Head(5) with item_id:\")\n",
    "print(events[[\"user_id\", \"session_id\", \"course_id\", \"item_id\", \"ts_epoch\"]].head(5).to_string(index=False))\n",
    "\n",
    "cell_end(\"CELL 03-06\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 03-07] Extract course sequences per session\n",
      "[CELL 03-07] start=2026-01-07T10:25:47\n",
      "[CELL 03-07] Extracted 291,565 session sequences\n",
      "[CELL 03-07] Deduplicate consecutive: True\n",
      "\n",
      "[CELL 03-07] Sample session sequence:\n",
      "  user_id: 1\n",
      "  session_id: 1_ed614e7cbc66a577d760911c5c1684d4\n",
      "  item_seq (first 10): [203]\n",
      "  length: 1\n",
      "[CELL 03-07] n_sessions=291565\n",
      "[CELL 03-07] elapsed=78.42s\n",
      "[CELL 03-07] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 03-07] Extract course sequences per session (deduplicate consecutive)\n",
    "\n",
    "t0 = cell_start(\"CELL 03-07\", \"Extract course sequences per session\")\n",
    "\n",
    "DEDUPE = CFG[\"pairs\"][\"deduplicate_consecutive\"]\n",
    "\n",
    "def dedupe_consecutive(items: list) -> list:\n",
    "    \"\"\"Remove consecutive duplicates: [A, A, B, C, C] → [A, B, C]\"\"\"\n",
    "    if not items:\n",
    "        return []\n",
    "    result = [items[0]]\n",
    "    for item in items[1:]:\n",
    "        if item != result[-1]:\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "# Group by session and extract chronological course sequence\n",
    "session_seqs = []\n",
    "for (user_id, session_id), group in events.groupby([\"user_id\", \"session_id\"]):\n",
    "    # Sort by timestamp within session (already sorted, but explicit)\n",
    "    group = group.sort_values(\"ts_epoch\")\n",
    "    \n",
    "    item_seq = group[\"item_id\"].tolist()\n",
    "    ts_seq = group[\"ts_epoch\"].tolist()\n",
    "    \n",
    "    if DEDUPE:\n",
    "        # Deduplicate consecutive courses (keeps first occurrence timestamp)\n",
    "        deduped_items = []\n",
    "        deduped_ts = []\n",
    "        for i, item in enumerate(item_seq):\n",
    "            if i == 0 or item != item_seq[i-1]:\n",
    "                deduped_items.append(item)\n",
    "                deduped_ts.append(ts_seq[i])\n",
    "        item_seq = deduped_items\n",
    "        ts_seq = deduped_ts\n",
    "    \n",
    "    session_seqs.append({\n",
    "        \"user_id\": user_id,\n",
    "        \"session_id\": session_id,\n",
    "        \"item_seq\": item_seq,\n",
    "        \"ts_seq\": ts_seq,\n",
    "    })\n",
    "\n",
    "print(f\"[CELL 03-07] Extracted {len(session_seqs):,} session sequences\")\n",
    "print(f\"[CELL 03-07] Deduplicate consecutive: {DEDUPE}\")\n",
    "\n",
    "# Sample check\n",
    "print(f\"\\n[CELL 03-07] Sample session sequence:\")\n",
    "sample = session_seqs[0]\n",
    "print(f\"  user_id: {sample['user_id']}\")\n",
    "print(f\"  session_id: {sample['session_id']}\")\n",
    "print(f\"  item_seq (first 10): {sample['item_seq'][:10]}\")\n",
    "print(f\"  length: {len(sample['item_seq'])}\")\n",
    "\n",
    "cell_end(\"CELL 03-07\", t0, n_sessions=len(session_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 03-08] Create prefix→label pairs\n",
      "[CELL 03-08] start=2026-01-07T10:27:05\n",
      "[CELL 03-08] Created 264,229 prefix→label pairs\n",
      "[CELL 03-08] Min prefix length: 1\n",
      "\n",
      "[CELL 03-08] Prefix length distribution:\n",
      "  Min: 1\n",
      "  p50: 3\n",
      "  p90: 22\n",
      "  p99: 140\n",
      "  Max: 901\n",
      "\n",
      "[CELL 03-08] Head(3):\n",
      " pair_id user_id                               session_id          prefix  label  label_ts_epoch  prefix_len\n",
      "       0 1000009 1000009_95163f59939941d9fd47d6c9b17fdaf6           [107]    133      1443241561           1\n",
      "       1 1000009 1000009_95163f59939941d9fd47d6c9b17fdaf6      [107, 133]    334      1443242059           2\n",
      "       2 1000009 1000009_95163f59939941d9fd47d6c9b17fdaf6 [107, 133, 334]    297      1443242155           3\n",
      "[CELL 03-08] n_pairs=264229\n",
      "[CELL 03-08] elapsed=1.64s\n",
      "[CELL 03-08] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 03-08] Create prefix→label pairs (chronological, no future leakage)\n",
    "\n",
    "t0 = cell_start(\"CELL 03-08\", \"Create prefix→label pairs\")\n",
    "\n",
    "MIN_PREFIX_LEN = int(CFG[\"pairs\"][\"min_prefix_len\"])\n",
    "\n",
    "pairs = []\n",
    "pair_id = 0\n",
    "\n",
    "for sess in session_seqs:\n",
    "    user_id = sess[\"user_id\"]\n",
    "    session_id = sess[\"session_id\"]\n",
    "    item_seq = sess[\"item_seq\"]\n",
    "    ts_seq = sess[\"ts_seq\"]\n",
    "    \n",
    "    # Need at least min_prefix_len + 1 items to create a pair\n",
    "    if len(item_seq) < MIN_PREFIX_LEN + 1:\n",
    "        continue\n",
    "    \n",
    "    # Create pairs: for each position t, prefix=[0:t], label=t\n",
    "    for t in range(MIN_PREFIX_LEN, len(item_seq)):\n",
    "        prefix = item_seq[:t]\n",
    "        label = item_seq[t]\n",
    "        \n",
    "        # Timestamps: prefix_max_ts < label_ts (no future leakage)\n",
    "        prefix_ts = ts_seq[:t]\n",
    "        label_ts = ts_seq[t]\n",
    "        \n",
    "        # Skip pairs where prefix_max_ts >= label_ts (can happen after deduplication)\n",
    "        # This ensures strict chronological ordering: all prefix events happen BEFORE label\n",
    "        if max(prefix_ts) >= label_ts:\n",
    "            continue  # Skip this pair (same-timestamp events after deduplication)\n",
    "        \n",
    "        pairs.append({\n",
    "            \"pair_id\": pair_id,\n",
    "            \"user_id\": user_id,\n",
    "            \"session_id\": session_id,\n",
    "            \"prefix\": prefix,  # list[int]\n",
    "            \"label\": int(label),  # int\n",
    "            \"label_ts_epoch\": int(label_ts),  # timestamp of label event\n",
    "            \"prefix_len\": int(len(prefix)),\n",
    "        })\n",
    "        pair_id += 1\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs)\n",
    "\n",
    "print(f\"[CELL 03-08] Created {len(pairs_df):,} prefix→label pairs\")\n",
    "print(f\"[CELL 03-08] Min prefix length: {MIN_PREFIX_LEN}\")\n",
    "print(f\"\\n[CELL 03-08] Prefix length distribution:\")\n",
    "print(f\"  Min: {pairs_df['prefix_len'].min()}\")\n",
    "print(f\"  p50: {pairs_df['prefix_len'].quantile(0.50):.0f}\")\n",
    "print(f\"  p90: {pairs_df['prefix_len'].quantile(0.90):.0f}\")\n",
    "print(f\"  p99: {pairs_df['prefix_len'].quantile(0.99):.0f}\")\n",
    "print(f\"  Max: {pairs_df['prefix_len'].max()}\")\n",
    "\n",
    "print(f\"\\n[CELL 03-08] Head(3):\")\n",
    "print(pairs_df[[\"pair_id\", \"user_id\", \"session_id\", \"prefix\", \"label\", \"label_ts_epoch\", \"prefix_len\"]].head(3).to_string(index=False))\n",
    "\n",
    "cell_end(\"CELL 03-08\", t0, n_pairs=int(len(pairs_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 03-09] Save pairs.parquet\n",
      "[CELL 03-09] start=2026-01-07T10:27:07\n",
      "[CELL 03-09] out=C:\\anonymous-users-mooc-session-meta\\data\\processed\\xuetangx\\pairs\\pairs.parquet\n",
      "[CELL 03-09] Saved: C:\\anonymous-users-mooc-session-meta\\data\\processed\\xuetangx\\pairs\\pairs.parquet\n",
      "[CELL 03-09] Size: 4.8 MB\n",
      "[CELL 03-09] SHA256: 51e62b1f05ea6f2df989f9eb77b2ddc3a684959e8dbc5e19c926950eed95ee4f\n",
      "[CELL 03-09] elapsed=0.56s\n",
      "[CELL 03-09] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 03-09] Save pairs.parquet\n",
    "\n",
    "t0 = cell_start(\"CELL 03-09\", \"Save pairs.parquet\", out=str(OUT_PAIRS_PARQUET))\n",
    "\n",
    "pairs_df.to_parquet(OUT_PAIRS_PARQUET, index=False, compression=\"zstd\")\n",
    "\n",
    "pairs_bytes = int(OUT_PAIRS_PARQUET.stat().st_size)\n",
    "pairs_sha = sha256_file(OUT_PAIRS_PARQUET)\n",
    "\n",
    "print(f\"[CELL 03-09] Saved: {OUT_PAIRS_PARQUET}\")\n",
    "print(f\"[CELL 03-09] Size: {pairs_bytes / 1024 / 1024:.1f} MB\")\n",
    "print(f\"[CELL 03-09] SHA256: {pairs_sha}\")\n",
    "\n",
    "cell_end(\"CELL 03-09\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 03-10] Register DuckDB view\n",
      "[CELL 03-10] start=2026-01-07T10:27:07\n",
      "[CELL 03-10] duckdb=C:\\anonymous-users-mooc-session-meta\\data\\interim\\xuetangx.duckdb\n",
      "[CELL 03-10] View xuetangx_pairs: 264,229 rows\n",
      "[CELL 03-10] Closed DuckDB connection\n",
      "[CELL 03-10] elapsed=0.07s\n",
      "[CELL 03-10] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 03-10] Register DuckDB view for pairs\n",
    "\n",
    "t0 = cell_start(\"CELL 03-10\", \"Register DuckDB view\", duckdb=str(DUCKDB_PATH))\n",
    "\n",
    "con = duckdb.connect(str(DUCKDB_PATH), read_only=False)\n",
    "\n",
    "con.execute(\"DROP VIEW IF EXISTS xuetangx_pairs;\")\n",
    "\n",
    "def esc_path(p: Path) -> str:\n",
    "    return str(p).replace(\"'\", \"''\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "CREATE VIEW xuetangx_pairs AS\n",
    "SELECT * FROM read_parquet('{esc_path(OUT_PAIRS_PARQUET)}')\n",
    "\"\"\")\n",
    "\n",
    "n_pairs = int(con.execute(\"SELECT COUNT(*) FROM xuetangx_pairs\").fetchone()[0])\n",
    "print(f\"[CELL 03-10] View xuetangx_pairs: {n_pairs:,} rows\")\n",
    "\n",
    "con.close()\n",
    "print(f\"[CELL 03-10] Closed DuckDB connection\")\n",
    "\n",
    "cell_end(\"CELL 03-10\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 03-11] Validation checks\n",
      "[CELL 03-11] start=2026-01-07T10:27:07\n",
      "[CELL 03-11] ✅ All labels in vocab [0, 342]\n",
      "[CELL 03-11] ✅ All prefix items in vocab [0, 342]\n",
      "[CELL 03-11] ✅ All prefixes have length >= 1\n",
      "\n",
      "[CELL 03-11] User-level pair counts:\n",
      "  Total users with pairs: 42,171\n",
      "  Min pairs/user: 1\n",
      "  p50 pairs/user: 2\n",
      "  p90 pairs/user: 13\n",
      "  Max pairs/user: 1318\n",
      "[CELL 03-11] elapsed=0.26s\n",
      "[CELL 03-11] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 03-11] Validation: sanity checks\n",
    "\n",
    "t0 = cell_start(\"CELL 03-11\", \"Validation checks\")\n",
    "\n",
    "# Check 1: All labels in vocab\n",
    "invalid_labels = pairs_df[pairs_df[\"label\"] >= n_items]\n",
    "if len(invalid_labels) > 0:\n",
    "    raise RuntimeError(f\"Found {len(invalid_labels)} pairs with label >= n_items={n_items}\")\n",
    "print(f\"[CELL 03-11] ✅ All labels in vocab [0, {n_items-1}]\")\n",
    "\n",
    "# Check 2: All prefix items in vocab\n",
    "all_prefix_items = [item for prefix in pairs_df[\"prefix\"] for item in prefix]\n",
    "max_prefix_item = max(all_prefix_items) if all_prefix_items else -1\n",
    "if max_prefix_item >= n_items:\n",
    "    raise RuntimeError(f\"Found prefix item {max_prefix_item} >= n_items={n_items}\")\n",
    "print(f\"[CELL 03-11] ✅ All prefix items in vocab [0, {n_items-1}]\")\n",
    "\n",
    "# Check 3: No empty prefixes (enforced by MIN_PREFIX_LEN)\n",
    "empty_prefix = pairs_df[pairs_df[\"prefix_len\"] < MIN_PREFIX_LEN]\n",
    "if len(empty_prefix) > 0:\n",
    "    raise RuntimeError(f\"Found {len(empty_prefix)} pairs with prefix_len < {MIN_PREFIX_LEN}\")\n",
    "print(f\"[CELL 03-11] ✅ All prefixes have length >= {MIN_PREFIX_LEN}\")\n",
    "\n",
    "# Check 4: User-level stats\n",
    "user_pair_counts = pairs_df.groupby(\"user_id\").size()\n",
    "print(f\"\\n[CELL 03-11] User-level pair counts:\")\n",
    "print(f\"  Total users with pairs: {len(user_pair_counts):,}\")\n",
    "print(f\"  Min pairs/user: {user_pair_counts.min()}\")\n",
    "print(f\"  p50 pairs/user: {user_pair_counts.quantile(0.50):.0f}\")\n",
    "print(f\"  p90 pairs/user: {user_pair_counts.quantile(0.90):.0f}\")\n",
    "print(f\"  Max pairs/user: {user_pair_counts.max()}\")\n",
    "\n",
    "cell_end(\"CELL 03-11\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 03-12] Write report + manifest\n",
      "[CELL 03-12] start=2026-01-07T10:27:08\n",
      "[CELL 03-12] Updated: C:\\anonymous-users-mooc-session-meta\\reports\\03_build_vocab_pairs_xuetangx\\20260107_102533\\report.json\n",
      "[CELL 03-12] Updated: C:\\anonymous-users-mooc-session-meta\\reports\\03_build_vocab_pairs_xuetangx\\20260107_102533\\manifest.json\n",
      "[CELL 03-12] elapsed=0.04s\n",
      "[CELL 03-12] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 03-12] Update report + manifest\n",
    "\n",
    "t0 = cell_start(\"CELL 03-12\", \"Write report + manifest\")\n",
    "\n",
    "report = read_json(REPORT_PATH)\n",
    "manifest = read_json(MANIFEST_PATH)\n",
    "\n",
    "# Metrics\n",
    "report[\"metrics\"] = {\n",
    "    \"n_items\": n_items,\n",
    "    \"n_pairs\": int(len(pairs_df)),\n",
    "    \"n_users_with_pairs\": int(len(user_pair_counts)),\n",
    "    \"min_prefix_len\": MIN_PREFIX_LEN,\n",
    "    \"prefix_len_p50\": float(pairs_df[\"prefix_len\"].quantile(0.50)),\n",
    "    \"prefix_len_p90\": float(pairs_df[\"prefix_len\"].quantile(0.90)),\n",
    "    \"prefix_len_max\": int(pairs_df[\"prefix_len\"].max()),\n",
    "    \"deduplicate_consecutive\": DEDUPE,\n",
    "}\n",
    "\n",
    "# Key findings\n",
    "report[\"key_findings\"].append(\n",
    "    f\"Built course vocabulary with {n_items} items (alphabetical ordering). \"\n",
    "    f\"Created {len(pairs_df):,} prefix→label pairs for next-course prediction. \"\n",
    "    f\"All pairs validated: chronological ordering, no future leakage, all items in vocab.\"\n",
    ")\n",
    "\n",
    "if DEDUPE:\n",
    "    report[\"key_findings\"].append(\n",
    "        \"Consecutive duplicate courses removed from sequences (e.g., [A,A,B] → [A,B]). \"\n",
    "        \"This focuses prediction on course transitions, not re-engagement patterns.\"\n",
    "    )\n",
    "\n",
    "# Sanity samples\n",
    "report[\"sanity_samples\"][\"pairs_head3\"] = pairs_df.head(3).to_dict(orient=\"records\")\n",
    "report[\"sanity_samples\"][\"vocab_sample\"] = {\n",
    "    \"first_5_courses\": unique_courses[:5],\n",
    "    \"last_5_courses\": unique_courses[-5:],\n",
    "}\n",
    "\n",
    "# Fingerprints\n",
    "report[\"data_fingerprints\"][\"course2id\"] = {\"path\": str(OUT_COURSE2ID), \"sha256\": course2id_sha}\n",
    "report[\"data_fingerprints\"][\"id2course\"] = {\"path\": str(OUT_ID2COURSE), \"sha256\": id2course_sha}\n",
    "report[\"data_fingerprints\"][\"pairs\"] = {\n",
    "    \"path\": str(OUT_PAIRS_PARQUET),\n",
    "    \"bytes\": pairs_bytes,\n",
    "    \"sha256\": pairs_sha,\n",
    "}\n",
    "\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "# Manifest\n",
    "def add_artifact(path: Path) -> None:\n",
    "    rec = {\"path\": str(path), \"bytes\": int(path.stat().st_size), \"sha256\": None, \"sha256_error\": None}\n",
    "    try:\n",
    "        rec[\"sha256\"] = sha256_file(path)\n",
    "    except PermissionError as e:\n",
    "        rec[\"sha256_error\"] = f\"PermissionError: {e}\"\n",
    "    manifest[\"artifacts\"].append(rec)\n",
    "\n",
    "add_artifact(OUT_COURSE2ID)\n",
    "add_artifact(OUT_ID2COURSE)\n",
    "add_artifact(OUT_PAIRS_PARQUET)\n",
    "\n",
    "write_json_atomic(MANIFEST_PATH, manifest)\n",
    "\n",
    "print(f\"[CELL 03-12] Updated: {REPORT_PATH}\")\n",
    "print(f\"[CELL 03-12] Updated: {MANIFEST_PATH}\")\n",
    "\n",
    "cell_end(\"CELL 03-12\", t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Notebook 03 Complete\n",
    "\n",
    "**Outputs:**\n",
    "- ✅ `data/processed/xuetangx/vocab/course2id.json` (343 courses)\n",
    "- ✅ `data/processed/xuetangx/vocab/id2course.json`\n",
    "- ✅ `data/processed/xuetangx/pairs/pairs.parquet` (all prefix→label pairs)\n",
    "- ✅ DuckDB view: `xuetangx_pairs`\n",
    "- ✅ `reports/03_build_vocab_pairs_xuetangx/<run_tag>/report.json`\n",
    "\n",
    "**Validation Passed:**\n",
    "- ✅ All labels in vocab [0, n_items-1]\n",
    "- ✅ All prefix items in vocab\n",
    "- ✅ Chronological ordering (prefix_max_ts < label_ts)\n",
    "- ✅ No future leakage\n",
    "\n",
    "**Next:** Notebook 04 (User Split)\n",
    "- Deterministic user-level split (80/10/10)\n",
    "- Disjoint train/val/test users (cold-start guarantee)\n",
    "- Split pairs by user assignment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
