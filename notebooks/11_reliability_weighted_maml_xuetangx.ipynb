{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 11: Reliability-Weighted MAML (XuetangX)\n\n## Contribution 3: Session Reliability for Meta-Learning Adaptation\n\n**Core Idea:** Higher-reliability sessions provide higher-confidence evidence for MAML adaptation. We weight the inner loop loss by session reliability scores.\n\n### Key Modification\n```python\n# Standard MAML (NB07):\nsupport_loss = criterion(logits, labels).mean()\n\n# Reliability-Weighted MAML (this notebook):\nper_sample_loss = criterion_none(logits, labels)  # reduction='none'\nsupport_loss = (reliability_weights * per_sample_loss).sum() / reliability_weights.sum()\n```\n\n### Reliability Score Definition\n```\nreliability = (intensity + extent + composition) / 3\n\nwhere:\n  intensity   = min(n_events / 100, 1.0)      # Event count\n  extent      = min(duration_sec / 1800, 1.0) # Session duration (capped at 30 min)\n  composition = n_action_types / 8            # Behavioral diversity\n```\n\n### Results: Comparison with Vanilla MAML (NB07)\n\n| Method | Test HR@10 | Test NDCG@10 | \n|--------|------------|--------------|\n| Vanilla MAML (NB07) | 47.35% | 37.41% |\n| **Reliability-Weighted MAML (NB11)** | **48.34%** | **37.71%** |\n| **Improvement** | **+0.99%** | **+0.30%** |\n\n**Finding:** Reliability weighting improves cold-start recommendation by giving more weight to high-quality sessions during adaptation.\n\n### Dataset (XuetangX)\n- Training episodes: 47,357 | Validation: 341 | Test: 313\n- Pairs with reliability: 281,979\n- Reliability score range: [0.0485, 1.0000]\n- Vocabulary: 1,518 courses\n\n### Inputs\n- `data/processed/xuetangx/pairs_with_reliability/pairs.parquet` (from NB03b)\n- `data/processed/xuetangx/episodes/episodes_*.parquet` (from NB05)\n\n### Outputs\n- `models/maml/reliability_weighted_maml.pt`\n- `reports/11_reliability_weighted_maml_xuetangx/<run_tag>/report.json`"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 11-00] start=2026-02-04T03:16:58\n",
      "[CELL 11-00] PyTorch: 2.10.0+cu128\n",
      "[CELL 11-00] CUDA available: True\n",
      "[CELL 11-00] REPO_ROOT: /workspace/anonymous-users-mooc-session-meta\n",
      "[CELL 11-00] Device: cuda\n",
      "[CELL 11-00] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11-00] Bootstrap\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "t0 = datetime.now()\n",
    "print(f\"[CELL 11-00] start={t0.isoformat(timespec='seconds')}\")\n",
    "print(f\"[CELL 11-00] PyTorch: {torch.__version__}\")\n",
    "print(f\"[CELL 11-00] CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists():\n",
    "            return p\n",
    "    raise RuntimeError(\"Could not find PROJECT_STATE.md.\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "print(f\"[CELL 11-00] REPO_ROOT: {REPO_ROOT}\")\n",
    "\n",
    "PATHS = {\n",
    "    \"DATA_PROCESSED\": REPO_ROOT / \"data\" / \"processed\",\n",
    "    \"MODELS\": REPO_ROOT / \"models\",\n",
    "    \"REPORTS\": REPO_ROOT / \"reports\",\n",
    "}\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[CELL 11-00] Device: {DEVICE}\")\n",
    "\n",
    "def cell_start(cell_id: str, title: str, **kwargs) -> float:\n",
    "    t = time.time()\n",
    "    print(f\"\\n[{cell_id}] {title}\")\n",
    "    print(f\"[{cell_id}] start={datetime.now().isoformat(timespec='seconds')}\")\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    return t\n",
    "\n",
    "def cell_end(cell_id: str, t0: float, **kwargs) -> None:\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    print(f\"[{cell_id}] elapsed={time.time()-t0:.2f}s\")\n",
    "    print(f\"[{cell_id}] done\")\n",
    "\n",
    "print(\"[CELL 11-00] done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 11-01] Seed everything\n",
      "[CELL 11-01] start=2026-02-04T03:16:59\n",
      "[CELL 11-01] seed=20260107\n",
      "[CELL 11-01] elapsed=0.00s\n",
      "[CELL 11-01] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11-01] Reproducibility + JSON helpers\n",
    "\n",
    "t0 = cell_start(\"CELL 11-01\", \"Seed everything\")\n",
    "\n",
    "GLOBAL_SEED = 20260107\n",
    "\n",
    "def seed_everything(seed: int) -> None:\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(GLOBAL_SEED)\n",
    "\n",
    "def write_json_atomic(path: Path, obj: Any, indent: int = 2) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tmp = path.with_suffix(path.suffix + f\".tmp_{uuid.uuid4().hex}\")\n",
    "    with tmp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=indent)\n",
    "    tmp.replace(path)\n",
    "\n",
    "def read_json(path: Path) -> Any:\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "cell_end(\"CELL 11-01\", t0, seed=GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 11-02] Configuration\n",
      "[CELL 11-02] start=2026-02-04T03:16:59\n",
      "[CELL 11-02] K=5, Q=10\n",
      "[CELL 11-02] Reliability weighting: True\n",
      "[CELL 11-02] Output dir: /workspace/anonymous-users-mooc-session-meta/reports/11_reliability_weighted_maml_xuetangx/20260204_031659\n",
      "[CELL 11-02] elapsed=0.00s\n",
      "[CELL 11-02] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11-02] Configuration\n",
    "\n",
    "t0 = cell_start(\"CELL 11-02\", \"Configuration\")\n",
    "\n",
    "NOTEBOOK_NAME = \"11_reliability_weighted_maml_xuetangx\"\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_ID = uuid.uuid4().hex\n",
    "\n",
    "OUT_DIR = PATHS[\"REPORTS\"] / NOTEBOOK_NAME / RUN_TAG\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Episode configuration\n",
    "K = 5   # Support set size\n",
    "Q = 10  # Query set size\n",
    "\n",
    "# Paths\n",
    "EPISODES_DIR = PATHS[\"DATA_PROCESSED\"] / \"xuetangx\" / \"episodes\"\n",
    "PAIRS_DIR = PATHS[\"DATA_PROCESSED\"] / \"xuetangx\" / \"pairs_with_reliability\"  # NEW: Use reliability pairs\n",
    "VOCAB_DIR = PATHS[\"DATA_PROCESSED\"] / \"xuetangx\" / \"vocab\"\n",
    "MODELS_DIR = PATHS[\"MODELS\"] / \"maml\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CFG = {\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"seed\": GLOBAL_SEED,\n",
    "    \"K\": K,\n",
    "    \"Q\": Q,\n",
    "    \"inputs\": {\n",
    "        \"episodes_train\": str(EPISODES_DIR / f\"episodes_train_K{K}_Q{Q}.parquet\"),\n",
    "        \"episodes_val\": str(EPISODES_DIR / f\"episodes_val_K{K}_Q{Q}.parquet\"),\n",
    "        \"episodes_test\": str(EPISODES_DIR / f\"episodes_test_K{K}_Q{Q}.parquet\"),\n",
    "        \"pairs_with_reliability\": str(PAIRS_DIR / \"pairs.parquet\"),\n",
    "        \"session_reliability\": str(PAIRS_DIR / \"session_reliability.parquet\"),\n",
    "        \"course2id\": str(VOCAB_DIR / \"course2id.json\"),\n",
    "        \"pretrained_gru\": str(PATHS[\"MODELS\"] / \"gru\" / \"gru4rec_pretrained.pt\"),\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"model\": str(MODELS_DIR / \"reliability_weighted_maml.pt\"),\n",
    "        \"report\": str(OUT_DIR / \"report.json\"),\n",
    "    },\n",
    "    \"gru_config\": {\n",
    "        \"embedding_dim\": 64,\n",
    "        \"hidden_dim\": 128,\n",
    "        \"num_layers\": 1,\n",
    "        \"dropout\": 0.2,\n",
    "        \"max_seq_len\": 50,\n",
    "    },\n",
    "    \"maml_config\": {\n",
    "        \"inner_lr\": 0.01,\n",
    "        \"outer_lr\": 0.001,\n",
    "        \"num_inner_steps\": 5,\n",
    "        \"meta_batch_size\": 32,\n",
    "        \"num_meta_iterations\": 3000,\n",
    "        \"use_second_order\": False,  # FOMAML for efficiency\n",
    "        \"use_reliability_weighting\": True,  # KEY: Enable reliability weighting\n",
    "    },\n",
    "    \"eval_config\": {\n",
    "        \"eval_every\": 100,\n",
    "        \"patience\": 10,\n",
    "    },\n",
    "}\n",
    "\n",
    "write_json_atomic(OUT_DIR / \"config.json\", CFG)\n",
    "\n",
    "print(f\"[CELL 11-02] K={K}, Q={Q}\")\n",
    "print(f\"[CELL 11-02] Reliability weighting: {CFG['maml_config']['use_reliability_weighting']}\")\n",
    "print(f\"[CELL 11-02] Output dir: {OUT_DIR}\")\n",
    "\n",
    "cell_end(\"CELL 11-02\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 11-03] Load data with reliability\n",
      "[CELL 11-03] start=2026-02-04T03:16:59\n",
      "[CELL 11-03] Episodes train: 47,357\n",
      "[CELL 11-03] Episodes val: 341\n",
      "[CELL 11-03] Episodes test: 313\n",
      "[CELL 11-03] Loaded pairs with reliability: 281,979\n",
      "[CELL 11-03] Reliability range: [0.0485, 1.0000]\n",
      "[CELL 11-03] Vocabulary: 1518 items\n",
      "[CELL 11-03] has_reliability=True\n",
      "[CELL 11-03] elapsed=0.28s\n",
      "[CELL 11-03] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11-03] Load data with reliability scores\n",
    "\n",
    "t0 = cell_start(\"CELL 11-03\", \"Load data with reliability\")\n",
    "\n",
    "# Load episodes\n",
    "episodes_train = pd.read_parquet(CFG[\"inputs\"][\"episodes_train\"])\n",
    "episodes_val = pd.read_parquet(CFG[\"inputs\"][\"episodes_val\"])\n",
    "episodes_test = pd.read_parquet(CFG[\"inputs\"][\"episodes_test\"])\n",
    "\n",
    "print(f\"[CELL 11-03] Episodes train: {len(episodes_train):,}\")\n",
    "print(f\"[CELL 11-03] Episodes val: {len(episodes_val):,}\")\n",
    "print(f\"[CELL 11-03] Episodes test: {len(episodes_test):,}\")\n",
    "\n",
    "# Load pairs with reliability (from NB03b)\n",
    "pairs_path = Path(CFG[\"inputs\"][\"pairs_with_reliability\"])\n",
    "if pairs_path.exists():\n",
    "    pairs_all = pd.read_parquet(pairs_path)\n",
    "    HAS_RELIABILITY = 'session_reliability' in pairs_all.columns\n",
    "    print(f\"[CELL 11-03] Loaded pairs with reliability: {len(pairs_all):,}\")\n",
    "    if HAS_RELIABILITY:\n",
    "        print(f\"[CELL 11-03] Reliability range: [{pairs_all['session_reliability'].min():.4f}, {pairs_all['session_reliability'].max():.4f}]\")\n",
    "else:\n",
    "    # Fallback to original pairs without reliability\n",
    "    print(f\"[CELL 11-03] WARNING: Pairs with reliability not found, using original pairs\")\n",
    "    pairs_all = pd.read_parquet(PATHS[\"DATA_PROCESSED\"] / \"xuetangx\" / \"pairs\" / \"pairs.parquet\")\n",
    "    pairs_all['session_reliability'] = 1.0  # Default uniform weight\n",
    "    HAS_RELIABILITY = False\n",
    "\n",
    "# Load vocab\n",
    "course2id = read_json(Path(CFG[\"inputs\"][\"course2id\"]))\n",
    "n_items = len(course2id)\n",
    "print(f\"[CELL 11-03] Vocabulary: {n_items} items\")\n",
    "\n",
    "# Create pair_id → reliability mapping\n",
    "pair_reliability_map = pairs_all.set_index('pair_id')['session_reliability'].to_dict()\n",
    "\n",
    "cell_end(\"CELL 11-03\", t0, has_reliability=HAS_RELIABILITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 11-04] Define GRU4Rec model\n",
      "[CELL 11-04] start=2026-02-04T03:16:59\n",
      "[CELL 11-04] GRU4Rec model defined\n",
      "[CELL 11-04] elapsed=0.00s\n",
      "[CELL 11-04] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11-04] GRU4Rec Model (same as NB07)\n",
    "\n",
    "t0 = cell_start(\"CELL 11-04\", \"Define GRU4Rec model\")\n",
    "\n",
    "class GRU4Rec(nn.Module):\n",
    "    \"\"\"GRU-based sequential recommendation model.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_items: int, embedding_dim: int, hidden_dim: int,\n",
    "                 num_layers: int = 1, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.n_items = n_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.item_embedding = nn.Embedding(n_items, embedding_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_dim, n_items)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, lengths: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len) item indices\n",
    "            lengths: (batch_size,) actual sequence lengths\n",
    "        Returns:\n",
    "            logits: (batch_size, n_items)\n",
    "        \"\"\"\n",
    "        embedded = self.item_embedding(x)  # (batch, seq, emb)\n",
    "        \n",
    "        if lengths is not None:\n",
    "            packed = nn.utils.rnn.pack_padded_sequence(\n",
    "                embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            _, hidden = self.gru(packed)\n",
    "        else:\n",
    "            _, hidden = self.gru(embedded)\n",
    "        \n",
    "        # hidden: (num_layers, batch, hidden_dim) -> take last layer\n",
    "        last_hidden = hidden[-1]  # (batch, hidden_dim)\n",
    "        logits = self.output_layer(last_hidden)  # (batch, n_items)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def get_named_parameters(self) -> OrderedDict:\n",
    "        \"\"\"Get parameters as OrderedDict for MAML.\"\"\"\n",
    "        return OrderedDict(self.named_parameters())\n",
    "\n",
    "print(f\"[CELL 11-04] GRU4Rec model defined\")\n",
    "cell_end(\"CELL 11-04\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 11-05] Initialize model\n",
      "[CELL 11-05] start=2026-02-04T03:16:59\n",
      "[CELL 11-05] No pretrained model found, using random init\n",
      "[CELL 11-05] Model parameters: 367,470\n",
      "[CELL 11-05] elapsed=0.23s\n",
      "[CELL 11-05] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11-05] Initialize model\n",
    "\n",
    "t0 = cell_start(\"CELL 11-05\", \"Initialize model\")\n",
    "\n",
    "gru_cfg = CFG[\"gru_config\"]\n",
    "\n",
    "meta_model = GRU4Rec(\n",
    "    n_items=n_items,\n",
    "    embedding_dim=gru_cfg[\"embedding_dim\"],\n",
    "    hidden_dim=gru_cfg[\"hidden_dim\"],\n",
    "    num_layers=gru_cfg[\"num_layers\"],\n",
    "    dropout=gru_cfg[\"dropout\"]\n",
    ").to(DEVICE)\n",
    "\n",
    "# Load pretrained weights if available\n",
    "pretrained_path = Path(CFG[\"inputs\"][\"pretrained_gru\"])\n",
    "if pretrained_path.exists():\n",
    "    state_dict = torch.load(pretrained_path, map_location=DEVICE)\n",
    "    meta_model.load_state_dict(state_dict)\n",
    "    print(f\"[CELL 11-05] Loaded pretrained GRU from {pretrained_path}\")\n",
    "else:\n",
    "    print(f\"[CELL 11-05] No pretrained model found, using random init\")\n",
    "\n",
    "n_params = sum(p.numel() for p in meta_model.parameters())\n",
    "print(f\"[CELL 11-05] Model parameters: {n_params:,}\")\n",
    "\n",
    "cell_end(\"CELL 11-05\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 11-06] Define episode helpers\n",
      "[CELL 11-06] start=2026-02-04T03:16:59\n",
      "[CELL 11-06] Episode helpers defined\n",
      "[CELL 11-06] elapsed=0.00s\n",
      "[CELL 11-06] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11-06] Episode data extraction helpers\n",
    "\n",
    "t0 = cell_start(\"CELL 11-06\", \"Define episode helpers\")\n",
    "\n",
    "def get_episode_data_with_reliability(episode_row, pairs_df, pair_reliability_map):\n",
    "    \"\"\"\n",
    "    Extract support and query pairs for an episode WITH reliability scores.\n",
    "    \n",
    "    Returns:\n",
    "        support_pairs: DataFrame with support set\n",
    "        query_pairs: DataFrame with query set\n",
    "        support_reliability: list of reliability scores for support pairs\n",
    "    \"\"\"\n",
    "    support_pair_ids = episode_row[\"support_pair_ids\"]\n",
    "    query_pair_ids = episode_row[\"query_pair_ids\"]\n",
    "    \n",
    "    support_pairs = pairs_df[pairs_df[\"pair_id\"].isin(support_pair_ids)].sort_values(\"label_ts_epoch\")\n",
    "    query_pairs = pairs_df[pairs_df[\"pair_id\"].isin(query_pair_ids)].sort_values(\"label_ts_epoch\")\n",
    "    \n",
    "    # Get reliability scores for support pairs\n",
    "    support_reliability = [pair_reliability_map.get(pid, 1.0) for pid in support_pairs['pair_id']]\n",
    "    \n",
    "    return support_pairs, query_pairs, support_reliability\n",
    "\n",
    "\n",
    "def prepare_batch(pairs_df, max_seq_len: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Prepare batch tensors from pairs DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        sequences: (batch, max_seq_len) padded sequences\n",
    "        lengths: (batch,) actual lengths\n",
    "        labels: (batch,) target labels\n",
    "    \"\"\"\n",
    "    batch_size = len(pairs_df)\n",
    "    sequences = torch.zeros(batch_size, max_seq_len, dtype=torch.long)\n",
    "    lengths = torch.zeros(batch_size, dtype=torch.long)\n",
    "    labels = torch.zeros(batch_size, dtype=torch.long)\n",
    "    \n",
    "    for i, (_, row) in enumerate(pairs_df.iterrows()):\n",
    "        prefix = row[\"prefix\"]\n",
    "        seq_len = min(len(prefix), max_seq_len)\n",
    "        \n",
    "        # Take last max_seq_len items (most recent)\n",
    "        if len(prefix) > max_seq_len:\n",
    "            prefix = prefix[-max_seq_len:]\n",
    "        \n",
    "        sequences[i, :seq_len] = torch.tensor(prefix[:seq_len])\n",
    "        lengths[i] = seq_len\n",
    "        labels[i] = row[\"label\"]\n",
    "    \n",
    "    return sequences, lengths, labels\n",
    "\n",
    "print(f\"[CELL 11-06] Episode helpers defined\")\n",
    "cell_end(\"CELL 11-06\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 11-07] Define Reliability-Weighted MAML\n",
      "[CELL 11-07] start=2026-02-04T03:16:59\n",
      "[CELL 11-07] Reliability-Weighted MAML inner loop defined (FIXED)\n",
      "[CELL 11-07] Key modification: weighted_loss = (reliability * per_sample_loss).sum() / reliability.sum()\n",
      "[CELL 11-07] Now computes gradients w.r.t. model.parameters() directly\n",
      "[CELL 11-07] elapsed=0.00s\n",
      "[CELL 11-07] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11-07] Reliability-Weighted MAML Inner Loop (KEY CONTRIBUTION) - FIXED\n",
    "\n",
    "t0 = cell_start(\"CELL 11-07\", \"Define Reliability-Weighted MAML\")\n",
    "\n",
    "import copy\n",
    "\n",
    "def reliability_weighted_inner_loop(\n",
    "    model: nn.Module,\n",
    "    support_seqs: torch.Tensor,\n",
    "    support_lengths: torch.Tensor,\n",
    "    support_labels: torch.Tensor,\n",
    "    support_reliability: torch.Tensor,\n",
    "    inner_lr: float,\n",
    "    num_inner_steps: int,\n",
    "    use_second_order: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    MAML inner loop with reliability-weighted loss.\n",
    "    Updates model parameters IN-PLACE.\n",
    "    \n",
    "    KEY MODIFICATION: Weight each support sample's loss by its reliability score.\n",
    "    \n",
    "    Standard MAML:   L = mean(loss_i)\n",
    "    Reliability:     L = sum(w_i * loss_i) / sum(w_i)\n",
    "    \"\"\"\n",
    "    # Criterion WITHOUT reduction (to get per-sample losses)\n",
    "    criterion_none = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "    for step in range(num_inner_steps):\n",
    "        # Forward pass using current model parameters\n",
    "        logits = model(support_seqs, support_lengths)\n",
    "        \n",
    "        # Per-sample cross-entropy loss\n",
    "        per_sample_loss = criterion_none(logits, support_labels)  # (K,)\n",
    "        \n",
    "        # RELIABILITY-WEIGHTED LOSS (KEY CONTRIBUTION)\n",
    "        # Higher reliability → higher weight → more influence on adaptation\n",
    "        weighted_loss = (support_reliability * per_sample_loss).sum() / (support_reliability.sum() + 1e-8)\n",
    "        \n",
    "        # Compute gradients w.r.t. model parameters directly\n",
    "        grads = torch.autograd.grad(\n",
    "            weighted_loss,\n",
    "            model.parameters(),\n",
    "            create_graph=use_second_order,\n",
    "            allow_unused=True  # Some params may not be used\n",
    "        )\n",
    "        \n",
    "        # Update model parameters in-place\n",
    "        with torch.no_grad():\n",
    "            for param, grad in zip(model.parameters(), grads):\n",
    "                if grad is not None:\n",
    "                    param.sub_(inner_lr * grad)\n",
    "\n",
    "\n",
    "def standard_inner_loop(\n",
    "    model: nn.Module,\n",
    "    support_seqs: torch.Tensor,\n",
    "    support_lengths: torch.Tensor,\n",
    "    support_labels: torch.Tensor,\n",
    "    inner_lr: float,\n",
    "    num_inner_steps: int,\n",
    "    use_second_order: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Standard MAML inner loop (for baseline comparison).\n",
    "    Updates model parameters IN-PLACE.\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for step in range(num_inner_steps):\n",
    "        logits = model(support_seqs, support_lengths)\n",
    "        loss = criterion(logits, support_labels)\n",
    "        \n",
    "        grads = torch.autograd.grad(\n",
    "            loss,\n",
    "            model.parameters(),\n",
    "            create_graph=use_second_order,\n",
    "            allow_unused=True\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for param, grad in zip(model.parameters(), grads):\n",
    "                if grad is not None:\n",
    "                    param.sub_(inner_lr * grad)\n",
    "\n",
    "\n",
    "print(f\"[CELL 11-07] Reliability-Weighted MAML inner loop defined (FIXED)\")\n",
    "print(f\"[CELL 11-07] Key modification: weighted_loss = (reliability * per_sample_loss).sum() / reliability.sum()\")\n",
    "print(f\"[CELL 11-07] Now computes gradients w.r.t. model.parameters() directly\")\n",
    "\n",
    "cell_end(\"CELL 11-07\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 11-08] Meta-training setup\n",
      "[CELL 11-08] start=2026-02-04T03:16:59\n",
      "[CELL 11-08] Inner LR: 0.01\n",
      "[CELL 11-08] Outer LR: 0.001\n",
      "[CELL 11-08] Inner steps: 5\n",
      "[CELL 11-08] Meta batch size: 32\n",
      "[CELL 11-08] Meta iterations: 3000\n",
      "[CELL 11-08] Use reliability weighting: True\n",
      "[CELL 11-08] Use second order: False\n",
      "[CELL 11-08] elapsed=0.72s\n",
      "[CELL 11-08] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11-08] Meta-training setup\n",
    "\n",
    "t0 = cell_start(\"CELL 11-08\", \"Meta-training setup\")\n",
    "\n",
    "maml_cfg = CFG[\"maml_config\"]\n",
    "\n",
    "meta_optimizer = torch.optim.Adam(meta_model.parameters(), lr=maml_cfg[\"outer_lr\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "inner_lr = maml_cfg[\"inner_lr\"]\n",
    "num_inner_steps = maml_cfg[\"num_inner_steps\"]\n",
    "meta_batch_size = maml_cfg[\"meta_batch_size\"]\n",
    "num_meta_iterations = maml_cfg[\"num_meta_iterations\"]\n",
    "max_seq_len = CFG[\"gru_config\"][\"max_seq_len\"]\n",
    "use_second_order = maml_cfg[\"use_second_order\"]\n",
    "use_reliability_weighting = maml_cfg[\"use_reliability_weighting\"]\n",
    "\n",
    "print(f\"[CELL 11-08] Inner LR: {inner_lr}\")\n",
    "print(f\"[CELL 11-08] Outer LR: {maml_cfg['outer_lr']}\")\n",
    "print(f\"[CELL 11-08] Inner steps: {num_inner_steps}\")\n",
    "print(f\"[CELL 11-08] Meta batch size: {meta_batch_size}\")\n",
    "print(f\"[CELL 11-08] Meta iterations: {num_meta_iterations}\")\n",
    "print(f\"[CELL 11-08] Use reliability weighting: {use_reliability_weighting}\")\n",
    "print(f\"[CELL 11-08] Use second order: {use_second_order}\")\n",
    "\n",
    "cell_end(\"CELL 11-08\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 11-09] Define evaluation\n",
      "[CELL 11-09] start=2026-02-04T03:17:00\n",
      "[CELL 11-09] Evaluation function defined (FIXED)\n",
      "[CELL 11-09] elapsed=0.00s\n",
      "[CELL 11-09] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11-09] Evaluation function - FIXED\n",
    "\n",
    "t0 = cell_start(\"CELL 11-09\", \"Define evaluation\")\n",
    "\n",
    "import warnings\n",
    "\n",
    "def evaluate_maml(\n",
    "    model: nn.Module,\n",
    "    episodes_df: pd.DataFrame,\n",
    "    pairs_df: pd.DataFrame,\n",
    "    pair_reliability_map: dict,\n",
    "    inner_lr: float,\n",
    "    num_inner_steps: int,\n",
    "    max_seq_len: int,\n",
    "    use_reliability_weighting: bool,\n",
    "    max_episodes: int = 100,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate MAML on validation/test episodes.\n",
    "    \n",
    "    Returns:\n",
    "        metrics: Dict with HR@10, NDCG@10, etc.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_hr10 = []\n",
    "    all_ndcg10 = []\n",
    "    \n",
    "    episodes_sample = episodes_df.sample(min(max_episodes, len(episodes_df)), random_state=GLOBAL_SEED)\n",
    "    \n",
    "    for _, episode in tqdm(episodes_sample.iterrows(), total=len(episodes_sample), desc=\"Evaluating\"):\n",
    "        # Save original weights (deep copy)\n",
    "        original_state = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        # Get episode data\n",
    "        support_pairs, query_pairs, support_reliability = get_episode_data_with_reliability(\n",
    "            episode, pairs_df, pair_reliability_map\n",
    "        )\n",
    "        \n",
    "        if len(support_pairs) == 0 or len(query_pairs) == 0:\n",
    "            model.load_state_dict(original_state)\n",
    "            continue\n",
    "        \n",
    "        # Prepare support batch\n",
    "        support_seqs, support_lengths, support_labels = prepare_batch(support_pairs, max_seq_len)\n",
    "        support_seqs = support_seqs.to(DEVICE)\n",
    "        support_lengths = support_lengths.to(DEVICE)\n",
    "        support_labels = support_labels.to(DEVICE)\n",
    "        support_reliability_tensor = torch.tensor(support_reliability, dtype=torch.float32).to(DEVICE)\n",
    "        \n",
    "        # Inner loop adaptation (updates model in-place)\n",
    "        model.train()\n",
    "        if use_reliability_weighting:\n",
    "            reliability_weighted_inner_loop(\n",
    "                model, support_seqs, support_lengths, support_labels,\n",
    "                support_reliability_tensor, inner_lr, num_inner_steps, False\n",
    "            )\n",
    "        else:\n",
    "            standard_inner_loop(\n",
    "                model, support_seqs, support_lengths, support_labels,\n",
    "                inner_lr, num_inner_steps, False\n",
    "            )\n",
    "        model.eval()\n",
    "        \n",
    "        # Evaluate on query set\n",
    "        query_seqs, query_lengths, query_labels = prepare_batch(query_pairs, max_seq_len)\n",
    "        query_seqs = query_seqs.to(DEVICE)\n",
    "        query_lengths = query_lengths.to(DEVICE)\n",
    "        query_labels = query_labels.to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            query_logits = model(query_seqs, query_lengths)\n",
    "            \n",
    "            # Compute HR@10, NDCG@10\n",
    "            _, top10_indices = query_logits.topk(10, dim=1)\n",
    "            \n",
    "            for i, label in enumerate(query_labels):\n",
    "                hit = (top10_indices[i] == label).any().item()\n",
    "                all_hr10.append(float(hit))\n",
    "                \n",
    "                if hit:\n",
    "                    rank = (top10_indices[i] == label).nonzero(as_tuple=True)[0].item() + 1\n",
    "                    ndcg = 1.0 / np.log2(rank + 1)\n",
    "                else:\n",
    "                    ndcg = 0.0\n",
    "                all_ndcg10.append(ndcg)\n",
    "        \n",
    "        # Restore original weights\n",
    "        model.load_state_dict(original_state)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return {\n",
    "        \"HR@10\": np.mean(all_hr10) * 100 if all_hr10 else 0.0,\n",
    "        \"NDCG@10\": np.mean(all_ndcg10) * 100 if all_ndcg10 else 0.0,\n",
    "        \"n_queries\": len(all_hr10),\n",
    "    }\n",
    "\n",
    "print(f\"[CELL 11-09] Evaluation function defined (FIXED)\")\n",
    "cell_end(\"CELL 11-09\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 11-10] Meta-training loop (FOMAML)\n",
      "[CELL 11-10] start=2026-02-04T03:17:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827fde1036fc46c0a65adc2c87efd86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Meta-training:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8425f0cb10482bb2a74f7e60ecb92c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 100] Loss: 5.8223 | Val HR@10: 32.00% | Val NDCG@10: 23.09%\n",
      "  -> New best! Saved model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52590abbdbca40c3a4cdc2b743dcdb74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 200] Loss: 5.2474 | Val HR@10: 34.80% | Val NDCG@10: 26.02%\n",
      "  -> New best! Saved model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc40cbcc15e34451852da683d750029e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 300] Loss: 5.7127 | Val HR@10: 36.60% | Val NDCG@10: 26.88%\n",
      "  -> New best! Saved model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641d873bca88403ca77f955e5404b43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 400] Loss: 5.2427 | Val HR@10: 40.00% | Val NDCG@10: 28.75%\n",
      "  -> New best! Saved model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b482da5c499044cbbbff8fbd9141fc86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 500] Loss: 5.4818 | Val HR@10: 41.60% | Val NDCG@10: 29.95%\n",
      "  -> New best! Saved model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b238bf2c8e47039ad97bc6a07e8ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 600] Loss: 4.9888 | Val HR@10: 44.00% | Val NDCG@10: 32.37%\n",
      "  -> New best! Saved model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d447dca7c0942c59294fa3d82ce8e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 700] Loss: 4.5757 | Val HR@10: 45.00% | Val NDCG@10: 33.05%\n",
      "  -> New best! Saved model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82008830999a4349a49259479a0208cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 800] Loss: 4.1929 | Val HR@10: 47.60% | Val NDCG@10: 34.28%\n",
      "  -> New best! Saved model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfcd8521c02a4dc28b186ced8c22946c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 900] Loss: 4.9273 | Val HR@10: 49.00% | Val NDCG@10: 35.96%\n",
      "  -> New best! Saved model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d7ac5aa1354877bbae96c8d2a997d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 1000] Loss: 4.3856 | Val HR@10: 49.80% | Val NDCG@10: 36.97%\n",
      "  -> New best! Saved model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082d9df451ce427590b1fc992326d1ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 1100] Loss: 4.9536 | Val HR@10: 50.80% | Val NDCG@10: 36.95%\n",
      "  -> New best! Saved model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb8d69167b84ea98d1d1837861a7514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 1200] Loss: 4.5574 | Val HR@10: 50.00% | Val NDCG@10: 37.03%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054caa3f403a40d089fb7d3f0f281477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 1300] Loss: 4.3743 | Val HR@10: 49.60% | Val NDCG@10: 37.37%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d080ededab0742f49a30e5d1658abcd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 1400] Loss: 4.0337 | Val HR@10: 49.80% | Val NDCG@10: 37.53%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c094c33d3eb4709a933b3aba3c8b9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 1500] Loss: 3.8936 | Val HR@10: 50.00% | Val NDCG@10: 38.35%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6364dcf07f0a4e74a5685bbbe116fc18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 1600] Loss: 3.2242 | Val HR@10: 48.80% | Val NDCG@10: 37.93%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9358d4cc50574d88841367f4ed110e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 1700] Loss: 3.3212 | Val HR@10: 50.80% | Val NDCG@10: 39.36%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c08da51313a40fba9b347c95d045b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 1800] Loss: 3.2605 | Val HR@10: 51.20% | Val NDCG@10: 39.58%\n",
      "  -> New best! Saved model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4896247880ed483db4e4cc9d262a3ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 1900] Loss: 3.7080 | Val HR@10: 51.60% | Val NDCG@10: 40.49%\n",
      "  -> New best! Saved model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f804a888ca4230affbf89ff3e8fe02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 2000] Loss: 3.2254 | Val HR@10: 50.80% | Val NDCG@10: 39.68%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87fd14b828d481d8c05303c44a4f791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 2100] Loss: 3.2472 | Val HR@10: 50.80% | Val NDCG@10: 40.03%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c369e18f09fa462694bdd0a2f13ac8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 2200] Loss: 3.5952 | Val HR@10: 51.60% | Val NDCG@10: 40.37%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b364ebc68c4a35a2f2cd09873f7d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 2300] Loss: 3.3283 | Val HR@10: 51.60% | Val NDCG@10: 41.30%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f743f96c72e43a996d4725dff3fe462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 2400] Loss: 2.7498 | Val HR@10: 51.60% | Val NDCG@10: 41.04%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071d6919a1894bae93146a7c1bfc5445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 2500] Loss: 2.9639 | Val HR@10: 52.60% | Val NDCG@10: 41.46%\n",
      "  -> New best! Saved model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37900a7d15e04e75833d082b9cd8cc00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 2600] Loss: 3.3968 | Val HR@10: 52.40% | Val NDCG@10: 41.79%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91045fd677e5423fbc7b225b85b0c5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 2700] Loss: 2.4927 | Val HR@10: 51.60% | Val NDCG@10: 41.32%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a7e741520344b8b3cc80782de275be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 2800] Loss: 2.9464 | Val HR@10: 52.20% | Val NDCG@10: 41.56%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce2d4bdbeea493da6e0efafe6101af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 2900] Loss: 2.9473 | Val HR@10: 52.20% | Val NDCG@10: 41.81%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0809796952d419f8d74d24a483a92df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iter 3000] Loss: 2.5238 | Val HR@10: 52.00% | Val NDCG@10: 41.68%\n",
      "\n",
      "[CELL 11-10] Training complete!\n",
      "[CELL 11-10] Best Val HR@10: 52.60% at iteration 2500\n",
      "[CELL 11-10] best_val_hr10=52.60%\n",
      "[CELL 11-10] elapsed=2348.10s\n",
      "[CELL 11-10] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11-10] Meta-training loop - FIXED for FOMAML\n",
    "\n",
    "t0 = cell_start(\"CELL 11-10\", \"Meta-training loop (FOMAML)\")\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Filter out the RNN contiguous memory warning (expected with deepcopy)\n",
    "warnings.filterwarnings('ignore', message='RNN module weights are not part of single contiguous chunk of memory')\n",
    "\n",
    "train_users = episodes_train[\"user_id\"].unique()\n",
    "eval_every = CFG[\"eval_config\"][\"eval_every\"]\n",
    "patience = CFG[\"eval_config\"][\"patience\"]\n",
    "\n",
    "best_val_hr10 = 0.0\n",
    "best_iteration = 0\n",
    "patience_counter = 0\n",
    "\n",
    "training_log = []\n",
    "\n",
    "meta_model.train()\n",
    "\n",
    "for meta_iter in tqdm(range(num_meta_iterations), desc=\"Meta-training\"):\n",
    "    # Sample meta-batch of episodes\n",
    "    meta_batch_users = np.random.choice(train_users, size=min(meta_batch_size, len(train_users)), replace=False)\n",
    "    meta_batch_episodes = episodes_train[episodes_train[\"user_id\"].isin(meta_batch_users)]\n",
    "    \n",
    "    if len(meta_batch_episodes) == 0:\n",
    "        continue\n",
    "    \n",
    "    meta_batch_episodes = meta_batch_episodes.sample(min(meta_batch_size, len(meta_batch_episodes)))\n",
    "    \n",
    "    meta_optimizer.zero_grad()\n",
    "    \n",
    "    # Zero gradients on meta_model\n",
    "    for p in meta_model.parameters():\n",
    "        if p.grad is not None:\n",
    "            p.grad.zero_()\n",
    "    \n",
    "    total_query_loss = 0.0\n",
    "    n_valid_episodes = 0\n",
    "    \n",
    "    for _, episode in meta_batch_episodes.iterrows():\n",
    "        # Get episode data with reliability\n",
    "        support_pairs, query_pairs, support_reliability = get_episode_data_with_reliability(\n",
    "            episode, pairs_all, pair_reliability_map\n",
    "        )\n",
    "        \n",
    "        if len(support_pairs) == 0 or len(query_pairs) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Prepare batches\n",
    "        support_seqs, support_lengths, support_labels = prepare_batch(support_pairs, max_seq_len)\n",
    "        support_seqs = support_seqs.to(DEVICE)\n",
    "        support_lengths = support_lengths.to(DEVICE)\n",
    "        support_labels = support_labels.to(DEVICE)\n",
    "        support_reliability_tensor = torch.tensor(support_reliability, dtype=torch.float32).to(DEVICE)\n",
    "        \n",
    "        query_seqs, query_lengths, query_labels = prepare_batch(query_pairs, max_seq_len)\n",
    "        query_seqs = query_seqs.to(DEVICE)\n",
    "        query_lengths = query_lengths.to(DEVICE)\n",
    "        query_labels = query_labels.to(DEVICE)\n",
    "        \n",
    "        # FOMAML: Deep copy model for this episode\n",
    "        adapted_model = copy.deepcopy(meta_model)\n",
    "        adapted_model.train()\n",
    "        \n",
    "        # Flatten GRU parameters to avoid memory warning and improve performance\n",
    "        adapted_model.gru.flatten_parameters()\n",
    "        \n",
    "        # Inner loop with reliability weighting (updates adapted_model in-place)\n",
    "        if use_reliability_weighting:\n",
    "            reliability_weighted_inner_loop(\n",
    "                adapted_model, support_seqs, support_lengths, support_labels,\n",
    "                support_reliability_tensor, inner_lr, num_inner_steps, use_second_order=False\n",
    "            )\n",
    "        else:\n",
    "            standard_inner_loop(\n",
    "                adapted_model, support_seqs, support_lengths, support_labels,\n",
    "                inner_lr, num_inner_steps, use_second_order=False\n",
    "            )\n",
    "        \n",
    "        # Query loss with adapted model\n",
    "        query_logits = adapted_model(query_seqs, query_lengths)\n",
    "        query_loss = criterion(query_logits, query_labels)\n",
    "        \n",
    "        # Backward on adapted model to get gradients\n",
    "        query_loss.backward()\n",
    "        \n",
    "        # FOMAML: Transfer gradients from adapted_model to meta_model\n",
    "        with torch.no_grad():\n",
    "            for meta_p, adapted_p in zip(meta_model.parameters(), adapted_model.parameters()):\n",
    "                if adapted_p.grad is not None:\n",
    "                    if meta_p.grad is None:\n",
    "                        meta_p.grad = adapted_p.grad.clone()\n",
    "                    else:\n",
    "                        meta_p.grad.add_(adapted_p.grad)\n",
    "        \n",
    "        total_query_loss += query_loss.item()\n",
    "        n_valid_episodes += 1\n",
    "        \n",
    "        # Clean up adapted model\n",
    "        del adapted_model\n",
    "    \n",
    "    if n_valid_episodes > 0:\n",
    "        # Average gradients across episodes\n",
    "        for p in meta_model.parameters():\n",
    "            if p.grad is not None:\n",
    "                p.grad.div_(n_valid_episodes)\n",
    "        \n",
    "        # Gradient step\n",
    "        meta_optimizer.step()\n",
    "        \n",
    "        avg_loss = total_query_loss / n_valid_episodes\n",
    "    else:\n",
    "        avg_loss = 0.0\n",
    "    \n",
    "    # Evaluation\n",
    "    if (meta_iter + 1) % eval_every == 0:\n",
    "        val_metrics = evaluate_maml(\n",
    "            meta_model, episodes_val, pairs_all, pair_reliability_map,\n",
    "            inner_lr, num_inner_steps, max_seq_len, use_reliability_weighting,\n",
    "            max_episodes=50\n",
    "        )\n",
    "        \n",
    "        log_entry = {\n",
    "            \"iteration\": meta_iter + 1,\n",
    "            \"train_loss\": avg_loss,\n",
    "            \"val_HR@10\": val_metrics[\"HR@10\"],\n",
    "            \"val_NDCG@10\": val_metrics[\"NDCG@10\"],\n",
    "        }\n",
    "        training_log.append(log_entry)\n",
    "        \n",
    "        print(f\"\\n[Iter {meta_iter+1}] Loss: {avg_loss:.4f} | Val HR@10: {val_metrics['HR@10']:.2f}% | Val NDCG@10: {val_metrics['NDCG@10']:.2f}%\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_metrics[\"HR@10\"] > best_val_hr10:\n",
    "            best_val_hr10 = val_metrics[\"HR@10\"]\n",
    "            best_iteration = meta_iter + 1\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save best model\n",
    "            torch.save(meta_model.state_dict(), CFG[\"outputs\"][\"model\"])\n",
    "            print(f\"  -> New best! Saved model.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping at iteration {meta_iter+1}\")\n",
    "                break\n",
    "\n",
    "print(f\"\\n[CELL 11-10] Training complete!\")\n",
    "print(f\"[CELL 11-10] Best Val HR@10: {best_val_hr10:.2f}% at iteration {best_iteration}\")\n",
    "\n",
    "cell_end(\"CELL 11-10\", t0, best_val_hr10=f\"{best_val_hr10:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 11-11] Final test evaluation\n",
      "[CELL 11-11] start=2026-02-04T03:56:08\n",
      "[CELL 11-11] Loaded best model from /workspace/anonymous-users-mooc-session-meta/models/maml/reliability_weighted_maml.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435d4da3e0e048a7a17d283941446885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 11-11] ===== TEST RESULTS =====\n",
      "[CELL 11-11] Test HR@10:   48.34%\n",
      "[CELL 11-11] Test NDCG@10: 37.71%\n",
      "[CELL 11-11] N queries:    3130\n",
      "[CELL 11-11] ===========================\n",
      "[CELL 11-11] test_HR10=48.34%\n",
      "[CELL 11-11] elapsed=6.99s\n",
      "[CELL 11-11] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11-11] Final evaluation on test set\n",
    "\n",
    "t0 = cell_start(\"CELL 11-11\", \"Final test evaluation\")\n",
    "\n",
    "# Load best model\n",
    "best_model_path = Path(CFG[\"outputs\"][\"model\"])\n",
    "if best_model_path.exists():\n",
    "    meta_model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "    print(f\"[CELL 11-11] Loaded best model from {best_model_path}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics = evaluate_maml(\n",
    "    meta_model, episodes_test, pairs_all, pair_reliability_map,\n",
    "    inner_lr, num_inner_steps, max_seq_len, use_reliability_weighting,\n",
    "    max_episodes=len(episodes_test)  # Evaluate on all test episodes\n",
    ")\n",
    "\n",
    "print(f\"\\n[CELL 11-11] ===== TEST RESULTS =====\")\n",
    "print(f\"[CELL 11-11] Test HR@10:   {test_metrics['HR@10']:.2f}%\")\n",
    "print(f\"[CELL 11-11] Test NDCG@10: {test_metrics['NDCG@10']:.2f}%\")\n",
    "print(f\"[CELL 11-11] N queries:    {test_metrics['n_queries']}\")\n",
    "print(f\"[CELL 11-11] ===========================\")\n",
    "\n",
    "cell_end(\"CELL 11-11\", t0, test_HR10=f\"{test_metrics['HR@10']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 11-12] Save report\n",
      "[CELL 11-12] start=2026-02-04T03:56:15\n",
      "[CELL 11-12] Report saved to /workspace/anonymous-users-mooc-session-meta/reports/11_reliability_weighted_maml_xuetangx/20260204_031659/report.json\n",
      "[CELL 11-12] elapsed=0.00s\n",
      "[CELL 11-12] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 11-12] Save report\n",
    "\n",
    "t0 = cell_start(\"CELL 11-12\", \"Save report\")\n",
    "\n",
    "report = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"config\": CFG,\n",
    "    \"results\": {\n",
    "        \"best_val_HR@10\": best_val_hr10,\n",
    "        \"best_iteration\": best_iteration,\n",
    "        \"test_HR@10\": test_metrics[\"HR@10\"],\n",
    "        \"test_NDCG@10\": test_metrics[\"NDCG@10\"],\n",
    "        \"test_n_queries\": test_metrics[\"n_queries\"],\n",
    "    },\n",
    "    \"training_log\": training_log,\n",
    "    \"key_findings\": [\n",
    "        f\"Reliability-Weighted MAML achieved {test_metrics['HR@10']:.2f}% HR@10 on test set.\",\n",
    "        f\"Best validation HR@10: {best_val_hr10:.2f}% at iteration {best_iteration}.\",\n",
    "        f\"Reliability weighting {'enabled' if use_reliability_weighting else 'disabled'}.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "write_json_atomic(OUT_DIR / \"report.json\", report)\n",
    "\n",
    "print(f\"[CELL 11-12] Report saved to {OUT_DIR / 'report.json'}\")\n",
    "\n",
    "cell_end(\"CELL 11-12\", t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Notebook 11 Complete\n\n### Main Result: Reliability-Weighted MAML vs Vanilla MAML\n\n| Method | Test HR@10 | Test NDCG@10 | Source |\n|--------|------------|--------------|--------|\n| Vanilla MAML | 47.35% | 37.41% | NB07 |\n| **Reliability-Weighted MAML** | **48.34%** | **37.71%** | NB11 |\n| **Improvement** | **+0.99%** | **+0.30%** | |\n\n**Conclusion:** Weighting the MAML inner loop by session reliability improves cold-start recommendation performance.\n\n---\n\n### Training Details\n\n| Parameter | Value |\n|-----------|-------|\n| Meta-iterations | 3,000 |\n| Inner LR | 0.01 |\n| Outer LR | 0.001 |\n| Inner steps | 5 |\n| Meta batch size | 32 |\n| Best Val HR@10 | **52.60%** (iteration 2500) |\n| Training time | 2,348s (~39 min) |\n\n### Training Progression\n| Iteration | Val HR@10 |\n|-----------|-----------|\n| 100 | 32.00% |\n| 500 | 41.60% |\n| 1000 | 49.80% |\n| 1500 | 50.00% |\n| 2000 | 50.80% |\n| **2500** | **52.60%** (best) |\n| 3000 | 52.00% |\n\n### Outputs\n- Model: `models/maml/reliability_weighted_maml.pt`\n- Report: `reports/11_reliability_weighted_maml_xuetangx/20260204_031659/report.json`\n\n### Key Contribution (Contribution 3)\n\n**Reliability-Weighted Inner Loop:**\n```python\nweighted_loss = (reliability * per_sample_loss).sum() / reliability.sum()\n```\n\n**Reliability Score:**\n```\nreliability = (intensity + extent + composition) / 3\n  - intensity:   min(n_events / 100, 1.0)\n  - extent:      min(duration_sec / 1800, 1.0)  \n  - composition: n_action_types / 8\n```\n\n**Intuition:** Sessions with more events, longer duration, and diverse actions provide more reliable signals for user preference learning. These high-reliability sessions should have more influence during MAML adaptation.\n\n### Validation (from NB10)\n- Point-biserial correlation between reliability and return: r = 0.217 (p < 0.001)\n- Users who return have higher mean reliability: 0.279 vs 0.192\n- Reliability predicts user engagement → valid quality signal"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (VENV)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}