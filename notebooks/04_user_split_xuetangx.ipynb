{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 04: User Split (XuetangX)\n",
    "\n",
    "**Purpose:** Create deterministic user-level split (80/10/10) for cold-start evaluation.\n",
    "\n",
    "**Cold-Start Focus:**\n",
    "- **Disjoint users** across train/val/test (no user appears in multiple splits)\n",
    "- Test users are **completely unseen** during training (true cold-start)\n",
    "- Split pairs by user assignment (user's ALL pairs go to same split)\n",
    "\n",
    "**Inputs:**\n",
    "- `data/processed/xuetangx/pairs/pairs.parquet` (264K pairs, 42K users)\n",
    "\n",
    "**Outputs:**\n",
    "- `data/processed/xuetangx/user_splits/users_train.json` (80% of users)\n",
    "- `data/processed/xuetangx/user_splits/users_val.json` (10% of users)\n",
    "- `data/processed/xuetangx/user_splits/users_test.json` (10% of users)\n",
    "- `data/processed/xuetangx/pairs/pairs_train.parquet`\n",
    "- `data/processed/xuetangx/pairs/pairs_val.parquet`\n",
    "- `data/processed/xuetangx/pairs/pairs_test.parquet`\n",
    "- DuckDB views: `xuetangx_pairs_train`, `xuetangx_pairs_val`, `xuetangx_pairs_test`\n",
    "- `reports/04_user_split_xuetangx/<run_tag>/report.json`\n",
    "\n",
    "**Strategy:**\n",
    "1. Load all pairs from Notebook 03\n",
    "2. Extract unique users, sort deterministically (alphabetical)\n",
    "3. Split users: 80% train, 10% val, 10% test (seeded random shuffle)\n",
    "4. Assign all pairs for each user to corresponding split\n",
    "5. Save user lists + split pairs separately\n",
    "6. Validate: no user overlap, all pairs assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 04-00] start=2026-01-07T14:40:34\n",
      "[CELL 04-00] CWD: C:\\anonymous-users-mooc-session-meta\\notebooks\n",
      "[CELL 04-00] REPO_ROOT: C:\\anonymous-users-mooc-session-meta\n",
      "[CELL 04-00] META_REGISTRY=C:\\anonymous-users-mooc-session-meta\\meta.json\n",
      "[CELL 04-00] DATA_INTERIM=C:\\anonymous-users-mooc-session-meta\\data\\interim\n",
      "[CELL 04-00] DATA_PROCESSED=C:\\anonymous-users-mooc-session-meta\\data\\processed\n",
      "[CELL 04-00] REPORTS=C:\\anonymous-users-mooc-session-meta\\reports\n",
      "[CELL 04-00] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-00] Bootstrap: repo root + paths + logger\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "t0 = datetime.now()\n",
    "print(f\"[CELL 04-00] start={t0.isoformat(timespec='seconds')}\")\n",
    "print(\"[CELL 04-00] CWD:\", Path.cwd().resolve())\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists():\n",
    "            return p\n",
    "    raise RuntimeError(\"Could not find PROJECT_STATE.md. Open notebook from within the repo.\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "print(\"[CELL 04-00] REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "PATHS = {\n",
    "    \"META_REGISTRY\": REPO_ROOT / \"meta.json\",\n",
    "    \"DATA_INTERIM\": REPO_ROOT / \"data\" / \"interim\",\n",
    "    \"DATA_PROCESSED\": REPO_ROOT / \"data\" / \"processed\",\n",
    "    \"REPORTS\": REPO_ROOT / \"reports\",\n",
    "}\n",
    "for k, v in PATHS.items():\n",
    "    print(f\"[CELL 04-00] {k}={v}\")\n",
    "\n",
    "def cell_start(cell_id: str, title: str, **kwargs: Any) -> float:\n",
    "    t = time.time()\n",
    "    print(f\"\\n[{cell_id}] {title}\")\n",
    "    print(f\"[{cell_id}] start={datetime.now().isoformat(timespec='seconds')}\")\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    return t\n",
    "\n",
    "def cell_end(cell_id: str, t0: float, **kwargs: Any) -> None:\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    print(f\"[{cell_id}] elapsed={time.time()-t0:.2f}s\")\n",
    "    print(f\"[{cell_id}] done\")\n",
    "\n",
    "print(\"[CELL 04-00] done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 04-01] Seed everything\n",
      "[CELL 04-01] start=2026-01-07T14:40:34\n",
      "[CELL 04-01] seed=20260107\n",
      "[CELL 04-01] elapsed=0.00s\n",
      "[CELL 04-01] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-01] Reproducibility: seed everything\n",
    "\n",
    "t0 = cell_start(\"CELL 04-01\", \"Seed everything\")\n",
    "\n",
    "GLOBAL_SEED = 20260107\n",
    "\n",
    "def seed_everything(seed: int) -> None:\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(GLOBAL_SEED)\n",
    "\n",
    "cell_end(\"CELL 04-01\", t0, seed=GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 04-02] JSON IO + hashing\n",
      "[CELL 04-02] start=2026-01-07T14:40:34\n",
      "[CELL 04-02] elapsed=0.00s\n",
      "[CELL 04-02] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-02] JSON IO + hashing helpers\n",
    "\n",
    "t0 = cell_start(\"CELL 04-02\", \"JSON IO + hashing\")\n",
    "\n",
    "def write_json_atomic(path: Path, obj: Any, indent: int = 2) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tmp = path.with_suffix(path.suffix + f\".tmp_{uuid.uuid4().hex}\")\n",
    "    with tmp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=indent)\n",
    "    tmp.replace(path)\n",
    "\n",
    "def read_json(path: Path) -> Any:\n",
    "    if not path.exists():\n",
    "        raise RuntimeError(f\"Missing JSON file: {path}\")\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "cell_end(\"CELL 04-02\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 04-03] Start run + init files + meta.json\n",
      "[CELL 04-03] start=2026-01-07T14:40:34\n",
      "[CELL 04-03] out_dir=C:\\anonymous-users-mooc-session-meta\\reports\\04_user_split_xuetangx\\20260107_144034\n",
      "[CELL 04-03] elapsed=0.01s\n",
      "[CELL 04-03] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-03] Run tagging + report/config/manifest + meta.json\n",
    "\n",
    "t0 = cell_start(\"CELL 04-03\", \"Start run + init files + meta.json\")\n",
    "\n",
    "NOTEBOOK_NAME = \"04_user_split_xuetangx\"\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_ID = uuid.uuid4().hex\n",
    "\n",
    "OUT_DIR = PATHS[\"REPORTS\"] / NOTEBOOK_NAME / RUN_TAG\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REPORT_PATH = OUT_DIR / \"report.json\"\n",
    "CONFIG_PATH = OUT_DIR / \"config.json\"\n",
    "MANIFEST_PATH = OUT_DIR / \"manifest.json\"\n",
    "\n",
    "DUCKDB_PATH = PATHS[\"DATA_INTERIM\"] / \"xuetangx.duckdb\"\n",
    "PAIRS_PARQUET = PATHS[\"DATA_PROCESSED\"] / \"xuetangx\" / \"pairs\" / \"pairs.parquet\"\n",
    "\n",
    "USER_SPLITS_DIR = PATHS[\"DATA_PROCESSED\"] / \"xuetangx\" / \"user_splits\"\n",
    "PAIRS_DIR = PATHS[\"DATA_PROCESSED\"] / \"xuetangx\" / \"pairs\"\n",
    "USER_SPLITS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_USERS_TRAIN = USER_SPLITS_DIR / \"users_train.json\"\n",
    "OUT_USERS_VAL = USER_SPLITS_DIR / \"users_val.json\"\n",
    "OUT_USERS_TEST = USER_SPLITS_DIR / \"users_test.json\"\n",
    "\n",
    "OUT_PAIRS_TRAIN = PAIRS_DIR / \"pairs_train.parquet\"\n",
    "OUT_PAIRS_VAL = PAIRS_DIR / \"pairs_val.parquet\"\n",
    "OUT_PAIRS_TEST = PAIRS_DIR / \"pairs_test.parquet\"\n",
    "\n",
    "CFG = {\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"seed\": GLOBAL_SEED,\n",
    "    \"inputs\": {\n",
    "        \"pairs\": str(PAIRS_PARQUET),\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"users_train\": str(OUT_USERS_TRAIN),\n",
    "        \"users_val\": str(OUT_USERS_VAL),\n",
    "        \"users_test\": str(OUT_USERS_TEST),\n",
    "        \"pairs_train\": str(OUT_PAIRS_TRAIN),\n",
    "        \"pairs_val\": str(OUT_PAIRS_VAL),\n",
    "        \"pairs_test\": str(OUT_PAIRS_TEST),\n",
    "        \"out_dir\": str(OUT_DIR),\n",
    "    },\n",
    "    \"split\": {\n",
    "        \"train_ratio\": 0.8,\n",
    "        \"val_ratio\": 0.1,\n",
    "        \"test_ratio\": 0.1,\n",
    "        \"strategy\": \"user_level_disjoint\",  # no user overlap between splits\n",
    "        \"ordering\": \"alphabetical_then_shuffle\",  # deterministic user ordering before split\n",
    "    }\n",
    "}\n",
    "\n",
    "write_json_atomic(CONFIG_PATH, CFG)\n",
    "\n",
    "report = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"repo_root\": str(REPO_ROOT),\n",
    "    \"metrics\": {},\n",
    "    \"key_findings\": [],\n",
    "    \"sanity_samples\": {},\n",
    "    \"data_fingerprints\": {},\n",
    "    \"notes\": [],\n",
    "}\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "manifest = {\"run_id\": RUN_ID, \"notebook\": NOTEBOOK_NAME, \"run_tag\": RUN_TAG, \"artifacts\": []}\n",
    "write_json_atomic(MANIFEST_PATH, manifest)\n",
    "\n",
    "# meta.json append-only\n",
    "META_PATH = PATHS[\"META_REGISTRY\"]\n",
    "if not META_PATH.exists():\n",
    "    write_json_atomic(META_PATH, {\"schema_version\": 1, \"runs\": []})\n",
    "meta = read_json(META_PATH)\n",
    "meta[\"runs\"].append({\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"out_dir\": str(OUT_DIR),\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "})\n",
    "write_json_atomic(META_PATH, meta)\n",
    "\n",
    "cell_end(\"CELL 04-03\", t0, out_dir=str(OUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 04-04] Load pairs\n",
      "[CELL 04-04] start=2026-01-07T14:40:34\n",
      "[CELL 04-04] pairs=C:\\anonymous-users-mooc-session-meta\\data\\processed\\xuetangx\\pairs\\pairs.parquet\n",
      "[CELL 04-04] Loaded pairs shape: (264229, 7)\n",
      "[CELL 04-04] Columns: ['pair_id', 'user_id', 'session_id', 'prefix', 'label', 'label_ts_epoch', 'prefix_len']\n",
      "\n",
      "[CELL 04-04] Head(3):\n",
      " pair_id user_id                               session_id          prefix  label  label_ts_epoch  prefix_len\n",
      "       0 1000009 1000009_95163f59939941d9fd47d6c9b17fdaf6           [107]    133      1443241561           1\n",
      "       1 1000009 1000009_95163f59939941d9fd47d6c9b17fdaf6      [107, 133]    334      1443242059           2\n",
      "       2 1000009 1000009_95163f59939941d9fd47d6c9b17fdaf6 [107, 133, 334]    297      1443242155           3\n",
      "[CELL 04-04] n_pairs=264229\n",
      "[CELL 04-04] elapsed=0.22s\n",
      "[CELL 04-04] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-04] Load pairs from Notebook 03\n",
    "\n",
    "t0 = cell_start(\"CELL 04-04\", \"Load pairs\", pairs=str(PAIRS_PARQUET))\n",
    "\n",
    "if not PAIRS_PARQUET.exists():\n",
    "    raise RuntimeError(f\"Missing pairs.parquet: {PAIRS_PARQUET}. Run Notebook 03 first.\")\n",
    "\n",
    "pairs_df = pd.read_parquet(PAIRS_PARQUET)\n",
    "\n",
    "print(f\"[CELL 04-04] Loaded pairs shape: {pairs_df.shape}\")\n",
    "print(f\"[CELL 04-04] Columns: {list(pairs_df.columns)}\")\n",
    "print(f\"\\n[CELL 04-04] Head(3):\")\n",
    "print(pairs_df.head(3).to_string(index=False))\n",
    "\n",
    "cell_end(\"CELL 04-04\", t0, n_pairs=int(len(pairs_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 04-05] Extract unique users\n",
      "[CELL 04-05] start=2026-01-07T14:40:34\n",
      "[CELL 04-05] Total unique users: 42,171\n",
      "[CELL 04-05] First 5 users: ['1000009', '1000019', '1000066', '1000080', '1000085']\n",
      "[CELL 04-05] Last 5 users: ['999956', '999958', '999970', '999981', '999996']\n",
      "[CELL 04-05] n_users=42171\n",
      "[CELL 04-05] elapsed=0.02s\n",
      "[CELL 04-05] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-05] Extract unique users (deterministic ordering)\n",
    "\n",
    "t0 = cell_start(\"CELL 04-05\", \"Extract unique users\")\n",
    "\n",
    "# Extract unique users and sort alphabetically (deterministic)\n",
    "unique_users = sorted(pairs_df[\"user_id\"].unique())\n",
    "\n",
    "n_users_total = len(unique_users)\n",
    "print(f\"[CELL 04-05] Total unique users: {n_users_total:,}\")\n",
    "print(f\"[CELL 04-05] First 5 users: {unique_users[:5]}\")\n",
    "print(f\"[CELL 04-05] Last 5 users: {unique_users[-5:]}\")\n",
    "\n",
    "# Verify: all pairs have a user_id\n",
    "n_missing = pairs_df[\"user_id\"].isna().sum()\n",
    "if n_missing > 0:\n",
    "    raise RuntimeError(f\"Found {n_missing} pairs with missing user_id\")\n",
    "\n",
    "cell_end(\"CELL 04-05\", t0, n_users=n_users_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 04-06] Split users (80/10/10)\n",
      "[CELL 04-06] start=2026-01-07T14:40:34\n",
      "[CELL 04-06] Shuffled users with seed=20260107\n",
      "[CELL 04-06] Split sizes:\n",
      "  Train: 33,736 users (80.0%)\n",
      "  Val:   4,217 users (10.0%)\n",
      "  Test:  4,218 users (10.0%)\n",
      "  Total: 42,171 users\n",
      "\n",
      "[CELL 04-06] [OK] No user overlap between splits\n",
      "[CELL 04-06] n_train=33736\n",
      "[CELL 04-06] n_val=4217\n",
      "[CELL 04-06] n_test=4218\n",
      "[CELL 04-06] elapsed=0.01s\n",
      "[CELL 04-06] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-06] Shuffle users (seeded) and split 80/10/10\n",
    "\n",
    "t0 = cell_start(\"CELL 04-06\", \"Split users (80/10/10)\")\n",
    "\n",
    "# Shuffle users with seed for reproducibility\n",
    "rng = np.random.RandomState(GLOBAL_SEED)\n",
    "shuffled_users = np.array(unique_users)\n",
    "rng.shuffle(shuffled_users)\n",
    "\n",
    "# Split indices: 80% train, 10% val, 10% test\n",
    "n_train = int(n_users_total * CFG[\"split\"][\"train_ratio\"])\n",
    "n_val = int(n_users_total * CFG[\"split\"][\"val_ratio\"])\n",
    "# Remaining goes to test (handles rounding)\n",
    "n_test = n_users_total - n_train - n_val\n",
    "\n",
    "users_train = shuffled_users[:n_train].tolist()\n",
    "users_val = shuffled_users[n_train:n_train+n_val].tolist()\n",
    "users_test = shuffled_users[n_train+n_val:].tolist()\n",
    "\n",
    "print(f\"[CELL 04-06] Shuffled users with seed={GLOBAL_SEED}\")\n",
    "print(f\"[CELL 04-06] Split sizes:\")\n",
    "print(f\"  Train: {len(users_train):,} users ({len(users_train)/n_users_total*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(users_val):,} users ({len(users_val)/n_users_total*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(users_test):,} users ({len(users_test)/n_users_total*100:.1f}%)\")\n",
    "print(f\"  Total: {len(users_train) + len(users_val) + len(users_test):,} users\")\n",
    "\n",
    "# Validation: no overlap\n",
    "train_set = set(users_train)\n",
    "val_set = set(users_val)\n",
    "test_set = set(users_test)\n",
    "\n",
    "overlap_train_val = train_set & val_set\n",
    "overlap_train_test = train_set & test_set\n",
    "overlap_val_test = val_set & test_set\n",
    "\n",
    "if overlap_train_val or overlap_train_test or overlap_val_test:\n",
    "    raise RuntimeError(f\"User overlap detected: train-val={len(overlap_train_val)}, train-test={len(overlap_train_test)}, val-test={len(overlap_val_test)}\")\n",
    "\n",
    "print(f\"\\n[CELL 04-06] [OK] No user overlap between splits\")\n",
    "\n",
    "cell_end(\"CELL 04-06\", t0, n_train=len(users_train), n_val=len(users_val), n_test=len(users_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 04-07] Save user splits\n",
      "[CELL 04-07] start=2026-01-07T14:40:34\n",
      "[CELL 04-07] Saved: users_train.json (SHA256: 698c7ef143352601...)\n",
      "[CELL 04-07] Saved: users_val.json (SHA256: d3d23ca226c91d04...)\n",
      "[CELL 04-07] Saved: users_test.json (SHA256: ca62c227326741e9...)\n",
      "[CELL 04-07] elapsed=0.06s\n",
      "[CELL 04-07] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-07] Save user splits\n",
    "\n",
    "t0 = cell_start(\"CELL 04-07\", \"Save user splits\")\n",
    "\n",
    "write_json_atomic(OUT_USERS_TRAIN, users_train)\n",
    "write_json_atomic(OUT_USERS_VAL, users_val)\n",
    "write_json_atomic(OUT_USERS_TEST, users_test)\n",
    "\n",
    "users_train_sha = sha256_file(OUT_USERS_TRAIN)\n",
    "users_val_sha = sha256_file(OUT_USERS_VAL)\n",
    "users_test_sha = sha256_file(OUT_USERS_TEST)\n",
    "\n",
    "print(f\"[CELL 04-07] Saved: {OUT_USERS_TRAIN.name} (SHA256: {users_train_sha[:16]}...)\")\n",
    "print(f\"[CELL 04-07] Saved: {OUT_USERS_VAL.name} (SHA256: {users_val_sha[:16]}...)\")\n",
    "print(f\"[CELL 04-07] Saved: {OUT_USERS_TEST.name} (SHA256: {users_test_sha[:16]}...)\")\n",
    "\n",
    "cell_end(\"CELL 04-07\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 04-08] Assign pairs to splits\n",
      "[CELL 04-08] start=2026-01-07T14:40:34\n",
      "[CELL 04-08] Pairs assigned to splits:\n",
      "  Train: 212,923 pairs (80.6%)\n",
      "  Val:   24,698 pairs (9.3%)\n",
      "  Test:  26,608 pairs (10.1%)\n",
      "  Total: 264,229 pairs\n",
      "\n",
      "[CELL 04-08] [OK] All pairs assigned to exactly one split\n",
      "\n",
      "[CELL 04-08] Unique users in pairs:\n",
      "  Train: 33,736 users\n",
      "  Val:   4,217 users\n",
      "  Test:  4,218 users\n",
      "\n",
      "[CELL 04-08] [OK] User counts match original split\n",
      "[CELL 04-08] n_pairs_train=212923\n",
      "[CELL 04-08] n_pairs_val=24698\n",
      "[CELL 04-08] n_pairs_test=26608\n",
      "[CELL 04-08] elapsed=0.09s\n",
      "[CELL 04-08] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-08] Assign pairs to splits based on user_id\n",
    "\n",
    "t0 = cell_start(\"CELL 04-08\", \"Assign pairs to splits\")\n",
    "\n",
    "# Create lookup sets for fast membership testing\n",
    "train_set = set(users_train)\n",
    "val_set = set(users_val)\n",
    "test_set = set(users_test)\n",
    "\n",
    "# Assign each pair to a split based on user_id\n",
    "pairs_train = pairs_df[pairs_df[\"user_id\"].isin(train_set)].copy()\n",
    "pairs_val = pairs_df[pairs_df[\"user_id\"].isin(val_set)].copy()\n",
    "pairs_test = pairs_df[pairs_df[\"user_id\"].isin(test_set)].copy()\n",
    "\n",
    "print(f\"[CELL 04-08] Pairs assigned to splits:\")\n",
    "print(f\"  Train: {len(pairs_train):,} pairs ({len(pairs_train)/len(pairs_df)*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(pairs_val):,} pairs ({len(pairs_val)/len(pairs_df)*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(pairs_test):,} pairs ({len(pairs_test)/len(pairs_df)*100:.1f}%)\")\n",
    "print(f\"  Total: {len(pairs_train) + len(pairs_val) + len(pairs_test):,} pairs\")\n",
    "\n",
    "# Validation: all pairs assigned\n",
    "n_total_assigned = len(pairs_train) + len(pairs_val) + len(pairs_test)\n",
    "if n_total_assigned != len(pairs_df):\n",
    "    raise RuntimeError(f\"Not all pairs assigned: {n_total_assigned} != {len(pairs_df)}\")\n",
    "\n",
    "print(f\"\\n[CELL 04-08] [OK] All pairs assigned to exactly one split\")\n",
    "\n",
    "# User stats per split\n",
    "n_users_train = pairs_train[\"user_id\"].nunique()\n",
    "n_users_val = pairs_val[\"user_id\"].nunique()\n",
    "n_users_test = pairs_test[\"user_id\"].nunique()\n",
    "\n",
    "print(f\"\\n[CELL 04-08] Unique users in pairs:\")\n",
    "print(f\"  Train: {n_users_train:,} users\")\n",
    "print(f\"  Val:   {n_users_val:,} users\")\n",
    "print(f\"  Test:  {n_users_test:,} users\")\n",
    "\n",
    "# Verify user counts match original split\n",
    "if n_users_train != len(users_train) or n_users_val != len(users_val) or n_users_test != len(users_test):\n",
    "    raise RuntimeError(f\"User count mismatch in pairs splits\")\n",
    "\n",
    "print(f\"\\n[CELL 04-08] [OK] User counts match original split\")\n",
    "\n",
    "cell_end(\"CELL 04-08\", t0, \n",
    "         n_pairs_train=int(len(pairs_train)), \n",
    "         n_pairs_val=int(len(pairs_val)), \n",
    "         n_pairs_test=int(len(pairs_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 04-09] Save split pairs\n",
      "[CELL 04-09] start=2026-01-07T14:40:34\n",
      "[CELL 04-09] Saved: pairs_train.parquet (4.0 MB, SHA256: a58c082770686850...)\n",
      "[CELL 04-09] Saved: pairs_val.parquet (0.5 MB, SHA256: 7ab45f2fcf94e9c7...)\n",
      "[CELL 04-09] Saved: pairs_test.parquet (0.6 MB, SHA256: 047794fc5de7ae7b...)\n",
      "[CELL 04-09] elapsed=0.35s\n",
      "[CELL 04-09] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-09] Save split pairs\n",
    "\n",
    "t0 = cell_start(\"CELL 04-09\", \"Save split pairs\")\n",
    "\n",
    "pairs_train.to_parquet(OUT_PAIRS_TRAIN, index=False, compression=\"zstd\")\n",
    "pairs_val.to_parquet(OUT_PAIRS_VAL, index=False, compression=\"zstd\")\n",
    "pairs_test.to_parquet(OUT_PAIRS_TEST, index=False, compression=\"zstd\")\n",
    "\n",
    "pairs_train_bytes = int(OUT_PAIRS_TRAIN.stat().st_size)\n",
    "pairs_val_bytes = int(OUT_PAIRS_VAL.stat().st_size)\n",
    "pairs_test_bytes = int(OUT_PAIRS_TEST.stat().st_size)\n",
    "\n",
    "pairs_train_sha = sha256_file(OUT_PAIRS_TRAIN)\n",
    "pairs_val_sha = sha256_file(OUT_PAIRS_VAL)\n",
    "pairs_test_sha = sha256_file(OUT_PAIRS_TEST)\n",
    "\n",
    "print(f\"[CELL 04-09] Saved: {OUT_PAIRS_TRAIN.name} ({pairs_train_bytes / 1024 / 1024:.1f} MB, SHA256: {pairs_train_sha[:16]}...)\")\n",
    "print(f\"[CELL 04-09] Saved: {OUT_PAIRS_VAL.name} ({pairs_val_bytes / 1024 / 1024:.1f} MB, SHA256: {pairs_val_sha[:16]}...)\")\n",
    "print(f\"[CELL 04-09] Saved: {OUT_PAIRS_TEST.name} ({pairs_test_bytes / 1024 / 1024:.1f} MB, SHA256: {pairs_test_sha[:16]}...)\")\n",
    "\n",
    "cell_end(\"CELL 04-09\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 04-10] Register DuckDB views\n",
      "[CELL 04-10] start=2026-01-07T14:40:35\n",
      "[CELL 04-10] duckdb=C:\\anonymous-users-mooc-session-meta\\data\\interim\\xuetangx.duckdb\n",
      "[CELL 04-10] View xuetangx_pairs_train: 212,923 rows\n",
      "[CELL 04-10] View xuetangx_pairs_val: 24,698 rows\n",
      "[CELL 04-10] View xuetangx_pairs_test: 26,608 rows\n",
      "[CELL 04-10] Closed DuckDB connection\n",
      "[CELL 04-10] elapsed=0.12s\n",
      "[CELL 04-10] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-10] Register DuckDB views for split pairs\n",
    "\n",
    "t0 = cell_start(\"CELL 04-10\", \"Register DuckDB views\", duckdb=str(DUCKDB_PATH))\n",
    "\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(str(DUCKDB_PATH), read_only=False)\n",
    "\n",
    "# Helper to escape Windows paths for DuckDB\n",
    "def esc_path(p: Path) -> str:\n",
    "    return str(p).replace(\"'\", \"''\")\n",
    "\n",
    "# Drop existing views\n",
    "con.execute(\"DROP VIEW IF EXISTS xuetangx_pairs_train;\")\n",
    "con.execute(\"DROP VIEW IF EXISTS xuetangx_pairs_val;\")\n",
    "con.execute(\"DROP VIEW IF EXISTS xuetangx_pairs_test;\")\n",
    "\n",
    "# Create views\n",
    "con.execute(f\"\"\"\n",
    "CREATE VIEW xuetangx_pairs_train AS\n",
    "SELECT * FROM read_parquet('{esc_path(OUT_PAIRS_TRAIN)}')\n",
    "\"\"\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "CREATE VIEW xuetangx_pairs_val AS\n",
    "SELECT * FROM read_parquet('{esc_path(OUT_PAIRS_VAL)}')\n",
    "\"\"\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "CREATE VIEW xuetangx_pairs_test AS\n",
    "SELECT * FROM read_parquet('{esc_path(OUT_PAIRS_TEST)}')\n",
    "\"\"\")\n",
    "\n",
    "n_train = int(con.execute(\"SELECT COUNT(*) FROM xuetangx_pairs_train\").fetchone()[0])\n",
    "n_val = int(con.execute(\"SELECT COUNT(*) FROM xuetangx_pairs_val\").fetchone()[0])\n",
    "n_test = int(con.execute(\"SELECT COUNT(*) FROM xuetangx_pairs_test\").fetchone()[0])\n",
    "\n",
    "print(f\"[CELL 04-10] View xuetangx_pairs_train: {n_train:,} rows\")\n",
    "print(f\"[CELL 04-10] View xuetangx_pairs_val: {n_val:,} rows\")\n",
    "print(f\"[CELL 04-10] View xuetangx_pairs_test: {n_test:,} rows\")\n",
    "\n",
    "con.close()\n",
    "print(f\"[CELL 04-10] Closed DuckDB connection\")\n",
    "\n",
    "cell_end(\"CELL 04-10\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 04-11] Cold-start eligibility check\n",
      "[CELL 04-11] start=2026-01-07T14:40:35\n",
      "[CELL 04-11] Cold-start eligibility (pairs per user):\n",
      "\n",
      "[CELL 04-11] Train split:\n",
      "  Min pairs/user: 1\n",
      "  p50 pairs/user: 2\n",
      "  p90 pairs/user: 13\n",
      "  Max pairs/user: 1318\n",
      "\n",
      "[CELL 04-11] Val split:\n",
      "  Min pairs/user: 1\n",
      "  p50 pairs/user: 2\n",
      "  p90 pairs/user: 12\n",
      "  Max pairs/user: 377\n",
      "\n",
      "[CELL 04-11] Test split:\n",
      "  Min pairs/user: 1\n",
      "  p50 pairs/user: 2\n",
      "  p90 pairs/user: 13\n",
      "  Max pairs/user: 564\n",
      "\n",
      "[CELL 04-11] Eligibility by K+Q budget:\n",
      "  K=5, Q=10 (min 15 pairs):\n",
      "    Train: 3,006/33,736 users (8.9%)\n",
      "    Val:   340/4,217 users (8.1%)\n",
      "    Test:  346/4,218 users (8.2%)\n",
      "  K=10, Q=20 (min 30 pairs):\n",
      "    Train: 1,070/33,736 users (3.2%)\n",
      "    Val:   120/4,217 users (2.8%)\n",
      "    Test:  139/4,218 users (3.3%)\n",
      "[CELL 04-11] elapsed=0.04s\n",
      "[CELL 04-11] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-11] Validation: cold-start eligibility check\n",
    "\n",
    "t0 = cell_start(\"CELL 04-11\", \"Cold-start eligibility check\")\n",
    "\n",
    "# For K-shot learning, we need users with >=K+Q pairs\n",
    "# From Notebook 02, we know K=5,Q=10 means >=15 pairs minimum\n",
    "\n",
    "K_CONFIGS = [(5, 10), (10, 20)]  # (K support, Q query)\n",
    "\n",
    "print(f\"[CELL 04-11] Cold-start eligibility (pairs per user):\")\n",
    "print(f\"\\n[CELL 04-11] Train split:\")\n",
    "train_pair_counts = pairs_train.groupby(\"user_id\").size()\n",
    "print(f\"  Min pairs/user: {train_pair_counts.min()}\")\n",
    "print(f\"  p50 pairs/user: {train_pair_counts.quantile(0.50):.0f}\")\n",
    "print(f\"  p90 pairs/user: {train_pair_counts.quantile(0.90):.0f}\")\n",
    "print(f\"  Max pairs/user: {train_pair_counts.max()}\")\n",
    "\n",
    "print(f\"\\n[CELL 04-11] Val split:\")\n",
    "val_pair_counts = pairs_val.groupby(\"user_id\").size()\n",
    "print(f\"  Min pairs/user: {val_pair_counts.min()}\")\n",
    "print(f\"  p50 pairs/user: {val_pair_counts.quantile(0.50):.0f}\")\n",
    "print(f\"  p90 pairs/user: {val_pair_counts.quantile(0.90):.0f}\")\n",
    "print(f\"  Max pairs/user: {val_pair_counts.max()}\")\n",
    "\n",
    "print(f\"\\n[CELL 04-11] Test split:\")\n",
    "test_pair_counts = pairs_test.groupby(\"user_id\").size()\n",
    "print(f\"  Min pairs/user: {test_pair_counts.min()}\")\n",
    "print(f\"  p50 pairs/user: {test_pair_counts.quantile(0.50):.0f}\")\n",
    "print(f\"  p90 pairs/user: {test_pair_counts.quantile(0.90):.0f}\")\n",
    "print(f\"  Max pairs/user: {test_pair_counts.max()}\")\n",
    "\n",
    "print(f\"\\n[CELL 04-11] Eligibility by K+Q budget:\")\n",
    "for K, Q in K_CONFIGS:\n",
    "    min_pairs = K + Q\n",
    "    n_eligible_train = (train_pair_counts >= min_pairs).sum()\n",
    "    n_eligible_val = (val_pair_counts >= min_pairs).sum()\n",
    "    n_eligible_test = (test_pair_counts >= min_pairs).sum()\n",
    "    print(f\"  K={K}, Q={Q} (min {min_pairs} pairs):\")\n",
    "    print(f\"    Train: {n_eligible_train:,}/{len(users_train):,} users ({n_eligible_train/len(users_train)*100:.1f}%)\")\n",
    "    print(f\"    Val:   {n_eligible_val:,}/{len(users_val):,} users ({n_eligible_val/len(users_val)*100:.1f}%)\")\n",
    "    print(f\"    Test:  {n_eligible_test:,}/{len(users_test):,} users ({n_eligible_test/len(users_test)*100:.1f}%)\")\n",
    "\n",
    "cell_end(\"CELL 04-11\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 04-12] Write report + manifest\n",
      "[CELL 04-12] start=2026-01-07T14:40:35\n",
      "[CELL 04-12] Updated: C:\\anonymous-users-mooc-session-meta\\reports\\04_user_split_xuetangx\\20260107_144034\\report.json\n",
      "[CELL 04-12] Updated: C:\\anonymous-users-mooc-session-meta\\reports\\04_user_split_xuetangx\\20260107_144034\\manifest.json\n",
      "[CELL 04-12] elapsed=0.04s\n",
      "[CELL 04-12] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-12] Update report + manifest\n",
    "\n",
    "t0 = cell_start(\"CELL 04-12\", \"Write report + manifest\")\n",
    "\n",
    "report = read_json(REPORT_PATH)\n",
    "manifest = read_json(MANIFEST_PATH)\n",
    "\n",
    "# Metrics\n",
    "report[\"metrics\"] = {\n",
    "    \"n_users_total\": n_users_total,\n",
    "    \"n_users_train\": len(users_train),\n",
    "    \"n_users_val\": len(users_val),\n",
    "    \"n_users_test\": len(users_test),\n",
    "    \"n_pairs_total\": int(len(pairs_df)),\n",
    "    \"n_pairs_train\": int(len(pairs_train)),\n",
    "    \"n_pairs_val\": int(len(pairs_val)),\n",
    "    \"n_pairs_test\": int(len(pairs_test)),\n",
    "    \"split_ratios\": {\n",
    "        \"train\": CFG[\"split\"][\"train_ratio\"],\n",
    "        \"val\": CFG[\"split\"][\"val_ratio\"],\n",
    "        \"test\": CFG[\"split\"][\"test_ratio\"],\n",
    "    },\n",
    "    \"pairs_per_user\": {\n",
    "        \"train\": {\n",
    "            \"min\": int(train_pair_counts.min()),\n",
    "            \"p50\": float(train_pair_counts.quantile(0.50)),\n",
    "            \"p90\": float(train_pair_counts.quantile(0.90)),\n",
    "            \"max\": int(train_pair_counts.max()),\n",
    "        },\n",
    "        \"val\": {\n",
    "            \"min\": int(val_pair_counts.min()),\n",
    "            \"p50\": float(val_pair_counts.quantile(0.50)),\n",
    "            \"p90\": float(val_pair_counts.quantile(0.90)),\n",
    "            \"max\": int(val_pair_counts.max()),\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"min\": int(test_pair_counts.min()),\n",
    "            \"p50\": float(test_pair_counts.quantile(0.50)),\n",
    "            \"p90\": float(test_pair_counts.quantile(0.90)),\n",
    "            \"max\": int(test_pair_counts.max()),\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Key findings\n",
    "report[\"key_findings\"].append(\n",
    "    f\"Created deterministic user-level split: {len(users_train):,} train ({len(users_train)/n_users_total*100:.1f}%), \"\n",
    "    f\"{len(users_val):,} val ({len(users_val)/n_users_total*100:.1f}%), \"\n",
    "    f\"{len(users_test):,} test ({len(users_test)/n_users_total*100:.1f}%) users. \"\n",
    "    f\"No user overlap between splits (cold-start guarantee).\"\n",
    ")\n",
    "\n",
    "report[\"key_findings\"].append(\n",
    "    f\"Pairs distributed: {len(pairs_train):,} train ({len(pairs_train)/len(pairs_df)*100:.1f}%), \"\n",
    "    f\"{len(pairs_val):,} val ({len(pairs_val)/len(pairs_df)*100:.1f}%), \"\n",
    "    f\"{len(pairs_test):,} test ({len(pairs_test)/len(pairs_df)*100:.1f}%). \"\n",
    "    f\"All pairs assigned to exactly one split based on user_id.\"\n",
    ")\n",
    "\n",
    "# Sanity samples\n",
    "report[\"sanity_samples\"][\"users_train_head5\"] = users_train[:5]\n",
    "report[\"sanity_samples\"][\"users_val_head5\"] = users_val[:5]\n",
    "report[\"sanity_samples\"][\"users_test_head5\"] = users_test[:5]\n",
    "\n",
    "# Fingerprints\n",
    "report[\"data_fingerprints\"][\"users_train\"] = {\"path\": str(OUT_USERS_TRAIN), \"sha256\": users_train_sha}\n",
    "report[\"data_fingerprints\"][\"users_val\"] = {\"path\": str(OUT_USERS_VAL), \"sha256\": users_val_sha}\n",
    "report[\"data_fingerprints\"][\"users_test\"] = {\"path\": str(OUT_USERS_TEST), \"sha256\": users_test_sha}\n",
    "report[\"data_fingerprints\"][\"pairs_train\"] = {\"path\": str(OUT_PAIRS_TRAIN), \"bytes\": pairs_train_bytes, \"sha256\": pairs_train_sha}\n",
    "report[\"data_fingerprints\"][\"pairs_val\"] = {\"path\": str(OUT_PAIRS_VAL), \"bytes\": pairs_val_bytes, \"sha256\": pairs_val_sha}\n",
    "report[\"data_fingerprints\"][\"pairs_test\"] = {\"path\": str(OUT_PAIRS_TEST), \"bytes\": pairs_test_bytes, \"sha256\": pairs_test_sha}\n",
    "\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "# Manifest\n",
    "def add_artifact(path: Path) -> None:\n",
    "    rec = {\"path\": str(path), \"bytes\": int(path.stat().st_size), \"sha256\": None, \"sha256_error\": None}\n",
    "    try:\n",
    "        rec[\"sha256\"] = sha256_file(path)\n",
    "    except PermissionError as e:\n",
    "        rec[\"sha256_error\"] = f\"PermissionError: {e}\"\n",
    "    manifest[\"artifacts\"].append(rec)\n",
    "\n",
    "add_artifact(OUT_USERS_TRAIN)\n",
    "add_artifact(OUT_USERS_VAL)\n",
    "add_artifact(OUT_USERS_TEST)\n",
    "add_artifact(OUT_PAIRS_TRAIN)\n",
    "add_artifact(OUT_PAIRS_VAL)\n",
    "add_artifact(OUT_PAIRS_TEST)\n",
    "\n",
    "write_json_atomic(MANIFEST_PATH, manifest)\n",
    "\n",
    "print(f\"[CELL 04-12] Updated: {REPORT_PATH}\")\n",
    "print(f\"[CELL 04-12] Updated: {MANIFEST_PATH}\")\n",
    "\n",
    "cell_end(\"CELL 04-12\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 04-13] Generate plots and tables\n",
      "[CELL 04-13] start=2026-01-07T14:40:35\n",
      "[CELL 04-13] Creating visualizations in C:\\anonymous-users-mooc-session-meta\\reports\\04_user_split_xuetangx\\20260107_144034\\visualizations\n",
      "[CELL 04-13] Saved: fig1_split_distribution.png\n",
      "[CELL 04-13] Saved: fig2_pairs_per_user.png\n",
      "[CELL 04-13] Saved: table1_split_summary.csv\n",
      "\n",
      "[CELL 04-13] Table 1: Split Summary\n",
      "Split  Users   Pairs Users (%) Pairs (%)  Min Pairs/User  Median Pairs/User  Max Pairs/User\n",
      "Train 33,736 212,923     80.0%     80.6%               1                  2            1318\n",
      "  Val  4,217  24,698     10.0%      9.3%               1                  2             377\n",
      " Test  4,218  26,608     10.0%     10.1%               1                  2             564\n",
      "[CELL 04-13] Saved: table2_cold_start_eligibility.csv\n",
      "\n",
      "[CELL 04-13] Table 2: Cold-Start Eligibility\n",
      "    Config  Min Pairs Train Eligible Val Eligible Test Eligible\n",
      " K=5, Q=10         15   3,006 (8.9%)   340 (8.1%)    346 (8.2%)\n",
      "K=10, Q=20         30   1,070 (3.2%)   120 (2.8%)    139 (3.3%)\n",
      "\n",
      "[CELL 04-13] All visualizations saved to C:\\anonymous-users-mooc-session-meta\\reports\\04_user_split_xuetangx\\20260107_144034\\visualizations\n",
      "[CELL 04-13] elapsed=9.16s\n",
      "[CELL 04-13] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-13] Visualizations: plots and tables for reporting\n",
    "\n",
    "t0 = cell_start(\"CELL 04-13\", \"Generate plots and tables\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "VIZ_DIR = OUT_DIR / \"visualizations\"\n",
    "VIZ_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"[CELL 04-13] Creating visualizations in {VIZ_DIR}\")\n",
    "\n",
    "# ===== PLOT 1: Split distribution (users and pairs) =====\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Users\n",
    "splits = ['Train', 'Val', 'Test']\n",
    "user_counts = [len(users_train), len(users_val), len(users_test)]\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
    "\n",
    "ax1.bar(splits, user_counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_ylabel('Number of Users', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('User Split Distribution', fontsize=12, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "for i, (split, count) in enumerate(zip(splits, user_counts)):\n",
    "    ax1.text(i, count + 500, f'{count:,}\\n({count/n_users_total*100:.1f}%)', \n",
    "             ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Pairs\n",
    "pair_counts = [len(pairs_train), len(pairs_val), len(pairs_test)]\n",
    "ax2.bar(splits, pair_counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.set_ylabel('Number of Pairs', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Pair Split Distribution', fontsize=12, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "for i, (split, count) in enumerate(zip(splits, pair_counts)):\n",
    "    ax2.text(i, count + 3000, f'{count:,}\\n({count/len(pairs_df)*100:.1f}%)', \n",
    "             ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / \"fig1_split_distribution.png\", bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"[CELL 04-13] Saved: fig1_split_distribution.png\")\n",
    "\n",
    "# ===== PLOT 2: Pairs per user distribution by split =====\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\n",
    "\n",
    "for idx, (split_name, pairs_split, ax) in enumerate([\n",
    "    ('Train', pairs_train, axes[0]),\n",
    "    ('Val', pairs_val, axes[1]),\n",
    "    ('Test', pairs_test, axes[2])\n",
    "]):\n",
    "    pair_counts_split = pairs_split.groupby('user_id').size()\n",
    "    \n",
    "    ax.hist(pair_counts_split, bins=50, color=colors[idx], alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(pair_counts_split.median(), color='red', linestyle='--', linewidth=2, label=f'Median={pair_counts_split.median():.0f}')\n",
    "    ax.set_xlabel('Pairs per User', fontsize=11, fontweight='bold')\n",
    "    if idx == 0:\n",
    "        ax.set_ylabel('Number of Users', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{split_name} Split', fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / \"fig2_pairs_per_user.png\", bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"[CELL 04-13] Saved: fig2_pairs_per_user.png\")\n",
    "\n",
    "# ===== TABLE 1: Split summary statistics =====\n",
    "table1 = pd.DataFrame([\n",
    "    {\n",
    "        'Split': 'Train',\n",
    "        'Users': f\"{len(users_train):,}\",\n",
    "        'Pairs': f\"{len(pairs_train):,}\",\n",
    "        'Users (%)': f\"{len(users_train)/n_users_total*100:.1f}%\",\n",
    "        'Pairs (%)': f\"{len(pairs_train)/len(pairs_df)*100:.1f}%\",\n",
    "        'Min Pairs/User': int(train_pair_counts.min()),\n",
    "        'Median Pairs/User': int(train_pair_counts.median()),\n",
    "        'Max Pairs/User': int(train_pair_counts.max()),\n",
    "    },\n",
    "    {\n",
    "        'Split': 'Val',\n",
    "        'Users': f\"{len(users_val):,}\",\n",
    "        'Pairs': f\"{len(pairs_val):,}\",\n",
    "        'Users (%)': f\"{len(users_val)/n_users_total*100:.1f}%\",\n",
    "        'Pairs (%)': f\"{len(pairs_val)/len(pairs_df)*100:.1f}%\",\n",
    "        'Min Pairs/User': int(val_pair_counts.min()),\n",
    "        'Median Pairs/User': int(val_pair_counts.median()),\n",
    "        'Max Pairs/User': int(val_pair_counts.max()),\n",
    "    },\n",
    "    {\n",
    "        'Split': 'Test',\n",
    "        'Users': f\"{len(users_test):,}\",\n",
    "        'Pairs': f\"{len(pairs_test):,}\",\n",
    "        'Users (%)': f\"{len(users_test)/n_users_total*100:.1f}%\",\n",
    "        'Pairs (%)': f\"{len(pairs_test)/len(pairs_df)*100:.1f}%\",\n",
    "        'Min Pairs/User': int(test_pair_counts.min()),\n",
    "        'Median Pairs/User': int(test_pair_counts.median()),\n",
    "        'Max Pairs/User': int(test_pair_counts.max()),\n",
    "    },\n",
    "])\n",
    "\n",
    "table1.to_csv(VIZ_DIR / \"table1_split_summary.csv\", index=False)\n",
    "print(f\"[CELL 04-13] Saved: table1_split_summary.csv\")\n",
    "print(f\"\\n[CELL 04-13] Table 1: Split Summary\")\n",
    "print(table1.to_string(index=False))\n",
    "\n",
    "# ===== TABLE 2: Cold-start eligibility by K+Q budget =====\n",
    "K_CONFIGS = [(5, 10), (10, 20)]\n",
    "eligibility_records = []\n",
    "\n",
    "for K, Q in K_CONFIGS:\n",
    "    min_pairs = K + Q\n",
    "    n_eligible_train = (train_pair_counts >= min_pairs).sum()\n",
    "    n_eligible_val = (val_pair_counts >= min_pairs).sum()\n",
    "    n_eligible_test = (test_pair_counts >= min_pairs).sum()\n",
    "    \n",
    "    eligibility_records.append({\n",
    "        'Config': f'K={K}, Q={Q}',\n",
    "        'Min Pairs': min_pairs,\n",
    "        'Train Eligible': f\"{n_eligible_train:,} ({n_eligible_train/len(users_train)*100:.1f}%)\",\n",
    "        'Val Eligible': f\"{n_eligible_val:,} ({n_eligible_val/len(users_val)*100:.1f}%)\",\n",
    "        'Test Eligible': f\"{n_eligible_test:,} ({n_eligible_test/len(users_test)*100:.1f}%)\",\n",
    "    })\n",
    "\n",
    "table2 = pd.DataFrame(eligibility_records)\n",
    "table2.to_csv(VIZ_DIR / \"table2_cold_start_eligibility.csv\", index=False)\n",
    "print(f\"[CELL 04-13] Saved: table2_cold_start_eligibility.csv\")\n",
    "print(f\"\\n[CELL 04-13] Table 2: Cold-Start Eligibility\")\n",
    "print(table2.to_string(index=False))\n",
    "\n",
    "print(f\"\\n[CELL 04-13] All visualizations saved to {VIZ_DIR}\")\n",
    "\n",
    "cell_end(\"CELL 04-13\", t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Notebook 04 Complete\n",
    "\n",
    "**Outputs:**\n",
    "- ✅ `data/processed/xuetangx/user_splits/users_train.json` (80% of users)\n",
    "- ✅ `data/processed/xuetangx/user_splits/users_val.json` (10% of users)\n",
    "- ✅ `data/processed/xuetangx/user_splits/users_test.json` (10% of users)\n",
    "- ✅ `data/processed/xuetangx/pairs/pairs_train.parquet`\n",
    "- ✅ `data/processed/xuetangx/pairs/pairs_val.parquet`\n",
    "- ✅ `data/processed/xuetangx/pairs/pairs_test.parquet`\n",
    "- ✅ DuckDB views: `xuetangx_pairs_train`, `xuetangx_pairs_val`, `xuetangx_pairs_test`\n",
    "- ✅ `reports/04_user_split_xuetangx/<run_tag>/report.json`\n",
    "\n",
    "**Validation Passed:**\n",
    "- ✅ No user overlap between splits (disjoint users)\n",
    "- ✅ All pairs assigned to exactly one split\n",
    "- ✅ User counts match in pairs and user lists\n",
    "- ✅ Cold-start guarantee: test users completely unseen during training\n",
    "\n",
    "**Next:** Notebook 05 (Episode Index)\n",
    "- Create episodic meta-learning indices\n",
    "- Sample K-shot support + Q query pairs per user\n",
    "- Chronological validation (support timestamps < query timestamps)\n",
    "- Multiple episodes per user for training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
