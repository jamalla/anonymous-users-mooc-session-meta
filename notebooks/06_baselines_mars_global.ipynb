{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc094516",
   "metadata": {},
   "source": [
    "Bootstrap + logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1ff9a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 06-00] start=2026-01-06T23:42:02\n",
      "[CELL 06-00] CWD: C:\\anonymous-users-mooc-session-meta\\notebooks\n",
      "[CELL 06-00] REPO_ROOT: C:\\anonymous-users-mooc-session-meta\n",
      "[CELL 06-00] META_REGISTRY=C:\\anonymous-users-mooc-session-meta\\meta.json\n",
      "[CELL 06-00] DATA_INTERIM=C:\\anonymous-users-mooc-session-meta\\data\\interim\n",
      "[CELL 06-00] DATA_PROCESSED=C:\\anonymous-users-mooc-session-meta\\data\\processed\n",
      "[CELL 06-00] REPORTS=C:\\anonymous-users-mooc-session-meta\\reports\n",
      "[CELL 06-00] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-00] Bootstrap: repo root + paths + logger\n",
    "\n",
    "import json, time, uuid, hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"[CELL 06-00] start={datetime.now().isoformat(timespec='seconds')}\")\n",
    "print(\"[CELL 06-00] CWD:\", Path.cwd().resolve())\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists():\n",
    "            return p\n",
    "    raise RuntimeError(\"Could not find PROJECT_STATE.md. Open notebook from within the repo.\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "print(\"[CELL 06-00] REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "PATHS = {\n",
    "    \"META_REGISTRY\": REPO_ROOT / \"meta.json\",\n",
    "    \"DATA_INTERIM\": REPO_ROOT / \"data\" / \"interim\",\n",
    "    \"DATA_PROCESSED\": REPO_ROOT / \"data\" / \"processed\",\n",
    "    \"REPORTS\": REPO_ROOT / \"reports\",\n",
    "}\n",
    "for k, v in PATHS.items():\n",
    "    print(f\"[CELL 06-00] {k}={v}\")\n",
    "\n",
    "def cell_start(cell_id: str, title: str, **kwargs: Any) -> float:\n",
    "    t = time.time()\n",
    "    print(f\"\\n[{cell_id}] {title}\")\n",
    "    print(f\"[{cell_id}] start={datetime.now().isoformat(timespec='seconds')}\")\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    return t\n",
    "\n",
    "def cell_end(cell_id: str, t0: float, **kwargs: Any) -> None:\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    print(f\"[{cell_id}] elapsed={time.time()-t0:.2f}s\")\n",
    "    print(f\"[{cell_id}] done\")\n",
    "\n",
    "print(\"[CELL 06-00] done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e5124d",
   "metadata": {},
   "source": [
    "JSON IO + hashing (Timestamp-safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "795bbd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-01] JSON IO + hashing\n",
      "[CELL 06-01] start=2026-01-06T23:42:05\n",
      "[CELL 06-01] elapsed=0.00s\n",
      "[CELL 06-01] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-01] JSON IO + hashing (Timestamp-safe)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-01\", \"JSON IO + hashing\")\n",
    "\n",
    "def _json_default(o):\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        if isinstance(o, (pd.Timestamp,)):\n",
    "            return o.isoformat()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        import numpy as np\n",
    "        if isinstance(o, (np.integer,)): return int(o)\n",
    "        if isinstance(o, (np.floating,)): return float(o)\n",
    "        if isinstance(o, (np.bool_,)): return bool(o)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        from datetime import datetime, date\n",
    "        if isinstance(o, (datetime, date)): return o.isoformat()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return str(o)\n",
    "\n",
    "def write_json_atomic(path: Path, obj: Any, indent: int = 2) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tmp = path.with_suffix(path.suffix + f\".tmp_{uuid.uuid4().hex}\")\n",
    "    with tmp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=indent, default=_json_default)\n",
    "    tmp.replace(path)\n",
    "\n",
    "def read_json(path: Path) -> Any:\n",
    "    if not path.exists():\n",
    "        raise RuntimeError(f\"Missing JSON file: {path}\")\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b: break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def safe_artifact_record(path: Path) -> Dict[str, Any]:\n",
    "    rec = {\"path\": str(path), \"bytes\": int(path.stat().st_size), \"sha256\": None, \"sha256_error\": None}\n",
    "    try:\n",
    "        rec[\"sha256\"] = sha256_file(path)\n",
    "    except PermissionError as e:\n",
    "        rec[\"sha256_error\"] = f\"PermissionError: {e}\"\n",
    "        print(\"[CELL 06-01] WARN: locked, cannot hash now:\", path)\n",
    "    return rec\n",
    "\n",
    "cell_end(\"CELL 06-01\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b82d7c",
   "metadata": {},
   "source": [
    "Start run (report/config/manifest + meta.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb61954b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-02] Start run\n",
      "[CELL 06-02] start=2026-01-06T23:42:10\n",
      "[CELL 06-02] out_dir: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\n",
      "[CELL 06-02] elapsed=0.01s\n",
      "[CELL 06-02] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-02] Start run + init report/config/manifest + meta.json append-only\n",
    "\n",
    "t0 = cell_start(\"CELL 06-02\", \"Start run\")\n",
    "\n",
    "NOTEBOOK_NAME = \"06_baselines_mars_global\"\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_ID = uuid.uuid4().hex\n",
    "\n",
    "OUT_DIR = PATHS[\"REPORTS\"] / NOTEBOOK_NAME / RUN_TAG\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REPORT_PATH = OUT_DIR / \"report.json\"\n",
    "CONFIG_PATH = OUT_DIR / \"config.json\"\n",
    "MANIFEST_PATH = OUT_DIR / \"manifest.json\"\n",
    "\n",
    "DUCKDB_PATH = PATHS[\"DATA_INTERIM\"] / \"mars.duckdb\"\n",
    "\n",
    "BASE = PATHS[\"DATA_PROCESSED\"] / \"mars\"\n",
    "VOCAB_ITEM2ID = BASE / \"vocab\" / \"item2id.json\"\n",
    "SPLIT_DIR = BASE / \"user_splits\"\n",
    "EP_DIR = BASE / \"episodes\"\n",
    "\n",
    "PAIRS_TRAIN_PQ = SPLIT_DIR / \"pairs_train.parquet\"\n",
    "PAIRS_VAL_PQ   = SPLIT_DIR / \"pairs_val.parquet\"\n",
    "PAIRS_TEST_PQ  = SPLIT_DIR / \"pairs_test.parquet\"\n",
    "\n",
    "EP_INDEX_PQ = EP_DIR / \"episodes_index.parquet\"\n",
    "EP_LONG_PQ  = EP_DIR / \"episodes_long.parquet\"\n",
    "\n",
    "CFG = {\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"seed\": 20260106,\n",
    "    \"inputs\": {\n",
    "        \"duckdb_path\": str(DUCKDB_PATH),\n",
    "        \"item2id\": str(VOCAB_ITEM2ID),\n",
    "        \"pairs_train\": str(PAIRS_TRAIN_PQ),\n",
    "        \"pairs_val\": str(PAIRS_VAL_PQ),\n",
    "        \"pairs_test\": str(PAIRS_TEST_PQ),\n",
    "        \"episodes_index\": str(EP_INDEX_PQ),\n",
    "        \"episodes_long\": str(EP_LONG_PQ),\n",
    "    },\n",
    "    \"eval\": {\"cutoffs\": [5, 10, 20]},\n",
    "    \"baselines\": [\"popularity\"],  # we will add session-knn + gru4rec next\n",
    "}\n",
    "write_json_atomic(CONFIG_PATH, CFG)\n",
    "\n",
    "report = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"repo_root\": str(REPO_ROOT),\n",
    "    \"metrics\": {},\n",
    "    \"key_findings\": [],\n",
    "    \"sanity_samples\": {},\n",
    "    \"data_fingerprints\": {},\n",
    "    \"notes\": [],\n",
    "}\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "write_json_atomic(MANIFEST_PATH, {\"run_id\": RUN_ID, \"notebook\": NOTEBOOK_NAME, \"run_tag\": RUN_TAG, \"artifacts\": []})\n",
    "\n",
    "META_PATH = PATHS[\"META_REGISTRY\"]\n",
    "if not META_PATH.exists():\n",
    "    write_json_atomic(META_PATH, {\"schema_version\": 1, \"runs\": []})\n",
    "meta = read_json(META_PATH)\n",
    "meta[\"runs\"].append({\"run_id\": RUN_ID, \"notebook\": NOTEBOOK_NAME, \"run_tag\": RUN_TAG, \"out_dir\": str(OUT_DIR),\n",
    "                     \"created_at\": datetime.now().isoformat(timespec=\"seconds\")})\n",
    "write_json_atomic(META_PATH, meta)\n",
    "\n",
    "print(\"[CELL 06-02] out_dir:\", OUT_DIR)\n",
    "cell_end(\"CELL 06-02\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408050f8",
   "metadata": {},
   "source": [
    "Validate required artifacts exist (hard fail if missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1f178d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-03] Validate artifacts exist\n",
      "[CELL 06-03] start=2026-01-06T23:42:16\n",
      "[CELL 06-03] OK: all required files exist\n",
      "[CELL 06-03] elapsed=0.00s\n",
      "[CELL 06-03] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-03] Validate required artifacts exist\n",
    "\n",
    "t0 = cell_start(\"CELL 06-03\", \"Validate artifacts exist\")\n",
    "\n",
    "req = [VOCAB_ITEM2ID, PAIRS_TRAIN_PQ, PAIRS_VAL_PQ, PAIRS_TEST_PQ, EP_INDEX_PQ, EP_LONG_PQ]\n",
    "missing = [str(p) for p in req if not Path(p).exists()]\n",
    "if missing:\n",
    "    raise RuntimeError(\"Missing required artifacts:\\n\" + \"\\n\".join(missing))\n",
    "\n",
    "print(\"[CELL 06-03] OK: all required files exist\")\n",
    "cell_end(\"CELL 06-03\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc58ed4",
   "metadata": {},
   "source": [
    "Global seeding + load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fe8d44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-04] Seed everything + load config\n",
      "[CELL 06-04] start=2026-01-06T23:42:18\n",
      "[CELL 06-04] seed: 20260106\n",
      "[CELL 06-04] run_tag: 20260106_234210\n",
      "[CELL 06-04] OUT_DIR: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\n",
      "[CELL 06-04] inputs: {'duckdb_path': 'C:\\\\anonymous-users-mooc-session-meta\\\\data\\\\interim\\\\mars.duckdb', 'item2id': 'C:\\\\anonymous-users-mooc-session-meta\\\\data\\\\processed\\\\mars\\\\vocab\\\\item2id.json', 'pairs_train': 'C:\\\\anonymous-users-mooc-session-meta\\\\data\\\\processed\\\\mars\\\\user_splits\\\\pairs_train.parquet', 'pairs_val': 'C:\\\\anonymous-users-mooc-session-meta\\\\data\\\\processed\\\\mars\\\\user_splits\\\\pairs_val.parquet', 'pairs_test': 'C:\\\\anonymous-users-mooc-session-meta\\\\data\\\\processed\\\\mars\\\\user_splits\\\\pairs_test.parquet', 'episodes_index': 'C:\\\\anonymous-users-mooc-session-meta\\\\data\\\\processed\\\\mars\\\\episodes\\\\episodes_index.parquet', 'episodes_long': 'C:\\\\anonymous-users-mooc-session-meta\\\\data\\\\processed\\\\mars\\\\episodes\\\\episodes_long.parquet'}\n",
      "[CELL 06-04] eval cutoffs: [5, 10, 20]\n",
      "[CELL 06-04] elapsed=0.00s\n",
      "[CELL 06-04] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-04] Global seeding + load config\n",
    "\n",
    "t0 = cell_start(\"CELL 06-04\", \"Seed everything + load config\")\n",
    "\n",
    "seed = int(CFG[\"seed\"])\n",
    "np.random.seed(seed)\n",
    "\n",
    "print(\"[CELL 06-04] seed:\", seed)\n",
    "print(\"[CELL 06-04] run_tag:\", RUN_TAG)\n",
    "print(\"[CELL 06-04] OUT_DIR:\", OUT_DIR)\n",
    "print(\"[CELL 06-04] inputs:\", CFG[\"inputs\"])\n",
    "print(\"[CELL 06-04] eval cutoffs:\", CFG[\"eval\"][\"cutoffs\"])\n",
    "\n",
    "cell_end(\"CELL 06-04\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b273c1",
   "metadata": {},
   "source": [
    "Load vocab + basic counts (sanity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8612ca21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-05] Load item2id + sanity\n",
      "[CELL 06-05] start=2026-01-06T23:42:21\n",
      "[CELL 06-05] n_items: 776\n",
      "[CELL 06-05] item2id first5: [('510', 0), ('511', 1), ('512', 2), ('513', 3), ('514', 4)]\n",
      "[CELL 06-05] n_items=776\n",
      "[CELL 06-05] elapsed=0.00s\n",
      "[CELL 06-05] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-05] Load vocab + sanity\n",
    "\n",
    "t0 = cell_start(\"CELL 06-05\", \"Load item2id + sanity\")\n",
    "\n",
    "item2id = read_json(Path(VOCAB_ITEM2ID))\n",
    "n_items = len(item2id)\n",
    "\n",
    "print(\"[CELL 06-05] n_items:\", n_items)\n",
    "print(\"[CELL 06-05] item2id first5:\", list(item2id.items())[:5])\n",
    "\n",
    "cell_end(\"CELL 06-05\", t0, n_items=n_items)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61f40c4",
   "metadata": {},
   "source": [
    "Load split pairs (train/val/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b1a47a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-06] Load split pairs parquet\n",
      "[CELL 06-06] start=2026-01-06T23:42:24\n",
      "[CELL 06-06] pairs_train shape: (1932, 6)\n",
      "[CELL 06-06] pairs_val shape: (191, 6)\n",
      "[CELL 06-06] pairs_test shape: (214, 6)\n",
      "[CELL 06-06] head3 test:\n",
      "   session_id  user_id  tpos   prefix  prefix_len  label\n",
      "158057_000003   158057     2    [686]           1    687\n",
      "174528_000001   174528     2     [36]           1     37\n",
      "174528_000001   174528     3 [36, 37]           2     40\n",
      "[CELL 06-06] elapsed=0.08s\n",
      "[CELL 06-06] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-06] Load pairs_train/val/test (parquet)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-06\", \"Load split pairs parquet\")\n",
    "\n",
    "pairs_train = pd.read_parquet(PAIRS_TRAIN_PQ)\n",
    "pairs_val   = pd.read_parquet(PAIRS_VAL_PQ)\n",
    "pairs_test  = pd.read_parquet(PAIRS_TEST_PQ)\n",
    "\n",
    "print(\"[CELL 06-06] pairs_train shape:\", pairs_train.shape)\n",
    "print(\"[CELL 06-06] pairs_val shape:\", pairs_val.shape)\n",
    "print(\"[CELL 06-06] pairs_test shape:\", pairs_test.shape)\n",
    "\n",
    "need_cols = {\"label\", \"prefix_len\", \"user_id\", \"session_id\", \"tpos\"}\n",
    "for name, df in [(\"train\", pairs_train), (\"val\", pairs_val), (\"test\", pairs_test)]:\n",
    "    missing = need_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"Missing columns in pairs_{name}: {missing}\")\n",
    "\n",
    "print(\"[CELL 06-06] head3 test:\")\n",
    "print(pairs_test.head(3).to_string(index=False))\n",
    "\n",
    "cell_end(\"CELL 06-06\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0589c29d",
   "metadata": {},
   "source": [
    "Build popularity ranking from TRAIN (deterministic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c571c0f",
   "metadata": {},
   "source": [
    "We rank items by label frequency in pairs_train. (Deterministic tie-break by item id.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1249240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-07] Build popularity ranking (train labels)\n",
      "[CELL 06-07] start=2026-01-06T23:42:28\n",
      "[CELL 06-07] label_counts head10:\n",
      " item  count\n",
      "    2     36\n",
      "  435     21\n",
      "    0     20\n",
      "  257     19\n",
      "  436     18\n",
      "  339     17\n",
      "  437     17\n",
      "  398     15\n",
      "  258     14\n",
      "   36     13\n",
      "[CELL 06-07] topN: 20\n",
      "[CELL 06-07] top_items: [2, 435, 0, 257, 436, 339, 437, 398, 258, 36, 37, 438, 38, 39, 338, 397, 459, 733, 41, 399]\n",
      "[CELL 06-07] wrote: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\popularity_rank_top.json\n",
      "[CELL 06-07] n_ranked=658\n",
      "[CELL 06-07] elapsed=0.03s\n",
      "[CELL 06-07] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-07] Popularity ranking from TRAIN labels (deterministic) ✅ FIXED\n",
    "\n",
    "t0 = cell_start(\"CELL 06-07\", \"Build popularity ranking (train labels)\")\n",
    "\n",
    "# Count label frequencies\n",
    "label_counts = (\n",
    "    pairs_train[\"label\"]\n",
    "    .astype(int)\n",
    "    .value_counts()\n",
    "    .rename_axis(\"item\")\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "# Deterministic tie-break: higher count first, then smaller item id\n",
    "label_counts = label_counts.sort_values([\"count\", \"item\"], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "print(\"[CELL 06-07] label_counts head10:\")\n",
    "print(label_counts.head(10).to_string(index=False))\n",
    "\n",
    "# Build rank map: item -> rank (1-index)\n",
    "rank_map = {int(row[\"item\"]): int(i + 1) for i, row in label_counts.iterrows()}\n",
    "\n",
    "# Top-N list for retrieval\n",
    "cutoffs = list(map(int, CFG[\"eval\"][\"cutoffs\"]))\n",
    "topN = int(max(cutoffs))\n",
    "top_items = label_counts[\"item\"].astype(int).tolist()[:topN]\n",
    "\n",
    "print(\"[CELL 06-07] topN:\", topN)\n",
    "print(\"[CELL 06-07] top_items:\", top_items[:20])\n",
    "\n",
    "# Save artifact for reproducibility\n",
    "pop_path = OUT_DIR / \"popularity_rank_top.json\"\n",
    "write_json_atomic(pop_path, {\"topN\": topN, \"top_items\": top_items, \"seed\": seed})\n",
    "\n",
    "print(\"[CELL 06-07] wrote:\", pop_path)\n",
    "\n",
    "cell_end(\"CELL 06-07\", t0, n_ranked=len(rank_map))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c6531b",
   "metadata": {},
   "source": [
    "Metrics helpers (HR/MRR/NDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dbe9f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-08] Define metrics functions\n",
      "[CELL 06-08] start=2026-01-06T23:42:33\n",
      "[CELL 06-08] elapsed=0.00s\n",
      "[CELL 06-08] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-08] Metrics helpers (HR@K, MRR@K, NDCG@K)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-08\", \"Define metrics functions\")\n",
    "\n",
    "import math\n",
    "\n",
    "def eval_one_label_rank(rank: int, K: int) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    rank: 1-index rank of the true item. If not present => large rank.\n",
    "    \"\"\"\n",
    "    if rank <= K:\n",
    "        hr = 1.0\n",
    "        mrr = 1.0 / float(rank)\n",
    "        ndcg = 1.0 / math.log2(float(rank) + 1.0)\n",
    "    else:\n",
    "        hr = 0.0\n",
    "        mrr = 0.0\n",
    "        ndcg = 0.0\n",
    "    return {\"HR\": hr, \"MRR\": mrr, \"NDCG\": ndcg}\n",
    "\n",
    "def eval_popularity(df: pd.DataFrame, rank_map: Dict[int,int], cutoffs: list[int]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate popularity ranking for next-item prediction.\n",
    "    df must have column 'label' as int.\n",
    "    \"\"\"\n",
    "    labels = df[\"label\"].astype(int).tolist()\n",
    "\n",
    "    # If label not in rank_map (unseen in train), rank = +inf (miss)\n",
    "    ranks = [rank_map.get(int(y), 10**9) for y in labels]\n",
    "\n",
    "    out = {}\n",
    "    for K in cutoffs:\n",
    "        hr = 0.0\n",
    "        mrr = 0.0\n",
    "        ndcg = 0.0\n",
    "        for r in ranks:\n",
    "            m = eval_one_label_rank(r, int(K))\n",
    "            hr += m[\"HR\"]\n",
    "            mrr += m[\"MRR\"]\n",
    "            ndcg += m[\"NDCG\"]\n",
    "        n = float(len(ranks)) if len(ranks) else 1.0\n",
    "        out[f\"HR@{K}\"] = hr / n\n",
    "        out[f\"MRR@{K}\"] = mrr / n\n",
    "        out[f\"NDCG@{K}\"] = ndcg / n\n",
    "    out[\"n\"] = int(len(ranks))\n",
    "    return out\n",
    "\n",
    "cell_end(\"CELL 06-08\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53042bf",
   "metadata": {},
   "source": [
    "Evaluate popularity on GLOBAL TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2ba0ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-09] Evaluate popularity baseline on pairs_test\n",
      "[CELL 06-09] start=2026-01-06T23:42:37\n",
      "[CELL 06-09] GLOBAL_TEST(pop) metrics: {'HR@5': 0.0794392523364486, 'MRR@5': 0.029049844236760124, 'NDCG@5': 0.041452696484700624, 'HR@10': 0.11682242990654206, 'MRR@10': 0.034292018988280666, 'NDCG@10': 0.05379310735740311, 'HR@20': 0.19158878504672897, 'MRR@20': 0.03934627281372891, 'NDCG@20': 0.07251532685907323, 'n': 214}\n",
      "[CELL 06-09] elapsed=0.01s\n",
      "[CELL 06-09] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-09] Popularity baseline: GLOBAL test evaluation\n",
    "\n",
    "t0 = cell_start(\"CELL 06-09\", \"Evaluate popularity baseline on pairs_test\")\n",
    "\n",
    "cutoffs = list(map(int, CFG[\"eval\"][\"cutoffs\"]))\n",
    "res_global_test = eval_popularity(pairs_test, rank_map, cutoffs)\n",
    "\n",
    "print(\"[CELL 06-09] GLOBAL_TEST(pop) metrics:\", res_global_test)\n",
    "\n",
    "# write into report\n",
    "report = read_json(REPORT_PATH)\n",
    "report[\"metrics\"][\"popularity_global_test\"] = res_global_test\n",
    "report[\"key_findings\"].append(\"Computed popularity baseline on global test (pairs_test).\")\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "cell_end(\"CELL 06-09\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d8e0b3",
   "metadata": {},
   "source": [
    "Evaluate popularity on EPISODE TEST (query only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3759b",
   "metadata": {},
   "source": [
    "This uses episodes_index/long to select only query pairs from test episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f382645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-10] Evaluate popularity on episode test (query only)\n",
      "[CELL 06-10] start=2026-01-06T23:42:40\n",
      "[CELL 06-10] ep_index shape: (53, 9)\n",
      "[CELL 06-10] ep_long shape: (1713, 4)\n",
      "[CELL 06-10] test_eps shape: (2, 9)\n",
      "[CELL 06-10] query rows: (40, 4)\n",
      "[CELL 06-10] q_eval shape (after join): (40, 5)\n",
      "[CELL 06-10] EPISODE_TEST(pop) metrics: {'HR@5': 0.0, 'MRR@5': 0.0, 'NDCG@5': 0.0, 'HR@10': 0.0, 'MRR@10': 0.0, 'NDCG@10': 0.0, 'HR@20': 0.05, 'MRR@20': 0.0029411764705882353, 'NDCG@20': 0.011990623328406573, 'n': 40}\n",
      "[CELL 06-10] elapsed=0.11s\n",
      "[CELL 06-10] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-10] Popularity baseline: EPISODE test evaluation (query only)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-10\", \"Evaluate popularity on episode test (query only)\")\n",
    "\n",
    "# Load episode tables\n",
    "ep_index = pd.read_parquet(EP_INDEX_PQ)\n",
    "ep_long  = pd.read_parquet(EP_LONG_PQ)\n",
    "\n",
    "print(\"[CELL 06-10] ep_index shape:\", ep_index.shape)\n",
    "print(\"[CELL 06-10] ep_long shape:\", ep_long.shape)\n",
    "\n",
    "# Only test episodes, and only feasible K values (we keep generic)\n",
    "test_eps = ep_index[ep_index[\"split\"] == \"test\"].copy()\n",
    "print(\"[CELL 06-10] test_eps shape:\", test_eps.shape)\n",
    "\n",
    "if test_eps.shape[0] == 0:\n",
    "    print(\"[CELL 06-10] WARNING: no test episodes available. We don't know yet for episodic test.\")\n",
    "    res_ep_test = {\"n\": 0}\n",
    "else:\n",
    "    # Query rows only for those episodes\n",
    "    test_ep_ids = set(test_eps[\"episode_id\"].astype(str).tolist())\n",
    "    qrows = ep_long[(ep_long[\"episode_id\"].isin(test_ep_ids)) & (ep_long[\"role\"] == \"query\")].copy()\n",
    "    print(\"[CELL 06-10] query rows:\", qrows.shape)\n",
    "\n",
    "    # Fetch labels by joining pair_id to mars_pairs_test_ts (view created in Notebook 05)\n",
    "    import duckdb\n",
    "    con_ep = duckdb.connect(str(DUCKDB_PATH), read_only=True)\n",
    "\n",
    "    # Pull all needed labels in one go\n",
    "    pair_ids = qrows[\"pair_id\"].astype(int).drop_duplicates().tolist()\n",
    "    if len(pair_ids) == 0:\n",
    "        res_ep_test = {\"n\": 0}\n",
    "    else:\n",
    "        # Build IN list safely (small list)\n",
    "        in_list = \",\".join([str(int(x)) for x in pair_ids])\n",
    "        df_labels = con_ep.execute(f\"\"\"\n",
    "        SELECT pair_id, CAST(label AS INTEGER) AS label\n",
    "        FROM mars_pairs_test_ts\n",
    "        WHERE pair_id IN ({in_list})\n",
    "        \"\"\").fetchdf()\n",
    "\n",
    "        con_ep.close()\n",
    "\n",
    "        if df_labels.shape[0] != len(pair_ids):\n",
    "            print(\"[CELL 06-10] WARN: some pair_ids missing from mars_pairs_test_ts\",\n",
    "                  \"expected\", len(pair_ids), \"got\", df_labels.shape[0])\n",
    "\n",
    "        # Merge back to get per-query true labels\n",
    "        q_eval = qrows.merge(df_labels, on=\"pair_id\", how=\"inner\")\n",
    "        print(\"[CELL 06-10] q_eval shape (after join):\", q_eval.shape)\n",
    "\n",
    "        res_ep_test = eval_popularity(q_eval, rank_map, cutoffs)\n",
    "\n",
    "print(\"[CELL 06-10] EPISODE_TEST(pop) metrics:\", res_ep_test)\n",
    "\n",
    "report = read_json(REPORT_PATH)\n",
    "report[\"metrics\"][\"popularity_episode_test_query\"] = res_ep_test\n",
    "report[\"key_findings\"].append(\"Computed popularity baseline on episodic test query set (episodes_index/long).\")\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "cell_end(\"CELL 06-10\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24abba9",
   "metadata": {},
   "source": [
    "Write manifest (artifacts) + close out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "982cceef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-11] Write manifest artifacts\n",
      "[CELL 06-11] start=2026-01-06T23:42:44\n",
      "[CELL 06-11] updated: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\manifest.json\n",
      "[CELL 06-11] n_artifacts=3\n",
      "[CELL 06-11] elapsed=0.04s\n",
      "[CELL 06-11] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-11] Update manifest (artifacts)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-11\", \"Write manifest artifacts\")\n",
    "\n",
    "manifest = read_json(MANIFEST_PATH)\n",
    "\n",
    "# record key artifacts for this run\n",
    "for p in [Path(CONFIG_PATH), Path(REPORT_PATH), pop_path]:\n",
    "    manifest[\"artifacts\"].append(safe_artifact_record(Path(p)))\n",
    "\n",
    "write_json_atomic(MANIFEST_PATH, manifest)\n",
    "print(\"[CELL 06-11] updated:\", MANIFEST_PATH)\n",
    "\n",
    "cell_end(\"CELL 06-11\", t0, n_artifacts=len(manifest[\"artifacts\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c2289a",
   "metadata": {},
   "source": [
    "Load sessionized tables (gap30m) + sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "771bd65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-12] Load sessions/events gap30m (parquet)\n",
      "[CELL 06-12] start=2026-01-06T23:42:47\n",
      "[CELL 06-12] sessions shape: (1322, 8)\n",
      "[CELL 06-12] events shape: (3659, 10)\n",
      "[CELL 06-12] sessions cols: ['session_id', 'user_id', 'sess_num', 'session_start_ts', 'session_end_ts', 'n_events', 'duration_sec', 'n_unique_items']\n",
      "[CELL 06-12] events cols: ['user_id', 'item_id', 'rating', 'ts', 'ts_epoch', 'sess_num', 'session_id', 'pos_in_sess', 'sess_len', 'ts_raw']\n",
      "[CELL 06-12] sessions head3:\n",
      "   session_id  user_id  sess_num          session_start_ts            session_end_ts  n_events  duration_sec  n_unique_items\n",
      "266810_000001   266810       1.0 2018-10-15 10:00:59+00:00 2018-10-15 10:00:59+00:00         1             0               1\n",
      "411044_000002   411044       2.0 2020-04-11 19:23:35+00:00 2020-04-11 19:23:35+00:00         1             0               1\n",
      "534018_000001   534018       1.0 2020-06-22 03:25:57+00:00 2020-06-22 03:25:57+00:00         1             0               1\n",
      "[CELL 06-12] events head3:\n",
      " user_id  item_id  rating                        ts   ts_epoch  sess_num   session_id  pos_in_sess  sess_len              ts_raw\n",
      "    3928      528      10 2020-06-24 02:12:28+00:00 1592964748       1.0  3928_000001            1         1 2020-06-24 10:12:28\n",
      "   73173   125621      10 2019-09-14 12:37:00+00:00 1568464620       1.0 73173_000001            1         1 2019-09-14 20:37:00\n",
      "   80299    43457       7 2019-01-31 01:26:27+00:00 1548897987       1.0 80299_000001            1         1 2019-01-31 09:26:27\n",
      "[CELL 06-12] normalized sessions head3 (with epochs):\n",
      "   session_id user_id  start_ts_epoch  end_ts_epoch  n_events\n",
      "266810_000001  266810         1539597       1539597         1\n",
      "411044_000002  411044         1586633       1586633         1\n",
      "534018_000001  534018         1592796       1592796         1\n",
      "[CELL 06-12] elapsed=0.03s\n",
      "[CELL 06-12] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-12] Load sessionized MARS sessions/events (gap30m) for Session-KNN  ✅ FIXED schema\n",
    "\n",
    "t0 = cell_start(\"CELL 06-12\", \"Load sessions/events gap30m (parquet)\")\n",
    "\n",
    "SESS_DIR = PATHS[\"DATA_PROCESSED\"] / \"mars\" / \"sessions\"\n",
    "SESSIONS_PQ = SESS_DIR / \"sessions_gap30m.parquet\"\n",
    "EVENTS_PQ   = SESS_DIR / \"events_gap30m.parquet\"\n",
    "\n",
    "for p in [SESSIONS_PQ, EVENTS_PQ]:\n",
    "    if not p.exists():\n",
    "        raise RuntimeError(f\"Missing required session file: {p}\")\n",
    "\n",
    "sessions = pd.read_parquet(SESSIONS_PQ)\n",
    "events   = pd.read_parquet(EVENTS_PQ)\n",
    "\n",
    "print(\"[CELL 06-12] sessions shape:\", sessions.shape)\n",
    "print(\"[CELL 06-12] events shape:\", events.shape)\n",
    "print(\"[CELL 06-12] sessions cols:\", list(sessions.columns))\n",
    "print(\"[CELL 06-12] events cols:\", list(events.columns))\n",
    "\n",
    "print(\"[CELL 06-12] sessions head3:\")\n",
    "print(sessions.head(3).to_string(index=False))\n",
    "print(\"[CELL 06-12] events head3:\")\n",
    "print(events.head(3).to_string(index=False))\n",
    "\n",
    "# ---- normalize sessions schema ----\n",
    "# expected canonical:\n",
    "# session_id, user_id, start_ts_epoch, end_ts_epoch, n_events\n",
    "if \"start_ts_epoch\" not in sessions.columns:\n",
    "    if \"session_start_ts\" in sessions.columns:\n",
    "        sessions[\"start_ts_epoch\"] = pd.to_datetime(sessions[\"session_start_ts\"], utc=True).astype(\"int64\") // 10**9\n",
    "    else:\n",
    "        raise RuntimeError(\"sessions missing start_ts_epoch and session_start_ts\")\n",
    "\n",
    "if \"end_ts_epoch\" not in sessions.columns:\n",
    "    if \"session_end_ts\" in sessions.columns:\n",
    "        sessions[\"end_ts_epoch\"] = pd.to_datetime(sessions[\"session_end_ts\"], utc=True).astype(\"int64\") // 10**9\n",
    "    else:\n",
    "        raise RuntimeError(\"sessions missing end_ts_epoch and session_end_ts\")\n",
    "\n",
    "need_sess_cols = {\"session_id\", \"user_id\", \"start_ts_epoch\", \"end_ts_epoch\", \"n_events\"}\n",
    "need_ev_cols   = {\"session_id\", \"user_id\", \"ts_epoch\", \"pos_in_sess\", \"item_id\"}\n",
    "\n",
    "miss_s = need_sess_cols - set(sessions.columns)\n",
    "miss_e = need_ev_cols - set(events.columns)\n",
    "if miss_s:\n",
    "    raise RuntimeError(f\"Missing columns in sessions_gap30m after normalization: {miss_s}\")\n",
    "if miss_e:\n",
    "    raise RuntimeError(f\"Missing columns in events_gap30m: {miss_e}\")\n",
    "\n",
    "# types\n",
    "sessions[\"session_id\"] = sessions[\"session_id\"].astype(str)\n",
    "sessions[\"user_id\"] = sessions[\"user_id\"].astype(str)\n",
    "events[\"session_id\"] = events[\"session_id\"].astype(str)\n",
    "events[\"user_id\"] = events[\"user_id\"].astype(str)\n",
    "\n",
    "print(\"[CELL 06-12] normalized sessions head3 (with epochs):\")\n",
    "print(sessions[[\"session_id\",\"user_id\",\"start_ts_epoch\",\"end_ts_epoch\",\"n_events\"]].head(3).to_string(index=False))\n",
    "\n",
    "cell_end(\"CELL 06-12\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2acb7c",
   "metadata": {},
   "source": [
    "Attach split labels to sessions/events (train/val/test users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50eebaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-13] Attach split labels via user_split_map\n",
      "[CELL 06-13] start=2026-01-06T23:42:52\n",
      "[CELL 06-13] sessions by split:\n",
      "split\n",
      "train    1084\n",
      "test      129\n",
      "val       109\n",
      "[CELL 06-13] events by split:\n",
      "split\n",
      "train    3016\n",
      "test      343\n",
      "val       300\n",
      "[CELL 06-13] elapsed=0.01s\n",
      "[CELL 06-13] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-13] Tag sessions/events with user split (train/val/test)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-13\", \"Attach split labels via user_split_map\")\n",
    "\n",
    "split_map_pq = PATHS[\"DATA_PROCESSED\"] / \"mars\" / \"user_splits\" / \"user_split_map.parquet\"\n",
    "if not split_map_pq.exists():\n",
    "    raise RuntimeError(f\"Missing split map: {split_map_pq}\")\n",
    "\n",
    "split_map = pd.read_parquet(split_map_pq)\n",
    "split_map[\"user_id\"] = split_map[\"user_id\"].astype(str)\n",
    "\n",
    "# cast ids to string for safe joins\n",
    "sessions[\"user_id\"] = sessions[\"user_id\"].astype(str)\n",
    "events[\"user_id\"] = events[\"user_id\"].astype(str)\n",
    "\n",
    "sessions = sessions.merge(split_map, on=\"user_id\", how=\"left\")\n",
    "events   = events.merge(split_map, on=\"user_id\", how=\"left\")\n",
    "\n",
    "if sessions[\"split\"].isna().any():\n",
    "    n_na = int(sessions[\"split\"].isna().sum())\n",
    "    raise RuntimeError(f\"Found {n_na} sessions with missing split label (user_id not in split_map).\")\n",
    "\n",
    "if events[\"split\"].isna().any():\n",
    "    n_na = int(events[\"split\"].isna().sum())\n",
    "    raise RuntimeError(f\"Found {n_na} events with missing split label (user_id not in split_map).\")\n",
    "\n",
    "print(\"[CELL 06-13] sessions by split:\")\n",
    "print(sessions[\"split\"].value_counts().to_string())\n",
    "print(\"[CELL 06-13] events by split:\")\n",
    "print(events[\"split\"].value_counts().to_string())\n",
    "\n",
    "cell_end(\"CELL 06-13\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef911d5",
   "metadata": {},
   "source": [
    "Build session sequences (item lists) per split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f3bee",
   "metadata": {},
   "source": [
    "We’ll build sequences from train sessions only, and evaluate on test sessions by predicting the next item at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "953b5e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-14] Build sequences per session\n",
      "[CELL 06-14] start=2026-01-06T23:42:55\n",
      "[CELL 06-14] seq_df shape: (1322, 5)\n",
      "[CELL 06-14] seq_df head3:\n",
      "   session_id split user_id                                                                                                                          items  n\n",
      "104074_000001 train  104074                                                                                                                        [32033]  1\n",
      "104074_000002 train  104074                                                         [52609, 52616, 52615, 52610, 52614, 52618, 52611, 52612, 52617, 52619] 10\n",
      "104074_000003 train  104074 [45209, 45206, 45207, 45211, 45214, 45212, 45213, 45215, 45216, 45224, 45225, 45226, 45227, 45219, 45223, 45232, 45233, 45234] 18\n",
      "[CELL 06-14] sessions >=2 by split: {'train': 450, 'val': 51, 'test': 60}\n",
      "[CELL 06-14] elapsed=0.04s\n",
      "[CELL 06-14] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-14] Build session sequences (ordered item_id lists) per split\n",
    "\n",
    "t0 = cell_start(\"CELL 06-14\", \"Build sequences per session\")\n",
    "\n",
    "# ensure proper ordering\n",
    "events_sorted = events.sort_values([\"session_id\", \"pos_in_sess\", \"ts_epoch\"], ascending=[True, True, True]).copy()\n",
    "\n",
    "# group to sequences\n",
    "seq_df = (\n",
    "    events_sorted.groupby([\"session_id\", \"split\"], as_index=False)\n",
    "    .agg(user_id=(\"user_id\", \"first\"),\n",
    "         items=(\"item_id\", lambda x: [int(v) for v in x.tolist()]),\n",
    "         n=(\"item_id\", \"size\"))\n",
    ")\n",
    "\n",
    "print(\"[CELL 06-14] seq_df shape:\", seq_df.shape)\n",
    "print(\"[CELL 06-14] seq_df head3:\")\n",
    "print(seq_df.head(3).to_string(index=False))\n",
    "\n",
    "# Keep only sessions with length >= 2 (needed for next-item prediction)\n",
    "seq_df = seq_df[seq_df[\"n\"] >= 2].reset_index(drop=True)\n",
    "\n",
    "train_sessions = seq_df[seq_df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "val_sessions   = seq_df[seq_df[\"split\"] == \"val\"].reset_index(drop=True)\n",
    "test_sessions  = seq_df[seq_df[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "print(\"[CELL 06-14] sessions >=2 by split:\",\n",
    "      {\"train\": int(train_sessions.shape[0]), \"val\": int(val_sessions.shape[0]), \"test\": int(test_sessions.shape[0])})\n",
    "\n",
    "cell_end(\"CELL 06-14\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a3ad90",
   "metadata": {},
   "source": [
    "Session-KNN implementation (simple, logged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02527c0d",
   "metadata": {},
   "source": [
    "This is a standard Session-KNN style:\n",
    "- represent each session as set of items\n",
    "- similarity = cosine over binary vectors (implemented via overlap / sqrt(lenA*lenB))\n",
    "- score candidate items by weighted sum from top-N neighbor sessions\n",
    "- evaluate next-item prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad8ac4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-15] Session-KNN baseline\n",
      "[CELL 06-15] start=2026-01-06T23:43:00\n",
      "[CELL 06-15] using all train sessions: 450\n",
      "[CELL 06-15] SESSION_KNN global TEST metrics: {'n': 214, 'HR@5': 0.5186915887850467, 'MRR@5': 0.33021806853582564, 'NDCG@5': 0.3770792651352578, 'HR@10': 0.5934579439252337, 'MRR@10': 0.3405670523661179, 'NDCG@10': 0.40162067741386975, 'HR@20': 0.6495327102803738, 'MRR@20': 0.3446143502341241, 'NDCG@20': 0.41598271648375024}\n",
      "[CELL 06-15] elapsed=0.04s\n",
      "[CELL 06-15] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-15] Session-KNN baseline (train sessions -> predict next items)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-15\", \"Session-KNN baseline\")\n",
    "\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "KNN_K = 100         # neighbors\n",
    "KNN_SAMPLE = 5000   # max train sessions to consider (speed cap)\n",
    "cutoffs = list(map(int, CFG[\"eval\"][\"cutoffs\"]))\n",
    "maxK = max(cutoffs)\n",
    "\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# Prepare train session index\n",
    "train_items = train_sessions[\"items\"].tolist()\n",
    "train_lens = np.array([len(s) for s in train_items], dtype=np.int32)\n",
    "\n",
    "# Optional speed cap: sample train sessions deterministically\n",
    "if len(train_items) > KNN_SAMPLE:\n",
    "    idx = rng.permutation(len(train_items))[:KNN_SAMPLE]\n",
    "    train_items = [train_items[i] for i in idx]\n",
    "    train_lens = train_lens[idx]\n",
    "    print(f\"[CELL 06-15] sampled train sessions for speed: {len(train_items)}\")\n",
    "else:\n",
    "    print(f\"[CELL 06-15] using all train sessions: {len(train_items)}\")\n",
    "\n",
    "# Build inverted index: item -> list of train session indices\n",
    "inv = defaultdict(list)\n",
    "for si, items in enumerate(train_items):\n",
    "    # unique items per session for overlap-based similarity\n",
    "    for it in set(items):\n",
    "        inv[int(it)].append(si)\n",
    "\n",
    "def session_sim(a_set, b_set, len_a, len_b) -> float:\n",
    "    # cosine on binary vectors: |A∩B| / sqrt(|A||B|)\n",
    "    inter = len(a_set & b_set)\n",
    "    if inter == 0:\n",
    "        return 0.0\n",
    "    return float(inter) / math.sqrt(float(len_a) * float(len_b))\n",
    "\n",
    "def recommend_next(prefix_items, exclude_set, topn=maxK):\n",
    "    \"\"\"\n",
    "    prefix_items: list[int] observed so far in the test session\n",
    "    exclude_set: items already seen in prefix (avoid recommending repeats)\n",
    "    \"\"\"\n",
    "    a_set = set(prefix_items)\n",
    "    len_a = len(a_set) if len(a_set) else 1\n",
    "\n",
    "    # Candidate neighbor sessions: union of sessions containing any item in prefix\n",
    "    cand_sessions = set()\n",
    "    for it in a_set:\n",
    "        cand_sessions.update(inv.get(int(it), []))\n",
    "\n",
    "    if not cand_sessions:\n",
    "        return []  # no neighbors\n",
    "\n",
    "    # Score neighbor similarity\n",
    "    sims = []\n",
    "    for si in cand_sessions:\n",
    "        b = set(train_items[si])\n",
    "        s = session_sim(a_set, b, len_a, len(b) if len(b) else 1)\n",
    "        if s > 0:\n",
    "            sims.append((si, s))\n",
    "\n",
    "    if not sims:\n",
    "        return []\n",
    "\n",
    "    sims.sort(key=lambda x: x[1], reverse=True)\n",
    "    sims = sims[:KNN_K]\n",
    "\n",
    "    # Score items from neighbors\n",
    "    score = defaultdict(float)\n",
    "    for si, s in sims:\n",
    "        for it in train_items[si]:\n",
    "            it = int(it)\n",
    "            if it in exclude_set:\n",
    "                continue\n",
    "            score[it] += s\n",
    "\n",
    "    if not score:\n",
    "        return []\n",
    "\n",
    "    ranked = sorted(score.items(), key=lambda x: (-x[1], x[0]))\n",
    "    return [it for it, _ in ranked[:topn]]\n",
    "\n",
    "def eval_sessions_knn(sess_df: pd.DataFrame, max_steps_per_session: int = 50) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate next-item prediction on sessions.\n",
    "    For each session, for t=1..len-1, predict item[t] from prefix item[:t].\n",
    "    \"\"\"\n",
    "    hr = {K: 0.0 for K in cutoffs}\n",
    "    mrr = {K: 0.0 for K in cutoffs}\n",
    "    ndcg = {K: 0.0 for K in cutoffs}\n",
    "    n = 0\n",
    "\n",
    "    for items in sess_df[\"items\"].tolist():\n",
    "        L = len(items)\n",
    "        steps = min(L - 1, max_steps_per_session)\n",
    "        for t in range(1, 1 + steps):\n",
    "            prefix = items[:t]\n",
    "            true = int(items[t])\n",
    "            recs = recommend_next(prefix, exclude_set=set(prefix), topn=maxK)\n",
    "            n += 1\n",
    "            # rank of true\n",
    "            if true in recs:\n",
    "                r = recs.index(true) + 1\n",
    "            else:\n",
    "                r = 10**9\n",
    "            for K in cutoffs:\n",
    "                if r <= K:\n",
    "                    hr[K] += 1.0\n",
    "                    mrr[K] += 1.0 / float(r)\n",
    "                    ndcg[K] += 1.0 / math.log2(float(r) + 1.0)\n",
    "\n",
    "    out = {\"n\": int(n)}\n",
    "    denom = float(n) if n else 1.0\n",
    "    for K in cutoffs:\n",
    "        out[f\"HR@{K}\"] = hr[K] / denom\n",
    "        out[f\"MRR@{K}\"] = mrr[K] / denom\n",
    "        out[f\"NDCG@{K}\"] = ndcg[K] / denom\n",
    "    return out\n",
    "\n",
    "# Evaluate on global TEST sessions\n",
    "res_sknn_test = eval_sessions_knn(test_sessions)\n",
    "\n",
    "print(\"[CELL 06-15] SESSION_KNN global TEST metrics:\", res_sknn_test)\n",
    "\n",
    "# Save into report\n",
    "report = read_json(REPORT_PATH)\n",
    "report[\"metrics\"][\"session_knn_global_test\"] = res_sknn_test\n",
    "report[\"key_findings\"].append(\"Computed Session-KNN baseline on test sessions (next-item prediction).\")\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "cell_end(\"CELL 06-15\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2cb9b2",
   "metadata": {},
   "source": [
    "Load episode test query targets (pair_id → session_id, tpos, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f98f3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-16] Episode test query targets from mars_pairs_test_ts\n",
      "[CELL 06-16] start=2026-01-06T23:43:07\n",
      "[CELL 06-16] test_eps: (2, 9)\n",
      "[CELL 06-16] unique query pair_ids: 25 total query rows: 40\n",
      "[CELL 06-16] pairs_meta shape: (25, 6)\n",
      "[CELL 06-16] pairs_meta head5:\n",
      " pair_id user_id    session_id  tpos  label  label_ts_epoch\n",
      "      25  234863 234863_000001     7    441      1538634777\n",
      "      26  234863 234863_000001     8    442      1538635313\n",
      "      27  234863 234863_000001     9    443      1538635501\n",
      "      28  234863 234863_000001    10    444      1538635701\n",
      "      29  234863 234863_000001    11    445      1538636791\n",
      "[CELL 06-16] q_eval shape: (40, 9)\n",
      "[CELL 06-16] elapsed=0.04s\n",
      "[CELL 06-16] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-16] Build episodic test query targets (session_id, tpos, label) from pair_id\n",
    "\n",
    "t0 = cell_start(\"CELL 06-16\", \"Episode test query targets from mars_pairs_test_ts\")\n",
    "\n",
    "ep_index = pd.read_parquet(EP_INDEX_PQ)\n",
    "ep_long  = pd.read_parquet(EP_LONG_PQ)\n",
    "\n",
    "test_eps = ep_index[ep_index[\"split\"] == \"test\"].copy()\n",
    "print(\"[CELL 06-16] test_eps:\", test_eps.shape)\n",
    "\n",
    "if test_eps.shape[0] == 0:\n",
    "    raise RuntimeError(\"No test episodes found. Cannot evaluate episodic Session-KNN.\")\n",
    "\n",
    "test_ep_ids = set(test_eps[\"episode_id\"].astype(str).tolist())\n",
    "qrows = ep_long[(ep_long[\"episode_id\"].isin(test_ep_ids)) & (ep_long[\"role\"] == \"query\")].copy()\n",
    "q_pair_ids = qrows[\"pair_id\"].astype(int).drop_duplicates().tolist()\n",
    "print(\"[CELL 06-16] unique query pair_ids:\", len(q_pair_ids), \"total query rows:\", qrows.shape[0])\n",
    "\n",
    "import duckdb\n",
    "con_ro = duckdb.connect(str(DUCKDB_PATH), read_only=True)\n",
    "\n",
    "# Fetch pair metadata from mars_pairs_test_ts (created in Notebook 05)\n",
    "in_list = \",\".join([str(int(x)) for x in q_pair_ids])\n",
    "pairs_meta = con_ro.execute(f\"\"\"\n",
    "SELECT\n",
    "  pair_id,\n",
    "  CAST(user_id AS VARCHAR) AS user_id,\n",
    "  CAST(session_id AS VARCHAR) AS session_id,\n",
    "  CAST(tpos AS INTEGER) AS tpos,\n",
    "  CAST(label AS INTEGER) AS label,\n",
    "  CAST(label_ts_epoch AS BIGINT) AS label_ts_epoch\n",
    "FROM mars_pairs_test_ts\n",
    "WHERE pair_id IN ({in_list})\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "con_ro.close()\n",
    "\n",
    "print(\"[CELL 06-16] pairs_meta shape:\", pairs_meta.shape)\n",
    "print(\"[CELL 06-16] pairs_meta head5:\")\n",
    "print(pairs_meta.head(5).to_string(index=False))\n",
    "\n",
    "# Join to add episode_id to each query row (some pair_id may repeat across episodes; keep rows)\n",
    "q_eval = qrows.merge(pairs_meta, on=\"pair_id\", how=\"inner\")\n",
    "print(\"[CELL 06-16] q_eval shape:\", q_eval.shape)\n",
    "\n",
    "if q_eval.shape[0] != qrows.shape[0]:\n",
    "    print(\"[CELL 06-16] WARN: some query rows missing after join\",\n",
    "          \"expected\", qrows.shape[0], \"got\", q_eval.shape[0])\n",
    "\n",
    "cell_end(\"CELL 06-16\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe392cf",
   "metadata": {},
   "source": [
    "Build fast lookup: session_id → ordered items list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ab7aa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-17] Build session_id -> ordered items lookup\n",
      "[CELL 06-17] start=2026-01-06T23:43:13\n",
      "[CELL 06-17] need_sessions: 1\n",
      "[CELL 06-17] built sess_items: 1\n",
      "[CELL 06-17] sample session: 234863_000001 len: 31 items[:10]: [43457, 43458, 43459, 43460, 43461, 43462, 43463, 43464, 43465, 43466]\n",
      "[CELL 06-17] elapsed=0.00s\n",
      "[CELL 06-17] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-17] Build session->ordered items lookup for test sessions\n",
    "\n",
    "t0 = cell_start(\"CELL 06-17\", \"Build session_id -> ordered items lookup\")\n",
    "\n",
    "# events_sorted was created in 06-14; ensure it's available\n",
    "try:\n",
    "    _ = events_sorted.shape\n",
    "except Exception:\n",
    "    events_sorted = events.sort_values([\"session_id\", \"pos_in_sess\", \"ts_epoch\"], ascending=[True, True, True]).copy()\n",
    "\n",
    "# Build dict only for sessions we need (from q_eval)\n",
    "need_sessions = set(q_eval[\"session_id\"].astype(str).tolist())\n",
    "print(\"[CELL 06-17] need_sessions:\", len(need_sessions))\n",
    "\n",
    "ev_need = events_sorted[events_sorted[\"session_id\"].astype(str).isin(need_sessions)].copy()\n",
    "\n",
    "# Create ordered list per session\n",
    "sess_items = (\n",
    "    ev_need.groupby(\"session_id\")[\"item_id\"]\n",
    "    .apply(lambda x: [int(v) for v in x.tolist()])\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "print(\"[CELL 06-17] built sess_items:\", len(sess_items))\n",
    "# sanity sample\n",
    "sample_k = next(iter(sess_items.keys()))\n",
    "print(\"[CELL 06-17] sample session:\", sample_k, \"len:\", len(sess_items[sample_k]), \"items[:10]:\", sess_items[sample_k][:10])\n",
    "\n",
    "cell_end(\"CELL 06-17\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8f6aef",
   "metadata": {},
   "source": [
    "Episodic Session-KNN evaluation (query-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a6d5fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-18] Session-KNN episodic test evaluation (query-only)\n",
      "[CELL 06-18] start=2026-01-06T23:43:15\n",
      "[CELL 06-18] SESSION_KNN episode TEST (query-only) metrics: {'n': 40, 'miss_prefix': 0, 'HR@5': 0.0, 'MRR@5': 0.0, 'NDCG@5': 0.0, 'HR@10': 0.0, 'MRR@10': 0.0, 'NDCG@10': 0.0, 'HR@20': 0.0, 'MRR@20': 0.0, 'NDCG@20': 0.0}\n",
      "[CELL 06-18] elapsed=0.02s\n",
      "[CELL 06-18] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-18] Session-KNN episodic test evaluation (query-only points)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-18\", \"Session-KNN episodic test evaluation (query-only)\")\n",
    "\n",
    "cutoffs = list(map(int, CFG[\"eval\"][\"cutoffs\"]))\n",
    "maxK = max(cutoffs)\n",
    "\n",
    "import math\n",
    "\n",
    "def eval_rank(rank: int, K: int) -> Dict[str, float]:\n",
    "    if rank <= K:\n",
    "        return {\n",
    "            \"HR\": 1.0,\n",
    "            \"MRR\": 1.0 / float(rank),\n",
    "            \"NDCG\": 1.0 / math.log2(float(rank) + 1.0),\n",
    "        }\n",
    "    return {\"HR\": 0.0, \"MRR\": 0.0, \"NDCG\": 0.0}\n",
    "\n",
    "hr = {K: 0.0 for K in cutoffs}\n",
    "mrr = {K: 0.0 for K in cutoffs}\n",
    "ndcg = {K: 0.0 for K in cutoffs}\n",
    "n = 0\n",
    "\n",
    "miss_prefix = 0\n",
    "\n",
    "for row in q_eval.itertuples(index=False):\n",
    "    # reconstruct prefix from session items up to tpos-1 (tpos is 1-indexed in your data)\n",
    "    sid = str(row.session_id)\n",
    "    tpos = int(row.tpos)\n",
    "    true = int(row.label)\n",
    "\n",
    "    items = sess_items.get(sid)\n",
    "    if items is None:\n",
    "        miss_prefix += 1\n",
    "        continue\n",
    "\n",
    "    # events pos_in_sess starts at 1, so prefix length = tpos-1\n",
    "    pref_len = max(tpos - 1, 1)\n",
    "    prefix = items[:pref_len]\n",
    "\n",
    "    recs = recommend_next(prefix, exclude_set=set(prefix), topn=maxK)\n",
    "\n",
    "    n += 1\n",
    "    if true in recs:\n",
    "        r = recs.index(true) + 1\n",
    "    else:\n",
    "        r = 10**9\n",
    "\n",
    "    for K in cutoffs:\n",
    "        m = eval_rank(r, int(K))\n",
    "        hr[K] += m[\"HR\"]\n",
    "        mrr[K] += m[\"MRR\"]\n",
    "        ndcg[K] += m[\"NDCG\"]\n",
    "\n",
    "out = {\"n\": int(n), \"miss_prefix\": int(miss_prefix)}\n",
    "den = float(n) if n else 1.0\n",
    "for K in cutoffs:\n",
    "    out[f\"HR@{K}\"] = hr[K] / den\n",
    "    out[f\"MRR@{K}\"] = mrr[K] / den\n",
    "    out[f\"NDCG@{K}\"] = ndcg[K] / den\n",
    "\n",
    "print(\"[CELL 06-18] SESSION_KNN episode TEST (query-only) metrics:\", out)\n",
    "\n",
    "report = read_json(REPORT_PATH)\n",
    "report[\"metrics\"][\"session_knn_episode_test_query\"] = out\n",
    "report[\"key_findings\"].append(\"Computed Session-KNN baseline on episodic test query points (cold-start protocol).\")\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "cell_end(\"CELL 06-18\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e71fc61",
   "metadata": {},
   "source": [
    "Diagnose why episodic Session-KNN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ecee2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-19] Diagnose episodic Session-KNN=0\n",
      "[CELL 06-19] start=2026-01-06T23:43:20\n",
      "[CELL 06-19] train_item_set size: 703\n",
      "[CELL 06-19] diagnostics: {'n': 40, 'recs_empty': 0, 'no_candidate_sessions': 0, 'true_in_prefix': 0, 'true_not_in_train': 40, 'avg_candidates': 64.425, 'avg_recs_len': 20.0}\n",
      "[CELL 06-19] sample query points head10:\n",
      "         episode_id    session_id  tpos  label  label_in_train\n",
      "test_K5_Q20_e000051 234863_000001     7    441               0\n",
      "test_K5_Q20_e000051 234863_000001     8    442               0\n",
      "test_K5_Q20_e000051 234863_000001     9    443               0\n",
      "test_K5_Q20_e000051 234863_000001    10    444               0\n",
      "test_K5_Q20_e000051 234863_000001    11    445               0\n",
      "test_K5_Q20_e000051 234863_000001    12    445               0\n",
      "test_K5_Q20_e000051 234863_000001    13    446               0\n",
      "test_K5_Q20_e000051 234863_000001    14    447               0\n",
      "test_K5_Q20_e000051 234863_000001    15    448               0\n",
      "test_K5_Q20_e000051 234863_000001    16    449               0\n",
      "[CELL 06-19] elapsed=0.01s\n",
      "[CELL 06-19] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-19] Diagnose episodic Session-KNN failures (why all zeros)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-19\", \"Diagnose episodic Session-KNN=0\")\n",
    "\n",
    "cutoffs = list(map(int, CFG[\"eval\"][\"cutoffs\"]))\n",
    "maxK = max(cutoffs)\n",
    "\n",
    "# Build train item set (from train sessions used by Session-KNN)\n",
    "train_item_set = set()\n",
    "for s in train_items:\n",
    "    for it in s:\n",
    "        train_item_set.add(int(it))\n",
    "print(\"[CELL 06-19] train_item_set size:\", len(train_item_set))\n",
    "\n",
    "stats = {\n",
    "    \"n\": 0,\n",
    "    \"recs_empty\": 0,\n",
    "    \"no_candidate_sessions\": 0,\n",
    "    \"true_in_prefix\": 0,\n",
    "    \"true_not_in_train\": 0,\n",
    "    \"avg_candidates\": 0.0,\n",
    "    \"avg_recs_len\": 0.0,\n",
    "}\n",
    "\n",
    "# helper: count candidate sessions quickly (same logic used in recommend_next)\n",
    "def count_candidates(prefix_items):\n",
    "    a_set = set(prefix_items)\n",
    "    cand = set()\n",
    "    for it in a_set:\n",
    "        cand.update(inv.get(int(it), []))\n",
    "    return len(cand)\n",
    "\n",
    "for row in q_eval.itertuples(index=False):\n",
    "    sid = str(row.session_id)\n",
    "    tpos = int(row.tpos)\n",
    "    true = int(row.label)\n",
    "\n",
    "    items = sess_items.get(sid)\n",
    "    if items is None:\n",
    "        continue\n",
    "\n",
    "    pref_len = max(tpos - 1, 0)\n",
    "    prefix = items[:pref_len]\n",
    "\n",
    "    stats[\"n\"] += 1\n",
    "\n",
    "    if true in set(prefix):\n",
    "        stats[\"true_in_prefix\"] += 1\n",
    "\n",
    "    if true not in train_item_set:\n",
    "        stats[\"true_not_in_train\"] += 1\n",
    "\n",
    "    nc = count_candidates(prefix) if len(prefix) else 0\n",
    "    stats[\"avg_candidates\"] += float(nc)\n",
    "    if nc == 0:\n",
    "        stats[\"no_candidate_sessions\"] += 1\n",
    "\n",
    "    recs = recommend_next(prefix, exclude_set=set(prefix), topn=maxK)\n",
    "    stats[\"avg_recs_len\"] += float(len(recs))\n",
    "    if len(recs) == 0:\n",
    "        stats[\"recs_empty\"] += 1\n",
    "\n",
    "# finalize averages\n",
    "den = float(stats[\"n\"]) if stats[\"n\"] else 1.0\n",
    "stats[\"avg_candidates\"] /= den\n",
    "stats[\"avg_recs_len\"] /= den\n",
    "\n",
    "print(\"[CELL 06-19] diagnostics:\", stats)\n",
    "\n",
    "# Also show a tiny sample of problematic rows\n",
    "sample = q_eval.head(10)[[\"episode_id\",\"session_id\",\"tpos\",\"label\"]].copy()\n",
    "sample[\"label_in_train\"] = sample[\"label\"].astype(int).apply(lambda x: int(x in train_item_set))\n",
    "print(\"[CELL 06-19] sample query points head10:\")\n",
    "print(sample.to_string(index=False))\n",
    "\n",
    "cell_end(\"CELL 06-19\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d2e35",
   "metadata": {},
   "source": [
    "Record coverage finding in report (no re-eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3b64dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-20] Write coverage finding to report\n",
      "[CELL 06-20] start=2026-01-06T23:43:25\n",
      "[CELL 06-20] wrote coverage note into report.json\n",
      "[CELL 06-20] elapsed=0.01s\n",
      "[CELL 06-20] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-20] Record item-coverage finding (episodic test labels unseen in train)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-20\", \"Write coverage finding to report\")\n",
    "\n",
    "report = read_json(REPORT_PATH)\n",
    "\n",
    "coverage_note = {\n",
    "    \"episode_test_query_n\": int(40),\n",
    "    \"episode_test_query_true_not_in_train\": int(40),\n",
    "    \"coverage_rate\": float(0.0),\n",
    "    \"implication\": \"All episodic test query labels are unseen in train sessions; Session-KNN (and any train-item-only model) cannot hit them -> HR/MRR/NDCG = 0 by definition.\"\n",
    "}\n",
    "\n",
    "report[\"sanity_samples\"][\"episode_test_item_coverage\"] = coverage_note\n",
    "report[\"key_findings\"].append(\"Episodic test query labels have 0% item coverage in train sessions; Session-KNN yields 0 by definition under this protocol.\")\n",
    "\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "print(\"[CELL 06-20] wrote coverage note into report.json\")\n",
    "\n",
    "cell_end(\"CELL 06-20\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b5c4fe",
   "metadata": {},
   "source": [
    "GRU4Rec - Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eed8378",
   "metadata": {},
   "source": [
    "Torch seeding + device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ceaf44f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-21] Torch seeding + device\n",
      "[CELL 06-21] start=2026-01-06T23:43:29\n",
      "[CELL 06-21] torch: 2.6.0+cu124\n",
      "[CELL 06-21] device: cuda\n",
      "[CELL 06-21] elapsed=7.76s\n",
      "[CELL 06-21] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-21] Torch seed + device\n",
    "\n",
    "t0 = cell_start(\"CELL 06-21\", \"Torch seeding + device\")\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.use_deterministic_algorithms(False)  # CPU GRU is deterministic enough; avoid hard errors\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"[CELL 06-21] torch:\", torch.__version__)\n",
    "print(\"[CELL 06-21] device:\", device)\n",
    "\n",
    "cell_end(\"CELL 06-21\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4e76d5",
   "metadata": {},
   "source": [
    "Map events to vocab item indices + build session_id → idx-sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb7d27",
   "metadata": {},
   "source": [
    "This makes sequences consistent with label in pairs (which is already item-index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bb4b7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-22] Map events to item2id + build sess_items_idx\n",
      "[CELL 06-22] start=2026-01-06T23:43:44\n",
      "[CELL 06-22] events mapped: 3659 / 3659 (1.000)\n",
      "[CELL 06-22] sess_items_idx built: 1322\n",
      "[CELL 06-22] sample session: 104074_000001 len: 1 items[:10]: [408]\n",
      "[CELL 06-22] elapsed=0.04s\n",
      "[CELL 06-22] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-22] Map events.item_id -> vocab index + build session->items_idx lookup\n",
    "\n",
    "t0 = cell_start(\"CELL 06-22\", \"Map events to item2id + build sess_items_idx\")\n",
    "\n",
    "# item2id keys might be str(original_id)\n",
    "def map_item_to_idx(x):\n",
    "    return item2id.get(str(int(x)), None)\n",
    "\n",
    "events_m = events.copy()\n",
    "events_m[\"item_idx\"] = events_m[\"item_id\"].apply(map_item_to_idx)\n",
    "\n",
    "n_total = int(events_m.shape[0])\n",
    "n_mapped = int(events_m[\"item_idx\"].notna().sum())\n",
    "print(\"[CELL 06-22] events mapped:\", n_mapped, \"/\", n_total, f\"({n_mapped/max(n_total,1):.3f})\")\n",
    "\n",
    "if n_mapped < n_total:\n",
    "    print(\"[CELL 06-22] WARN: some events had item_id not found in item2id. They will be dropped for GRU4Rec sequences.\")\n",
    "\n",
    "events_m = events_m.dropna(subset=[\"item_idx\"]).copy()\n",
    "events_m[\"item_idx\"] = events_m[\"item_idx\"].astype(int)\n",
    "events_m[\"session_id\"] = events_m[\"session_id\"].astype(str)\n",
    "\n",
    "# ensure ordering inside sessions\n",
    "events_m = events_m.sort_values([\"session_id\", \"pos_in_sess\", \"ts_epoch\"], ascending=[True, True, True])\n",
    "\n",
    "# session -> list of item_idx\n",
    "sess_items_idx = (\n",
    "    events_m.groupby(\"session_id\")[\"item_idx\"]\n",
    "    .apply(lambda x: [int(v) for v in x.tolist()])\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "print(\"[CELL 06-22] sess_items_idx built:\", len(sess_items_idx))\n",
    "# sanity sample\n",
    "sk = next(iter(sess_items_idx.keys()))\n",
    "print(\"[CELL 06-22] sample session:\", sk, \"len:\", len(sess_items_idx[sk]), \"items[:10]:\", sess_items_idx[sk][:10])\n",
    "\n",
    "cell_end(\"CELL 06-22\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56992435",
   "metadata": {},
   "source": [
    "Build GRU4Rec training sequences from TRAIN sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e213f36",
   "metadata": {},
   "source": [
    "We train on train sessions only: next-item prediction for each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e262b6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-23] Build GRU training sequences from train sessions\n",
      "[CELL 06-23] start=2026-01-06T23:43:54\n",
      "[CELL 06-23] train sessions kept: 450 train points: 1932\n",
      "[CELL 06-23] val sessions kept: 51 val points: 191\n",
      "[CELL 06-23] elapsed=0.01s\n",
      "[CELL 06-23] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-23] Build GRU4Rec training data from TRAIN sessions\n",
    "\n",
    "t0 = cell_start(\"CELL 06-23\", \"Build GRU training sequences from train sessions\")\n",
    "\n",
    "# Use train session ids from sessions dataframe (already split-labeled in 06-13)\n",
    "train_sess_ids = sessions.loc[sessions[\"split\"] == \"train\", \"session_id\"].astype(str).unique().tolist()\n",
    "val_sess_ids   = sessions.loc[sessions[\"split\"] == \"val\", \"session_id\"].astype(str).unique().tolist()\n",
    "test_sess_ids  = sessions.loc[sessions[\"split\"] == \"test\", \"session_id\"].astype(str).unique().tolist()\n",
    "\n",
    "def build_xy_from_sessions(sess_ids):\n",
    "    X = []\n",
    "    Y = []\n",
    "    kept = 0\n",
    "    for sid in sess_ids:\n",
    "        seq = sess_items_idx.get(str(sid))\n",
    "        if seq is None or len(seq) < 2:\n",
    "            continue\n",
    "        # create step-wise training points\n",
    "        # inputs: seq[:t], target: seq[t]\n",
    "        for t in range(1, len(seq)):\n",
    "            X.append(seq[:t])\n",
    "            Y.append(int(seq[t]))\n",
    "        kept += 1\n",
    "    return X, np.array(Y, dtype=np.int64), kept\n",
    "\n",
    "X_train, y_train, n_train_sess_kept = build_xy_from_sessions(train_sess_ids)\n",
    "X_val, y_val, n_val_sess_kept = build_xy_from_sessions(val_sess_ids)\n",
    "\n",
    "print(\"[CELL 06-23] train sessions kept:\", n_train_sess_kept, \"train points:\", len(X_train))\n",
    "print(\"[CELL 06-23] val sessions kept:\", n_val_sess_kept, \"val points:\", len(X_val))\n",
    "\n",
    "if len(X_train) == 0:\n",
    "    raise RuntimeError(\"No training points built for GRU4Rec. Check mapping/sessionization.\")\n",
    "\n",
    "cell_end(\"CELL 06-23\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a60cc9b",
   "metadata": {},
   "source": [
    "Dataloader (pad) + GRU4Rec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5d5716b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-24] Define dataset/loader + model\n",
      "[CELL 06-24] start=2026-01-06T23:43:58\n",
      "[CELL 06-24] model params: 177840\n",
      "[CELL 06-24] elapsed=1.53s\n",
      "[CELL 06-24] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-24] Dataloader + GRU4Rec model definition\n",
    "\n",
    "t0 = cell_start(\"CELL 06-24\", \"Define dataset/loader + model\")\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PrefixDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], int(self.y[i])\n",
    "\n",
    "def collate_pad(batch):\n",
    "    seqs, ys = zip(*batch)\n",
    "    lens = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    maxlen = int(lens.max().item())\n",
    "    xpad = torch.zeros((len(seqs), maxlen), dtype=torch.long)\n",
    "    for i, s in enumerate(seqs):\n",
    "        xpad[i, :len(s)] = torch.tensor(s, dtype=torch.long)\n",
    "    y = torch.tensor(ys, dtype=torch.long)\n",
    "    return xpad, lens, y\n",
    "\n",
    "class GRU4Rec(nn.Module):\n",
    "    def __init__(self, n_items: int, emb_dim: int = 64, hid_dim: int = 100, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(n_items, emb_dim)\n",
    "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hid_dim, batch_first=True)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(hid_dim, n_items)\n",
    "\n",
    "    def forward(self, xpad, lens):\n",
    "        # xpad: [B,T]\n",
    "        emb = self.emb(xpad)  # [B,T,E]\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lens.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, h = self.gru(packed)  # h: [1,B,H]\n",
    "        h_last = h[-1]                   # [B,H]\n",
    "        h_last = self.drop(h_last)\n",
    "        logits = self.out(h_last)        # [B,n_items]\n",
    "        return logits\n",
    "\n",
    "BATCH = 256\n",
    "train_loader = DataLoader(PrefixDataset(X_train, y_train), batch_size=BATCH, shuffle=True, collate_fn=collate_pad)\n",
    "val_loader   = DataLoader(PrefixDataset(X_val, y_val), batch_size=BATCH, shuffle=False, collate_fn=collate_pad)\n",
    "\n",
    "model = GRU4Rec(n_items=n_items, emb_dim=64, hid_dim=100, dropout=0.1).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0)\n",
    "\n",
    "print(\"[CELL 06-24] model params:\", sum(p.numel() for p in model.parameters()))\n",
    "cell_end(\"CELL 06-24\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af31d9c8",
   "metadata": {},
   "source": [
    "Train GRU4Rec (with val loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a547822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-25] Train GRU4Rec\n",
      "[CELL 06-25] start=2026-01-06T23:44:04\n",
      "[CELL 06-25] epoch=1/10 train_loss=6.6368 val_loss=6.5919\n",
      "[CELL 06-25] saved best: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\models\\gru4rec_best.pt\n",
      "[CELL 06-25] epoch=2/10 train_loss=6.5155 val_loss=6.5008\n",
      "[CELL 06-25] saved best: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\models\\gru4rec_best.pt\n",
      "[CELL 06-25] epoch=3/10 train_loss=6.3966 val_loss=6.4044\n",
      "[CELL 06-25] saved best: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\models\\gru4rec_best.pt\n",
      "[CELL 06-25] epoch=4/10 train_loss=6.2678 val_loss=6.2971\n",
      "[CELL 06-25] saved best: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\models\\gru4rec_best.pt\n",
      "[CELL 06-25] epoch=5/10 train_loss=6.1195 val_loss=6.1691\n",
      "[CELL 06-25] saved best: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\models\\gru4rec_best.pt\n",
      "[CELL 06-25] epoch=6/10 train_loss=5.9355 val_loss=6.0174\n",
      "[CELL 06-25] saved best: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\models\\gru4rec_best.pt\n",
      "[CELL 06-25] epoch=7/10 train_loss=5.7191 val_loss=5.8569\n",
      "[CELL 06-25] saved best: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\models\\gru4rec_best.pt\n",
      "[CELL 06-25] epoch=8/10 train_loss=5.4857 val_loss=5.6725\n",
      "[CELL 06-25] saved best: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\models\\gru4rec_best.pt\n",
      "[CELL 06-25] epoch=9/10 train_loss=5.2472 val_loss=5.4934\n",
      "[CELL 06-25] saved best: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\models\\gru4rec_best.pt\n",
      "[CELL 06-25] epoch=10/10 train_loss=4.9904 val_loss=5.3132\n",
      "[CELL 06-25] saved best: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\models\\gru4rec_best.pt\n",
      "[CELL 06-25] loaded best weights: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\models\\gru4rec_best.pt\n",
      "[CELL 06-25] best_val=5.313231945037842\n",
      "[CELL 06-25] elapsed=1.60s\n",
      "[CELL 06-25] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-25] Train GRU4Rec (log train/val loss)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-25\", \"Train GRU4Rec\")\n",
    "\n",
    "EPOCHS = 10\n",
    "best_val = float(\"inf\")\n",
    "best_path = OUT_DIR / \"models\" / \"gru4rec_best.pt\"\n",
    "(best_path.parent).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def run_epoch(loader, train: bool):\n",
    "    model.train(train)\n",
    "    total = 0.0\n",
    "    n = 0\n",
    "    for xpad, lens, y in loader:\n",
    "        xpad = xpad.to(device)\n",
    "        lens = lens.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(xpad, lens)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        if train:\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        total += float(loss.item()) * int(y.shape[0])\n",
    "        n += int(y.shape[0])\n",
    "    return total / max(n, 1)\n",
    "\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    tr_loss = run_epoch(train_loader, train=True)\n",
    "    va_loss = run_epoch(val_loader, train=False) if len(X_val) else float(\"nan\")\n",
    "    print(f\"[CELL 06-25] epoch={ep}/{EPOCHS} train_loss={tr_loss:.4f} val_loss={va_loss:.4f}\")\n",
    "\n",
    "    if len(X_val) and va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "        print(\"[CELL 06-25] saved best:\", best_path)\n",
    "\n",
    "# load best if available\n",
    "if best_path.exists():\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "    print(\"[CELL 06-25] loaded best weights:\", best_path)\n",
    "\n",
    "cell_end(\"CELL 06-25\", t0, best_val=best_val if len(X_val) else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb3d3f2",
   "metadata": {},
   "source": [
    "GRU4Rec evaluation helper (rank metrics on query points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec27bcd7",
   "metadata": {},
   "source": [
    "This will evaluate on:\n",
    "- Global test pairs via mars_pairs_test_ts (214 rows)\n",
    "- Episodic test query via q_eval (40 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "012afd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-26] Define GRU4Rec evaluation on query points\n",
      "[CELL 06-26] start=2026-01-06T23:44:12\n",
      "[CELL 06-26] elapsed=0.00s\n",
      "[CELL 06-26] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-26] GRU4Rec eval helpers (rank-based)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-26\", \"Define GRU4Rec evaluation on query points\")\n",
    "\n",
    "import math\n",
    "\n",
    "def gru_topk(prefix_list_batch, topk: int):\n",
    "    # prefix_list_batch: list[list[int]]\n",
    "    # returns list[list[int]] topk item indices\n",
    "    if len(prefix_list_batch) == 0:\n",
    "        return []\n",
    "    lens = torch.tensor([len(s) for s in prefix_list_batch], dtype=torch.long)\n",
    "    maxlen = int(lens.max().item())\n",
    "    xpad = torch.zeros((len(prefix_list_batch), maxlen), dtype=torch.long)\n",
    "    for i, s in enumerate(prefix_list_batch):\n",
    "        xpad[i, :len(s)] = torch.tensor(s, dtype=torch.long)\n",
    "\n",
    "    xpad = xpad.to(device)\n",
    "    lens = lens.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(xpad, lens)  # [B,n_items]\n",
    "        _, idx = torch.topk(logits, k=topk, dim=1)\n",
    "    return idx.cpu().numpy().tolist()\n",
    "\n",
    "def eval_query_points_gru(df_points: pd.DataFrame, sess_items_idx: dict, cutoffs: list[int], batch_size: int = 256):\n",
    "    \"\"\"\n",
    "    df_points columns required: session_id (str), tpos (int), label (int)\n",
    "    Prefix is session items up to tpos-1.\n",
    "    \"\"\"\n",
    "    maxK = max(cutoffs)\n",
    "\n",
    "    # build prefixes + labels\n",
    "    prefixes = []\n",
    "    labels = []\n",
    "    for r in df_points.itertuples(index=False):\n",
    "        sid = str(r.session_id)\n",
    "        tpos = int(r.tpos)\n",
    "        y = int(r.label)\n",
    "        seq = sess_items_idx.get(sid)\n",
    "        if seq is None:\n",
    "            continue\n",
    "        pref = seq[:max(tpos - 1, 0)]\n",
    "        if len(pref) == 0:\n",
    "            continue\n",
    "        prefixes.append(pref)\n",
    "        labels.append(y)\n",
    "\n",
    "    n = len(labels)\n",
    "    out = {\"n\": int(n)}\n",
    "    if n == 0:\n",
    "        for K in cutoffs:\n",
    "            out[f\"HR@{K}\"] = 0.0\n",
    "            out[f\"MRR@{K}\"] = 0.0\n",
    "            out[f\"NDCG@{K}\"] = 0.0\n",
    "        return out\n",
    "\n",
    "    hr = {K: 0.0 for K in cutoffs}\n",
    "    mrr = {K: 0.0 for K in cutoffs}\n",
    "    ndcg = {K: 0.0 for K in cutoffs}\n",
    "\n",
    "    # batch\n",
    "    for i in range(0, n, batch_size):\n",
    "        pbatch = prefixes[i:i+batch_size]\n",
    "        ybatch = labels[i:i+batch_size]\n",
    "        top = gru_topk(pbatch, topk=maxK)\n",
    "\n",
    "        for recs, y in zip(top, ybatch):\n",
    "            if y in recs:\n",
    "                rnk = recs.index(y) + 1\n",
    "            else:\n",
    "                rnk = 10**9\n",
    "            for K in cutoffs:\n",
    "                if rnk <= K:\n",
    "                    hr[K] += 1.0\n",
    "                    mrr[K] += 1.0 / float(rnk)\n",
    "                    ndcg[K] += 1.0 / math.log2(float(rnk) + 1.0)\n",
    "\n",
    "    den = float(n)\n",
    "    for K in cutoffs:\n",
    "        out[f\"HR@{K}\"] = hr[K] / den\n",
    "        out[f\"MRR@{K}\"] = mrr[K] / den\n",
    "        out[f\"NDCG@{K}\"] = ndcg[K] / den\n",
    "    return out\n",
    "\n",
    "cell_end(\"CELL 06-26\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdb4b4c",
   "metadata": {},
   "source": [
    "Evaluate GRU4Rec on GLOBAL test pairs (mars_pairs_test_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7abaad36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-27] GRU4Rec eval: global test pairs\n",
      "[CELL 06-27] start=2026-01-06T23:44:19\n",
      "[CELL 06-27] pairs_test_ts shape: (214, 3)\n",
      "[CELL 06-27] GRU4REC GLOBAL_TEST metrics: {'n': 214, 'HR@5': 0.48598130841121495, 'MRR@5': 0.42204049844236763, 'NDCG@5': 0.43816739292111306, 'HR@10': 0.5046728971962616, 'MRR@10': 0.42459019433318507, 'NDCG@10': 0.4442703620467722, 'HR@20': 0.5373831775700935, 'MRR@20': 0.42708077229105273, 'NDCG@20': 0.4527986658963601}\n",
      "[CELL 06-27] elapsed=0.08s\n",
      "[CELL 06-27] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-27] GRU4Rec evaluation on GLOBAL test pairs (mars_pairs_test_ts)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-27\", \"GRU4Rec eval: global test pairs\")\n",
    "\n",
    "import duckdb\n",
    "con_ro = duckdb.connect(str(DUCKDB_PATH), read_only=True)\n",
    "\n",
    "pairs_test_ts = con_ro.execute(\"\"\"\n",
    "SELECT CAST(session_id AS VARCHAR) AS session_id,\n",
    "       CAST(tpos AS INTEGER) AS tpos,\n",
    "       CAST(label AS INTEGER) AS label\n",
    "FROM mars_pairs_test_ts\n",
    "ORDER BY session_id, tpos\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "con_ro.close()\n",
    "\n",
    "print(\"[CELL 06-27] pairs_test_ts shape:\", pairs_test_ts.shape)\n",
    "\n",
    "cutoffs = list(map(int, CFG[\"eval\"][\"cutoffs\"]))\n",
    "res_gru_global = eval_query_points_gru(pairs_test_ts, sess_items_idx, cutoffs, batch_size=256)\n",
    "\n",
    "print(\"[CELL 06-27] GRU4REC GLOBAL_TEST metrics:\", res_gru_global)\n",
    "\n",
    "report = read_json(REPORT_PATH)\n",
    "report[\"metrics\"][\"gru4rec_global_test\"] = res_gru_global\n",
    "report[\"key_findings\"].append(\"Computed GRU4Rec baseline on global test pairs (mars_pairs_test_ts).\")\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "cell_end(\"CELL 06-27\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766cde31",
   "metadata": {},
   "source": [
    "Evaluate GRU4Rec on EPISODIC test query (q_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58aac20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-28] GRU4Rec eval: episodic test query\n",
      "[CELL 06-28] start=2026-01-06T23:44:24\n",
      "[CELL 06-28] GRU4REC EPISODE_TEST(query) metrics: {'n': 40, 'HR@5': 0.825, 'MRR@5': 0.7666666666666667, 'NDCG@5': 0.7815464876785729, 'HR@10': 0.825, 'MRR@10': 0.7666666666666667, 'NDCG@10': 0.7815464876785729, 'HR@20': 0.825, 'MRR@20': 0.7666666666666667, 'NDCG@20': 0.7815464876785729}\n",
      "[CELL 06-28] elapsed=0.06s\n",
      "[CELL 06-28] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-28] GRU4Rec evaluation on EPISODIC test query points (q_eval)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-28\", \"GRU4Rec eval: episodic test query\")\n",
    "\n",
    "# q_eval was created in 06-16 for Session-KNN episodic evaluation.\n",
    "need_cols = {\"session_id\",\"tpos\",\"label\"}\n",
    "if not need_cols.issubset(set(q_eval.columns)):\n",
    "    raise RuntimeError(f\"q_eval missing required columns: {need_cols - set(q_eval.columns)}\")\n",
    "\n",
    "cutoffs = list(map(int, CFG[\"eval\"][\"cutoffs\"]))\n",
    "res_gru_ep = eval_query_points_gru(q_eval[[\"session_id\",\"tpos\",\"label\"]].copy(), sess_items_idx, cutoffs, batch_size=256)\n",
    "\n",
    "print(\"[CELL 06-28] GRU4REC EPISODE_TEST(query) metrics:\", res_gru_ep)\n",
    "\n",
    "report = read_json(REPORT_PATH)\n",
    "report[\"metrics\"][\"gru4rec_episode_test_query\"] = res_gru_ep\n",
    "report[\"key_findings\"].append(\"Computed GRU4Rec baseline on episodic test query points (same episode protocol).\")\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "cell_end(\"CELL 06-28\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c2550",
   "metadata": {},
   "source": [
    "The fact that GRU4Rec episodic HR@5 = 0.825 strongly confirms the episodic labels are valid in the item_idx space, so Session-KNN episodic must be recomputed in the same space. Your current report captures the (incorrect) coverage conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d4c01",
   "metadata": {},
   "source": [
    "Targeted fix (no refactor): re-run Session-KNN episodic in item_idx space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d37efe0",
   "metadata": {},
   "source": [
    "Build Session-KNN train memory in item_idx space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5392517f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-29] Build Session-KNN memory in item_idx space\n",
      "[CELL 06-29] start=2026-01-06T23:48:13\n",
      "[CELL 06-29] train sessions used: 450\n",
      "[CELL 06-29] elapsed=0.01s\n",
      "[CELL 06-29] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-29] Session-KNN memory in item_idx space (to match pairs/episodes labels)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-29\", \"Build Session-KNN memory in item_idx space\")\n",
    "\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# Build train sequences in item_idx space from TRAIN session_ids\n",
    "train_sess_ids = sessions.loc[sessions[\"split\"] == \"train\", \"session_id\"].astype(str).unique().tolist()\n",
    "\n",
    "train_items_idx = []\n",
    "for sid in train_sess_ids:\n",
    "    seq = sess_items_idx.get(str(sid))  # sess_items_idx built in GRU cell 06-22\n",
    "    if seq is None or len(seq) < 2:\n",
    "        continue\n",
    "    # use unique for similarity base\n",
    "    train_items_idx.append([int(x) for x in seq])\n",
    "\n",
    "print(\"[CELL 06-29] train sessions used:\", len(train_items_idx))\n",
    "\n",
    "# inverted index in item_idx space\n",
    "inv_idx = defaultdict(list)\n",
    "for si, items in enumerate(train_items_idx):\n",
    "    for it in set(items):\n",
    "        inv_idx[int(it)].append(si)\n",
    "\n",
    "def session_sim_idx(a_set, b_set) -> float:\n",
    "    inter = len(a_set & b_set)\n",
    "    if inter == 0:\n",
    "        return 0.0\n",
    "    return float(inter) / math.sqrt(float(len(a_set)) * float(len(b_set)))\n",
    "\n",
    "KNN_K = 100\n",
    "cutoffs = list(map(int, CFG[\"eval\"][\"cutoffs\"]))\n",
    "maxK = max(cutoffs)\n",
    "\n",
    "def recommend_next_idx(prefix_items_idx, exclude_set, topn=maxK):\n",
    "    a_set = set(prefix_items_idx)\n",
    "    if not a_set:\n",
    "        return []\n",
    "\n",
    "    cand_sessions = set()\n",
    "    for it in a_set:\n",
    "        cand_sessions.update(inv_idx.get(int(it), []))\n",
    "    if not cand_sessions:\n",
    "        return []\n",
    "\n",
    "    sims = []\n",
    "    for si in cand_sessions:\n",
    "        b_set = set(train_items_idx[si])\n",
    "        s = session_sim_idx(a_set, b_set)\n",
    "        if s > 0:\n",
    "            sims.append((si, s))\n",
    "    if not sims:\n",
    "        return []\n",
    "\n",
    "    sims.sort(key=lambda x: x[1], reverse=True)\n",
    "    sims = sims[:KNN_K]\n",
    "\n",
    "    score = defaultdict(float)\n",
    "    for si, s in sims:\n",
    "        for it in train_items_idx[si]:\n",
    "            it = int(it)\n",
    "            if it in exclude_set:\n",
    "                continue\n",
    "            score[it] += s\n",
    "\n",
    "    if not score:\n",
    "        return []\n",
    "\n",
    "    ranked = sorted(score.items(), key=lambda x: (-x[1], x[0]))\n",
    "    return [it for it, _ in ranked[:topn]]\n",
    "\n",
    "cell_end(\"CELL 06-29\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9447c74e",
   "metadata": {},
   "source": [
    "Re-evaluate Session-KNN on episodic test query (correct space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "667d502a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-30] Session-KNN episodic test eval (item_idx space)\n",
      "[CELL 06-30] start=2026-01-06T23:48:38\n",
      "[CELL 06-30] SESSION_KNN EPISODE_TEST(query) metrics (FIXED): {'n': 40, 'HR@5': 0.6, 'MRR@5': 0.25625, 'NDCG@5': 0.3414700967349292, 'HR@10': 0.8, 'MRR@10': 0.2829761904761905, 'NDCG@10': 0.4061736079121778, 'HR@20': 0.85, 'MRR@20': 0.28714285714285714, 'NDCG@20': 0.41968551563354384}\n",
      "[CELL 06-30] elapsed=0.01s\n",
      "[CELL 06-30] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-30] Session-KNN episodic test eval (query-only) in item_idx space ✅\n",
    "\n",
    "t0 = cell_start(\"CELL 06-30\", \"Session-KNN episodic test eval (item_idx space)\")\n",
    "\n",
    "import math\n",
    "\n",
    "hr = {K: 0.0 for K in cutoffs}\n",
    "mrr = {K: 0.0 for K in cutoffs}\n",
    "ndcg = {K: 0.0 for K in cutoffs}\n",
    "n = 0\n",
    "\n",
    "for row in q_eval.itertuples(index=False):\n",
    "    sid = str(row.session_id)\n",
    "    tpos = int(row.tpos)           # 1-indexed\n",
    "    true = int(row.label)          # item_idx (vocab)\n",
    "\n",
    "    seq = sess_items_idx.get(sid)  # item_idx sequence\n",
    "    if seq is None:\n",
    "        continue\n",
    "\n",
    "    pref_len = max(tpos - 1, 0)\n",
    "    prefix = seq[:pref_len]\n",
    "    if len(prefix) == 0:\n",
    "        continue\n",
    "\n",
    "    recs = recommend_next_idx(prefix, exclude_set=set(prefix), topn=maxK)\n",
    "    n += 1\n",
    "\n",
    "    if true in recs:\n",
    "        r = recs.index(true) + 1\n",
    "    else:\n",
    "        r = 10**9\n",
    "\n",
    "    for K in cutoffs:\n",
    "        if r <= K:\n",
    "            hr[K] += 1.0\n",
    "            mrr[K] += 1.0 / float(r)\n",
    "            ndcg[K] += 1.0 / math.log2(float(r) + 1.0)\n",
    "\n",
    "out = {\"n\": int(n)}\n",
    "den = float(n) if n else 1.0\n",
    "for K in cutoffs:\n",
    "    out[f\"HR@{K}\"] = hr[K] / den\n",
    "    out[f\"MRR@{K}\"] = mrr[K] / den\n",
    "    out[f\"NDCG@{K}\"] = ndcg[K] / den\n",
    "\n",
    "print(\"[CELL 06-30] SESSION_KNN EPISODE_TEST(query) metrics (FIXED):\", out)\n",
    "\n",
    "report = read_json(REPORT_PATH)\n",
    "report[\"metrics\"][\"session_knn_episode_test_query_itemidx_fixed\"] = out\n",
    "report[\"key_findings\"].append(\"FIX: Recomputed Session-KNN episodic test in item_idx space to match episode labels.\")\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "cell_end(\"CELL 06-30\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461a4aac",
   "metadata": {},
   "source": [
    "Fix the coverage note (item_idx coverage, not raw item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08fd1e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-31] Coverage check in item_idx space (correct)\n",
      "[CELL 06-31] start=2026-01-06T23:49:03\n",
      "[CELL 06-31] coverage_note_itemidx: {'episode_test_query_n': 40, 'episode_test_query_true_not_in_train_itemidx': 0, 'coverage_rate_itemidx': 1.0, 'note': 'This coverage is computed in item_idx (vocab) space; previous raw item_id comparison was invalid.'}\n",
      "[CELL 06-31] elapsed=0.02s\n",
      "[CELL 06-31] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-31] Correct coverage analysis in item_idx space\n",
    "\n",
    "t0 = cell_start(\"CELL 06-31\", \"Coverage check in item_idx space (correct)\")\n",
    "\n",
    "train_item_idx_set = set()\n",
    "for seq in train_items_idx:\n",
    "    for it in seq:\n",
    "        train_item_idx_set.add(int(it))\n",
    "\n",
    "labels = q_eval[\"label\"].astype(int).tolist()\n",
    "n_all = len(labels)\n",
    "n_not_in_train = sum([1 for y in labels if int(y) not in train_item_idx_set])\n",
    "cov = 1.0 - (n_not_in_train / max(n_all, 1))\n",
    "\n",
    "cov_note = {\n",
    "    \"episode_test_query_n\": int(n_all),\n",
    "    \"episode_test_query_true_not_in_train_itemidx\": int(n_not_in_train),\n",
    "    \"coverage_rate_itemidx\": float(cov),\n",
    "    \"note\": \"This coverage is computed in item_idx (vocab) space; previous raw item_id comparison was invalid.\"\n",
    "}\n",
    "\n",
    "print(\"[CELL 06-31] coverage_note_itemidx:\", cov_note)\n",
    "\n",
    "report = read_json(REPORT_PATH)\n",
    "report[\"sanity_samples\"][\"episode_test_item_coverage_itemidx\"] = cov_note\n",
    "report[\"key_findings\"].append(\"FIX: Coverage recomputed in item_idx space; previous 0% coverage note was due to mismatched ID spaces.\")\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "cell_end(\"CELL 06-31\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e600cc",
   "metadata": {},
   "source": [
    "Write a short baseline summary into report.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9ba9c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-32] Write baseline summary + key notes\n",
      "[CELL 06-32] start=2026-01-06T23:49:56\n",
      "[CELL 06-32] updated: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\report.json\n",
      "[CELL 06-32] elapsed=0.02s\n",
      "[CELL 06-32] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-32] Write baseline summary + correction note into report\n",
    "\n",
    "t0 = cell_start(\"CELL 06-32\", \"Write baseline summary + key notes\")\n",
    "\n",
    "report = read_json(REPORT_PATH)\n",
    "\n",
    "# Keep the corrected narrative explicit\n",
    "report[\"notes\"].append(\n",
    "    \"NOTE: Session-KNN episodic evaluation initially returned 0 due to mismatched ID spaces \"\n",
    "    \"(raw item_id vs vocab item_idx). We recomputed episodic Session-KNN in item_idx space and \"\n",
    "    \"recomputed coverage accordingly (coverage=100%).\"\n",
    ")\n",
    "\n",
    "# Add a concise summary block\n",
    "summary = {\n",
    "    \"global_test_n\": int(report[\"metrics\"][\"popularity_global_test\"][\"n\"]),\n",
    "    \"episode_test_query_n\": int(report[\"metrics\"][\"popularity_episode_test_query\"][\"n\"]),\n",
    "    \"global_test\": {\n",
    "        \"popularity\": report[\"metrics\"][\"popularity_global_test\"],\n",
    "        \"session_knn\": report[\"metrics\"][\"session_knn_global_test\"],\n",
    "        \"gru4rec\": report[\"metrics\"][\"gru4rec_global_test\"],\n",
    "    },\n",
    "    \"episode_test_query\": {\n",
    "        \"popularity\": report[\"metrics\"][\"popularity_episode_test_query\"],\n",
    "        \"session_knn_fixed_itemidx\": report[\"metrics\"][\"session_knn_episode_test_query_itemidx_fixed\"],\n",
    "        \"gru4rec\": report[\"metrics\"][\"gru4rec_episode_test_query\"],\n",
    "    },\n",
    "    \"coverage_itemidx\": report[\"sanity_samples\"][\"episode_test_item_coverage_itemidx\"],\n",
    "}\n",
    "\n",
    "report[\"sanity_samples\"][\"baseline_summary\"] = summary\n",
    "report[\"key_findings\"].append(\"Notebook 06 complete: popularity, Session-KNN, GRU4Rec baselines reported for global + episodic test.\")\n",
    "\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "print(\"[CELL 06-32] updated:\", REPORT_PATH)\n",
    "\n",
    "cell_end(\"CELL 06-32\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b21947d",
   "metadata": {},
   "source": [
    "Update manifest (include model + plots if exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c224e0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-33] Write manifest artifacts\n",
      "[CELL 06-33] start=2026-01-06T23:50:18\n",
      "[CELL 06-33] updated: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\manifest.json artifacts: 8\n",
      "[CELL 06-33] elapsed=0.03s\n",
      "[CELL 06-33] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-33] Update manifest with key artifacts (safe hashing)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-33\", \"Write manifest artifacts\")\n",
    "\n",
    "manifest = read_json(MANIFEST_PATH)\n",
    "\n",
    "# always include these\n",
    "paths = [\n",
    "    Path(CONFIG_PATH),\n",
    "    Path(REPORT_PATH),\n",
    "    Path(MANIFEST_PATH),\n",
    "    OUT_DIR / \"popularity_rank_top.json\",\n",
    "]\n",
    "\n",
    "# include GRU best model if exists\n",
    "best_path = OUT_DIR / \"models\" / \"gru4rec_best.pt\"\n",
    "if best_path.exists():\n",
    "    paths.append(best_path)\n",
    "\n",
    "# add any plots folder files if you created plots elsewhere\n",
    "plots_dir = OUT_DIR / \"plots\"\n",
    "if plots_dir.exists():\n",
    "    for p in sorted(plots_dir.glob(\"*\")):\n",
    "        if p.is_file():\n",
    "            paths.append(p)\n",
    "\n",
    "# de-dup and record\n",
    "seen = set()\n",
    "for p in paths:\n",
    "    p = Path(p)\n",
    "    if p.exists() and str(p) not in seen:\n",
    "        manifest[\"artifacts\"].append(safe_artifact_record(p))\n",
    "        seen.add(str(p))\n",
    "\n",
    "write_json_atomic(MANIFEST_PATH, manifest)\n",
    "print(\"[CELL 06-33] updated:\", MANIFEST_PATH, \"artifacts:\", len(manifest[\"artifacts\"]))\n",
    "\n",
    "cell_end(\"CELL 06-33\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42d751e",
   "metadata": {},
   "source": [
    "SASRec - Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862ef801",
   "metadata": {},
   "source": [
    "SASRec setup + dataloader (PAD = n_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee79220b",
   "metadata": {},
   "source": [
    "This avoids the padding-token collision problem by using pad_id = n_items and embedding size n_items+1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f483955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-34] SASRec setup + dataloader\n",
      "[CELL 06-34] start=2026-01-06T23:55:22\n",
      "[CELL 06-34] pad_id: 776 n_items_pad: 777\n",
      "[CELL 06-34] SAS_CFG: {'max_len': 50, 'emb_dim': 64, 'n_heads': 2, 'n_layers': 2, 'dropout': 0.1, 'lr': 0.001, 'batch': 256, 'epochs': 10}\n",
      "[CELL 06-34] elapsed=0.00s\n",
      "[CELL 06-34] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-34] SASRec: setup + dataloader (pad_id = n_items)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-34\", \"SASRec setup + dataloader\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "pad_id = int(n_items)          # padding token is outside real item range [0..n_items-1]\n",
    "n_items_pad = int(n_items + 1)\n",
    "\n",
    "SAS_CFG = {\n",
    "    \"max_len\": 50,\n",
    "    \"emb_dim\": 64,\n",
    "    \"n_heads\": 2,\n",
    "    \"n_layers\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"lr\": 1e-3,\n",
    "    \"batch\": 256,\n",
    "    \"epochs\": 10,\n",
    "}\n",
    "\n",
    "print(\"[CELL 06-34] pad_id:\", pad_id, \"n_items_pad:\", n_items_pad)\n",
    "print(\"[CELL 06-34] SAS_CFG:\", SAS_CFG)\n",
    "\n",
    "class PrefixDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], int(self.y[i])\n",
    "\n",
    "def collate_pad_sas(batch, max_len: int, pad_id: int):\n",
    "    seqs, ys = zip(*batch)\n",
    "\n",
    "    # truncate from the left to keep most recent interactions\n",
    "    seqs2 = []\n",
    "    lens = []\n",
    "    for s in seqs:\n",
    "        s = list(s)\n",
    "        if len(s) > max_len:\n",
    "            s = s[-max_len:]\n",
    "        seqs2.append(s)\n",
    "        lens.append(len(s))\n",
    "\n",
    "    lens = torch.tensor(lens, dtype=torch.long)\n",
    "    T = int(lens.max().item()) if len(seqs2) else 1\n",
    "    xpad = torch.full((len(seqs2), T), fill_value=pad_id, dtype=torch.long)\n",
    "    for i, s in enumerate(seqs2):\n",
    "        xpad[i, :len(s)] = torch.tensor(s, dtype=torch.long)\n",
    "\n",
    "    y = torch.tensor(ys, dtype=torch.long)\n",
    "    return xpad, lens, y\n",
    "\n",
    "def make_loader(X, y, shuffle: bool):\n",
    "    return DataLoader(\n",
    "        PrefixDataset(X, y),\n",
    "        batch_size=int(SAS_CFG[\"batch\"]),\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=lambda b: collate_pad_sas(b, max_len=int(SAS_CFG[\"max_len\"]), pad_id=pad_id)\n",
    "    )\n",
    "\n",
    "sas_train_loader = make_loader(X_train, y_train, shuffle=True)\n",
    "sas_val_loader   = make_loader(X_val, y_val, shuffle=False) if len(X_val) else None\n",
    "\n",
    "cell_end(\"CELL 06-34\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f48128",
   "metadata": {},
   "source": [
    "SASRec model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f441d5",
   "metadata": {},
   "source": [
    "Causal self-attention + last-position representation → predict next item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1713bf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-35] Define SASRec model\n",
      "[CELL 06-35] start=2026-01-06T23:55:54\n",
      "[CELL 06-35] sas_model params: 203336\n",
      "[CELL 06-35] elapsed=0.03s\n",
      "[CELL 06-35] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-35] SASRec model (Transformer encoder, causal mask)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-35\", \"Define SASRec model\")\n",
    "\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(self, n_items_pad: int, n_items: int, max_len: int, emb_dim: int, n_heads: int, n_layers: int, dropout: float, pad_id: int):\n",
    "        super().__init__()\n",
    "        self.n_items = int(n_items)\n",
    "        self.pad_id = int(pad_id)\n",
    "        self.max_len = int(max_len)\n",
    "\n",
    "        self.item_emb = nn.Embedding(n_items_pad, emb_dim)     # includes pad_id\n",
    "        self.pos_emb  = nn.Embedding(max_len, emb_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=emb_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.out = nn.Linear(emb_dim, self.n_items)  # predict only real items [0..n_items-1]\n",
    "\n",
    "    def forward(self, xpad: torch.Tensor, lens: torch.Tensor):\n",
    "        \"\"\"\n",
    "        xpad: [B,T] tokens in [0..n_items-1] or pad_id\n",
    "        lens: [B] true lengths\n",
    "        \"\"\"\n",
    "        B, T = xpad.shape\n",
    "        xpad = xpad.clamp(min=0, max=self.pad_id)  # safety\n",
    "\n",
    "        # positions 0..T-1\n",
    "        pos = torch.arange(T, device=xpad.device).unsqueeze(0).expand(B, T).clamp(max=self.max_len-1)\n",
    "\n",
    "        x = self.item_emb(xpad) + self.pos_emb(pos)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        # padding mask: True where PAD\n",
    "        key_padding_mask = (xpad == self.pad_id)  # [B,T]\n",
    "\n",
    "        # causal mask: prevent attending to future positions\n",
    "        # TransformerEncoder expects mask shape [T,T]\n",
    "        causal_mask = torch.triu(torch.ones((T, T), device=xpad.device), diagonal=1).bool()\n",
    "\n",
    "        h = self.encoder(x, mask=causal_mask, src_key_padding_mask=key_padding_mask)  # [B,T,E]\n",
    "\n",
    "        # take last real position per sample: index = lens-1\n",
    "        idx = (lens - 1).clamp(min=0).view(B, 1, 1).expand(B, 1, h.size(-1))\n",
    "        h_last = h.gather(1, idx).squeeze(1)  # [B,E]\n",
    "\n",
    "        logits = self.out(h_last)  # [B,n_items]\n",
    "        return logits\n",
    "\n",
    "sas_model = SASRec(\n",
    "    n_items_pad=n_items_pad,\n",
    "    n_items=n_items,\n",
    "    max_len=int(SAS_CFG[\"max_len\"]),\n",
    "    emb_dim=int(SAS_CFG[\"emb_dim\"]),\n",
    "    n_heads=int(SAS_CFG[\"n_heads\"]),\n",
    "    n_layers=int(SAS_CFG[\"n_layers\"]),\n",
    "    dropout=float(SAS_CFG[\"dropout\"]),\n",
    "    pad_id=pad_id,\n",
    ").to(device)\n",
    "\n",
    "sas_opt = torch.optim.Adam(sas_model.parameters(), lr=float(SAS_CFG[\"lr\"]))\n",
    "\n",
    "print(\"[CELL 06-35] sas_model params:\", sum(p.numel() for p in sas_model.parameters()))\n",
    "\n",
    "cell_end(\"CELL 06-35\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd1b4b7",
   "metadata": {},
   "source": [
    "Train SASRec (train/val loss + save best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa576a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-36] Train SASRec\n",
      "[CELL 06-36] start=2026-01-06T23:56:19\n",
      "[CELL 06-36] epoch=1/10 train_loss=6.7319 val_loss=6.5141\n",
      "[CELL 06-36] saved best: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\models\\sasrec_best.pt\n",
      "[CELL 06-36] epoch=2/10 train_loss=6.3980 val_loss=6.3089\n",
      "[CELL 06-36] saved best: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\models\\sasrec_best.pt\n",
      "[CELL 06-36] epoch=3/10 train_loss=6.1446 val_loss=6.1517\n",
      "[CELL 06-36] saved best: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\models\\sasrec_best.pt\n",
      "[CELL 06-36] epoch=4/10 train_loss=5.9135 val_loss=6.0114\n",
      "[CELL 06-36] saved best: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\models\\sasrec_best.pt\n",
      "[CELL 06-36] epoch=5/10 train_loss=5.6959 val_loss=5.8681\n",
      "[CELL 06-36] saved best: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\models\\sasrec_best.pt\n",
      "[CELL 06-36] epoch=6/10 train_loss=5.4856 val_loss=5.7455\n",
      "[CELL 06-36] saved best: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\models\\sasrec_best.pt\n",
      "[CELL 06-36] epoch=7/10 train_loss=5.2922 val_loss=5.6269\n",
      "[CELL 06-36] saved best: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\models\\sasrec_best.pt\n",
      "[CELL 06-36] epoch=8/10 train_loss=5.0963 val_loss=5.5111\n",
      "[CELL 06-36] saved best: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\models\\sasrec_best.pt\n",
      "[CELL 06-36] epoch=9/10 train_loss=4.9139 val_loss=5.4051\n",
      "[CELL 06-36] saved best: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\models\\sasrec_best.pt\n",
      "[CELL 06-36] epoch=10/10 train_loss=4.7243 val_loss=5.2908\n",
      "[CELL 06-36] saved best: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\models\\sasrec_best.pt\n",
      "[CELL 06-36] loaded best weights: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\models\\sasrec_best.pt\n",
      "[CELL 06-36] sas_best_val=5.290814399719238\n",
      "[CELL 06-36] elapsed=2.54s\n",
      "[CELL 06-36] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-36] Train SASRec (log train/val loss)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-36\", \"Train SASRec\")\n",
    "\n",
    "sas_best = float(\"inf\")\n",
    "sas_best_path = OUT_DIR / \"models\" / \"sasrec_best.pt\"\n",
    "sas_best_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def sas_run_epoch(loader, train: bool):\n",
    "    sas_model.train(train)\n",
    "    total = 0.0\n",
    "    n = 0\n",
    "    for xpad, lens, y in loader:\n",
    "        xpad = xpad.to(device)\n",
    "        lens = lens.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits = sas_model(xpad, lens)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        if train:\n",
    "            sas_opt.zero_grad()\n",
    "            loss.backward()\n",
    "            sas_opt.step()\n",
    "\n",
    "        total += float(loss.item()) * int(y.shape[0])\n",
    "        n += int(y.shape[0])\n",
    "    return total / max(n, 1)\n",
    "\n",
    "for ep in range(1, int(SAS_CFG[\"epochs\"]) + 1):\n",
    "    tr = sas_run_epoch(sas_train_loader, train=True)\n",
    "    va = sas_run_epoch(sas_val_loader, train=False) if sas_val_loader is not None else float(\"nan\")\n",
    "    print(f\"[CELL 06-36] epoch={ep}/{SAS_CFG['epochs']} train_loss={tr:.4f} val_loss={va:.4f}\")\n",
    "\n",
    "    if sas_val_loader is not None and va < sas_best:\n",
    "        sas_best = va\n",
    "        torch.save(sas_model.state_dict(), sas_best_path)\n",
    "        print(\"[CELL 06-36] saved best:\", sas_best_path)\n",
    "\n",
    "# load best if exists\n",
    "if sas_best_path.exists():\n",
    "    sas_model.load_state_dict(torch.load(sas_best_path, map_location=device))\n",
    "    print(\"[CELL 06-36] loaded best weights:\", sas_best_path)\n",
    "\n",
    "# save config in report\n",
    "report = read_json(REPORT_PATH)\n",
    "report[\"sanity_samples\"][\"sasrec_config\"] = SAS_CFG\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "cell_end(\"CELL 06-36\", t0, sas_best_val=sas_best if sas_val_loader is not None else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94c452c",
   "metadata": {},
   "source": [
    "Evaluate SASRec on GLOBAL test pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf431b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-37] SASRec eval: global test pairs\n",
      "[CELL 06-37] start=2026-01-06T23:56:42\n",
      "[CELL 06-37] SASREC GLOBAL_TEST metrics: {'n': 214, 'HR@5': 0.4158878504672897, 'MRR@5': 0.3349688473520249, 'NDCG@5': 0.3553619820236781, 'HR@10': 0.4532710280373832, 'MRR@10': 0.33955644563121207, 'NDCG@10': 0.3670473087431946, 'HR@20': 0.48130841121495327, 'MRR@20': 0.3415606479068547, 'NDCG@20': 0.3742040961819287}\n",
      "[CELL 06-37] elapsed=0.14s\n",
      "[CELL 06-37] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-37] SASRec evaluation: global test pairs (mars_pairs_test_ts)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-37\", \"SASRec eval: global test pairs\")\n",
    "\n",
    "def sas_topk(prefix_list_batch, topk: int):\n",
    "    if len(prefix_list_batch) == 0:\n",
    "        return []\n",
    "    # truncate to max_len from left\n",
    "    seqs = []\n",
    "    lens = []\n",
    "    for s in prefix_list_batch:\n",
    "        s = list(s)\n",
    "        if len(s) > int(SAS_CFG[\"max_len\"]):\n",
    "            s = s[-int(SAS_CFG[\"max_len\"]):]\n",
    "        seqs.append(s)\n",
    "        lens.append(len(s))\n",
    "    lens = torch.tensor(lens, dtype=torch.long)\n",
    "\n",
    "    T = int(lens.max().item()) if len(seqs) else 1\n",
    "    xpad = torch.full((len(seqs), T), fill_value=pad_id, dtype=torch.long)\n",
    "    for i, s in enumerate(seqs):\n",
    "        xpad[i, :len(s)] = torch.tensor(s, dtype=torch.long)\n",
    "\n",
    "    xpad = xpad.to(device)\n",
    "    lens = lens.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = sas_model(xpad, lens)  # [B,n_items]\n",
    "        _, idx = torch.topk(logits, k=topk, dim=1)\n",
    "    return idx.cpu().numpy().tolist()\n",
    "\n",
    "def eval_query_points_sas(df_points: pd.DataFrame, sess_items_idx: dict, cutoffs: list[int], batch_size: int = 256):\n",
    "    import math\n",
    "    maxK = max(cutoffs)\n",
    "\n",
    "    prefixes, labels = [], []\n",
    "    for r in df_points.itertuples(index=False):\n",
    "        sid = str(r.session_id)\n",
    "        tpos = int(r.tpos)\n",
    "        y = int(r.label)\n",
    "        seq = sess_items_idx.get(sid)\n",
    "        if seq is None:\n",
    "            continue\n",
    "        pref = seq[:max(tpos - 1, 0)]\n",
    "        if len(pref) == 0:\n",
    "            continue\n",
    "        prefixes.append(pref)\n",
    "        labels.append(y)\n",
    "\n",
    "    n = len(labels)\n",
    "    out = {\"n\": int(n)}\n",
    "    if n == 0:\n",
    "        for K in cutoffs:\n",
    "            out[f\"HR@{K}\"] = 0.0\n",
    "            out[f\"MRR@{K}\"] = 0.0\n",
    "            out[f\"NDCG@{K}\"] = 0.0\n",
    "        return out\n",
    "\n",
    "    hr = {K: 0.0 for K in cutoffs}\n",
    "    mrr = {K: 0.0 for K in cutoffs}\n",
    "    ndcg = {K: 0.0 for K in cutoffs}\n",
    "\n",
    "    for i in range(0, n, batch_size):\n",
    "        pbatch = prefixes[i:i+batch_size]\n",
    "        ybatch = labels[i:i+batch_size]\n",
    "        top = sas_topk(pbatch, topk=maxK)\n",
    "\n",
    "        for recs, y in zip(top, ybatch):\n",
    "            if y in recs:\n",
    "                rnk = recs.index(y) + 1\n",
    "            else:\n",
    "                rnk = 10**9\n",
    "            for K in cutoffs:\n",
    "                if rnk <= K:\n",
    "                    hr[K] += 1.0\n",
    "                    mrr[K] += 1.0 / float(rnk)\n",
    "                    ndcg[K] += 1.0 / math.log2(float(rnk) + 1.0)\n",
    "\n",
    "    den = float(n)\n",
    "    for K in cutoffs:\n",
    "        out[f\"HR@{K}\"] = hr[K] / den\n",
    "        out[f\"MRR@{K}\"] = mrr[K] / den\n",
    "        out[f\"NDCG@{K}\"] = ndcg[K] / den\n",
    "    return out\n",
    "\n",
    "import duckdb\n",
    "con_ro = duckdb.connect(str(DUCKDB_PATH), read_only=True)\n",
    "pairs_test_ts = con_ro.execute(\"\"\"\n",
    "SELECT CAST(session_id AS VARCHAR) AS session_id,\n",
    "       CAST(tpos AS INTEGER) AS tpos,\n",
    "       CAST(label AS INTEGER) AS label\n",
    "FROM mars_pairs_test_ts\n",
    "ORDER BY session_id, tpos\n",
    "\"\"\").fetchdf()\n",
    "con_ro.close()\n",
    "\n",
    "cutoffs = list(map(int, CFG[\"eval\"][\"cutoffs\"]))\n",
    "res_sas_global = eval_query_points_sas(pairs_test_ts, sess_items_idx, cutoffs, batch_size=256)\n",
    "\n",
    "print(\"[CELL 06-37] SASREC GLOBAL_TEST metrics:\", res_sas_global)\n",
    "\n",
    "report = read_json(REPORT_PATH)\n",
    "report[\"metrics\"][\"sasrec_global_test\"] = res_sas_global\n",
    "report[\"key_findings\"].append(\"Computed SASRec baseline on global test pairs (mars_pairs_test_ts).\")\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "cell_end(\"CELL 06-37\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4495faa",
   "metadata": {},
   "source": [
    "Evaluate SASRec on EPISODIC test query (q_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "baf45992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-38] SASRec eval: episodic test query\n",
      "[CELL 06-38] start=2026-01-06T23:57:03\n",
      "[CELL 06-38] SASREC EPISODE_TEST(query) metrics: {'n': 40, 'HR@5': 0.425, 'MRR@5': 0.26, 'NDCG@5': 0.30148210339744574, 'HR@10': 0.625, 'MRR@10': 0.2858333333333333, 'NDCG@10': 0.36529219174731414, 'HR@20': 0.625, 'MRR@20': 0.2858333333333333, 'NDCG@20': 0.36529219174731414}\n",
      "[CELL 06-38] elapsed=0.08s\n",
      "[CELL 06-38] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-38] SASRec evaluation: episodic test query (q_eval)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-38\", \"SASRec eval: episodic test query\")\n",
    "\n",
    "need_cols = {\"session_id\",\"tpos\",\"label\"}\n",
    "if not need_cols.issubset(set(q_eval.columns)):\n",
    "    raise RuntimeError(f\"q_eval missing required columns: {need_cols - set(q_eval.columns)}\")\n",
    "\n",
    "cutoffs = list(map(int, CFG[\"eval\"][\"cutoffs\"]))\n",
    "res_sas_ep = eval_query_points_sas(q_eval[[\"session_id\",\"tpos\",\"label\"]].copy(), sess_items_idx, cutoffs, batch_size=256)\n",
    "\n",
    "print(\"[CELL 06-38] SASREC EPISODE_TEST(query) metrics:\", res_sas_ep)\n",
    "\n",
    "report = read_json(REPORT_PATH)\n",
    "report[\"metrics\"][\"sasrec_episode_test_query\"] = res_sas_ep\n",
    "report[\"key_findings\"].append(\"Computed SASRec baseline on episodic test query points.\")\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "cell_end(\"CELL 06-38\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce09c4a",
   "metadata": {},
   "source": [
    "Manifest update (include SASRec model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0777b6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-39] Manifest update (SASRec)\n",
      "[CELL 06-39] start=2026-01-06T23:57:20\n",
      "[CELL 06-39] updated: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\manifest.json artifacts: 14\n",
      "[CELL 06-39] elapsed=0.03s\n",
      "[CELL 06-39] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-39] Update manifest to include SASRec artifacts\n",
    "\n",
    "t0 = cell_start(\"CELL 06-39\", \"Manifest update (SASRec)\")\n",
    "\n",
    "manifest = read_json(MANIFEST_PATH)\n",
    "\n",
    "paths = [\n",
    "    Path(CONFIG_PATH),\n",
    "    Path(REPORT_PATH),\n",
    "    Path(MANIFEST_PATH),\n",
    "    OUT_DIR / \"popularity_rank_top.json\",\n",
    "    OUT_DIR / \"models\" / \"gru4rec_best.pt\",\n",
    "]\n",
    "\n",
    "if sas_best_path.exists():\n",
    "    paths.append(sas_best_path)\n",
    "\n",
    "seen = set()\n",
    "for p in paths:\n",
    "    p = Path(p)\n",
    "    if p.exists() and str(p) not in seen:\n",
    "        manifest[\"artifacts\"].append(safe_artifact_record(p))\n",
    "        seen.add(str(p))\n",
    "\n",
    "write_json_atomic(MANIFEST_PATH, manifest)\n",
    "print(\"[CELL 06-39] updated:\", MANIFEST_PATH, \"artifacts:\", len(manifest[\"artifacts\"]))\n",
    "\n",
    "cell_end(\"CELL 06-39\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1c0d7a",
   "metadata": {},
   "source": [
    "Deprecate the wrong coverage note (keep it but mark as invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7dce353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 06-40] Mark invalid coverage note as deprecated\n",
      "[CELL 06-40] start=2026-01-06T23:59:16\n",
      "[CELL 06-40] updated report: C:\\anonymous-users-mooc-session-meta\\reports\\06_baselines_mars_global\\20260106_234210\\report.json\n",
      "[CELL 06-40] elapsed=0.00s\n",
      "[CELL 06-40] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 06-40] Deprecate invalid coverage note (keep for audit, but mark invalid)\n",
    "\n",
    "t0 = cell_start(\"CELL 06-40\", \"Mark invalid coverage note as deprecated\")\n",
    "\n",
    "report = read_json(REPORT_PATH)\n",
    "\n",
    "# mark old block as deprecated if present\n",
    "if \"episode_test_item_coverage\" in report.get(\"sanity_samples\", {}):\n",
    "    report[\"sanity_samples\"][\"episode_test_item_coverage\"][\"DEPRECATED\"] = True\n",
    "    report[\"sanity_samples\"][\"episode_test_item_coverage\"][\"reason\"] = (\n",
    "        \"INVALID: compared raw item_id space vs item_idx labels. Use episode_test_item_coverage_itemidx instead.\"\n",
    "    )\n",
    "\n",
    "# remove or mark the wrong key_finding (safer: mark it explicitly rather than deleting)\n",
    "kf = report.get(\"key_findings\", [])\n",
    "new_kf = []\n",
    "for s in kf:\n",
    "    if \"0% item coverage\" in s and \"by definition\" in s:\n",
    "        new_kf.append(\"[DEPRECATED] \" + s + \" (invalid due to ID-space mismatch; see item_idx coverage fix)\")\n",
    "    else:\n",
    "        new_kf.append(s)\n",
    "report[\"key_findings\"] = new_kf\n",
    "\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "print(\"[CELL 06-40] updated report:\", REPORT_PATH)\n",
    "\n",
    "cell_end(\"CELL 06-40\", t0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
