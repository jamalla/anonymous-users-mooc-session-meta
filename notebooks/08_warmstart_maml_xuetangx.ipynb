{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 08: Warm-Start MAML with Residual Adaptation (XuetangX) - Contribution 1\n\n**Purpose:** Improve MAML by initializing from pre-trained weights AND using residual adaptation to preserve pre-trained knowledge.\n\n**Baseline:** Vanilla MAML with GRU4Rec base model = 30.52% Acc@1 (from Notebook 07)\n\n**Research Motivation:**\n- Vanilla MAML uses random initialization + only K=5 support pairs\n- This is insufficient for learning good representations from scratch\n- **Problem with naive warm-start**: Meta-training can \"forget\" pre-trained knowledge\n- **Solution**: Residual adaptation - freeze pretrained, only learn delta\n\n**Key Insight (Residual Adaptation):**\n```\nNaive Warm-Start (fails):\n  θ_pretrained → meta-train → θ_meta (overwrites pretrained!)\n\nResidual Warm-Start (proposed):\n  θ_pretrained (FROZEN) + Δθ (learnable) = θ_effective\n  - Only meta-learn Δθ\n  - Inner loop adapts Δθ → Δφ_user\n  - Pre-trained knowledge preserved\n```\n\n**Similar to:** LoRA (Low-Rank Adaptation), Adapter layers, Residual fine-tuning\n\n**Inputs:**\n- `data/processed/xuetangx/episodes/episodes_{train|val|test}_K5_Q10.parquet`\n- `models/baselines/gru_global.pth` (pre-trained GRU4Rec weights)\n- `data/processed/xuetangx/vocab/course2id.json` (1,518 courses)\n\n**Outputs:**\n- `models/contributions/warmstart_residual_maml_K5.pth`\n- `results/warmstart_residual_maml_K5_Q10.json`\n\n**Results:**\n| Model | Acc@1 | vs Baseline |\n|-------|-------|-------------|\n| Vanilla MAML (baseline) | 30.52% | - |\n| Naive Warm-Start MAML | ~24% | -6.52 pp (FAILED) |\n| **Residual Warm-Start MAML** | **34.95%** | **+4.43 pp** |"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 08-00] start=2026-02-03T02:39:49\n",
      "[CELL 08-00] CWD: /workspace/anonymous-users-mooc-session-meta/notebooks\n",
      "[CELL 08-00] REPO_ROOT: /workspace/anonymous-users-mooc-session-meta\n",
      "[CELL 08-00] REPO_ROOT=/workspace/anonymous-users-mooc-session-meta\n",
      "[CELL 08-00] META_REGISTRY=/workspace/anonymous-users-mooc-session-meta/meta.json\n",
      "[CELL 08-00] DATA_INTERIM=/workspace/anonymous-users-mooc-session-meta/data/interim\n",
      "[CELL 08-00] DATA_PROCESSED=/workspace/anonymous-users-mooc-session-meta/data/processed\n",
      "[CELL 08-00] MODELS=/workspace/anonymous-users-mooc-session-meta/models\n",
      "[CELL 08-00] RESULTS=/workspace/anonymous-users-mooc-session-meta/results\n",
      "[CELL 08-00] REPORTS=/workspace/anonymous-users-mooc-session-meta/reports\n",
      "[CELL 08-00] PyTorch device: cuda\n",
      "[CELL 08-00] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-00] Bootstrap: repo root + paths + logger\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import copy\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "t0 = datetime.now()\n",
    "print(f\"[CELL 08-00] start={t0.isoformat(timespec='seconds')}\")\n",
    "print(\"[CELL 08-00] CWD:\", Path.cwd().resolve())\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists():\n",
    "            return p\n",
    "    raise RuntimeError(\"Could not find repo root (no PROJECT_STATE.md)\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "print(f\"[CELL 08-00] REPO_ROOT: {REPO_ROOT}\")\n",
    "\n",
    "sys.path.insert(0, str(REPO_ROOT / \"src\"))\n",
    "\n",
    "META_REGISTRY = REPO_ROOT / \"meta.json\"\n",
    "DATA_INTERIM = REPO_ROOT / \"data\" / \"interim\"\n",
    "DATA_PROCESSED = REPO_ROOT / \"data\" / \"processed\"\n",
    "MODELS = REPO_ROOT / \"models\"\n",
    "RESULTS = REPO_ROOT / \"results\"\n",
    "REPORTS = REPO_ROOT / \"reports\"\n",
    "\n",
    "PATHS = {\n",
    "    \"REPO_ROOT\": REPO_ROOT,\n",
    "    \"META_REGISTRY\": META_REGISTRY,\n",
    "    \"DATA_INTERIM\": DATA_INTERIM,\n",
    "    \"DATA_PROCESSED\": DATA_PROCESSED,\n",
    "    \"MODELS\": MODELS,\n",
    "    \"RESULTS\": RESULTS,\n",
    "    \"REPORTS\": REPORTS,\n",
    "}\n",
    "\n",
    "for name, path in PATHS.items():\n",
    "    print(f\"[CELL 08-00] {name}={path}\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[CELL 08-00] PyTorch device: {DEVICE}\")\n",
    "\n",
    "def cell_start(cell_id: str, description: str = \"\") -> datetime:\n",
    "    t = datetime.now()\n",
    "    msg = f\"[{cell_id}] start={t.isoformat(timespec='seconds')}\"\n",
    "    if description:\n",
    "        msg += f\" | {description}\"\n",
    "    print(msg)\n",
    "    return t\n",
    "\n",
    "def cell_end(cell_id: str, t_start: datetime, **kv) -> None:\n",
    "    elapsed = (datetime.now() - t_start).total_seconds()\n",
    "    for k, v in kv.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    print(f\"[{cell_id}] done in {elapsed:.1f}s\")\n",
    "\n",
    "print(\"[CELL 08-00] done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 08-01] start=2026-02-03T02:39:49 | Configuration\n",
      "[CELL 08-01] RUN_TAG: 20260203_023949_b371f0db\n",
      "[CELL 08-01] Configuration:\n",
      "  - Contribution: warm_start_residual_maml\n",
      "  - Residual Adaptation: True\n",
      "  - Freeze Pretrained: True\n",
      "  - Delta Init Scale: 0.01\n",
      "  - Pre-trained model: models/baselines/gru_global.pth\n",
      "  - Episode: K=5, Q=10\n",
      "  - Model: embed_dim=64, hidden_dim=128\n",
      "  - MAML: inner_lr=0.01, outer_lr=0.0001\n",
      "  - Inner steps: 3\n",
      "  - Meta iterations: 3000\n",
      "[CELL 08-01] done in 0.0s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-01] Configuration\n",
    "\n",
    "t0 = cell_start(\"CELL 08-01\", \"Configuration\")\n",
    "\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + uuid.uuid4().hex[:8]\n",
    "print(f\"[CELL 08-01] RUN_TAG: {RUN_TAG}\")\n",
    "\n",
    "CFG = {\n",
    "    \"notebook\": \"08_warmstart_residual_maml_xuetangx\",\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"dataset\": \"xuetangx\",\n",
    "    \"contribution\": \"warm_start_residual_maml\",\n",
    "    \n",
    "    # Episode config (same as notebook 07)\n",
    "    \"episode_config\": {\n",
    "        \"K\": 5,   # support set size\n",
    "        \"Q\": 10,  # query set size\n",
    "    },\n",
    "    \n",
    "    # Model config (MUST match GRU4Rec baseline from notebook 06)\n",
    "    \"model_config\": {\n",
    "        \"embed_dim\": 64,\n",
    "        \"hidden_dim\": 128,  # MUST match pre-trained GRU4Rec\n",
    "        \"n_layers\": 1,\n",
    "        \"dropout\": 0.1,\n",
    "    },\n",
    "    \n",
    "    # MAML config - UPDATED with lower learning rates\n",
    "    \"maml_config\": {\n",
    "        \"inner_lr\": 0.01,            # alpha: learning rate for inner loop\n",
    "        \"outer_lr\": 0.0001,          # beta: LOWERED from 0.001 to preserve pretrained\n",
    "        \"inner_steps\": 3,            # REDUCED from 5 to be less aggressive\n",
    "        \"meta_batch_size\": 32,       # tasks per meta-batch\n",
    "        \"num_meta_iterations\": 3000,\n",
    "        \"use_second_order\": False,   # FOMAML\n",
    "        \"val_every\": 100,\n",
    "        \"checkpoint_every\": 500,\n",
    "    },\n",
    "    \n",
    "    # Residual Warm-Start config (NEW - KEY CONTRIBUTION)\n",
    "    \"residual_config\": {\n",
    "        \"use_residual\": True,           # Enable residual adaptation\n",
    "        \"freeze_pretrained\": True,      # Freeze pretrained weights\n",
    "        \"delta_init_scale\": 0.01,       # Initialize delta weights small\n",
    "        \"residual_alpha\": 1.0,          # Weight for residual: output = pretrained + alpha * delta\n",
    "    },\n",
    "    \n",
    "    # Warm-Start paths\n",
    "    \"warmstart_config\": {\n",
    "        \"pretrained_path\": \"models/baselines/gru_global.pth\",\n",
    "    },\n",
    "    \n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(CFG[\"seed\"])\n",
    "torch.manual_seed(CFG[\"seed\"])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(CFG[\"seed\"])\n",
    "\n",
    "print(f\"[CELL 08-01] Configuration:\")\n",
    "print(f\"  - Contribution: {CFG['contribution']}\")\n",
    "print(f\"  - Residual Adaptation: {CFG['residual_config']['use_residual']}\")\n",
    "print(f\"  - Freeze Pretrained: {CFG['residual_config']['freeze_pretrained']}\")\n",
    "print(f\"  - Delta Init Scale: {CFG['residual_config']['delta_init_scale']}\")\n",
    "print(f\"  - Pre-trained model: {CFG['warmstart_config']['pretrained_path']}\")\n",
    "print(f\"  - Episode: K={CFG['episode_config']['K']}, Q={CFG['episode_config']['Q']}\")\n",
    "print(f\"  - Model: embed_dim={CFG['model_config']['embed_dim']}, hidden_dim={CFG['model_config']['hidden_dim']}\")\n",
    "print(f\"  - MAML: inner_lr={CFG['maml_config']['inner_lr']}, outer_lr={CFG['maml_config']['outer_lr']}\")\n",
    "print(f\"  - Inner steps: {CFG['maml_config']['inner_steps']}\")\n",
    "print(f\"  - Meta iterations: {CFG['maml_config']['num_meta_iterations']}\")\n",
    "\n",
    "cell_end(\"CELL 08-01\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 08-02] start=2026-02-03T02:39:49 | Setup paths\n",
      "[CELL 08-02] Input episodes: /workspace/anonymous-users-mooc-session-meta/data/processed/xuetangx/episodes/episodes_train_K5_Q10.parquet\n",
      "[CELL 08-02] Pre-trained GRU4Rec: /workspace/anonymous-users-mooc-session-meta/models/baselines/gru_global.pth\n",
      "[CELL 08-02] Pre-trained exists: True\n",
      "[CELL 08-02] Output model: /workspace/anonymous-users-mooc-session-meta/models/contributions/warmstart_maml_K5.pth\n",
      "[CELL 08-02] Output results: /workspace/anonymous-users-mooc-session-meta/results/warmstart_maml_K5_Q10.json\n",
      "[CELL 08-02] done in 0.0s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-02] Setup paths and create directories\n",
    "\n",
    "t0 = cell_start(\"CELL 08-02\", \"Setup paths\")\n",
    "\n",
    "# Input paths\n",
    "EPISODES_DIR = DATA_PROCESSED / \"xuetangx\" / \"episodes\"\n",
    "VOCAB_DIR = DATA_PROCESSED / \"xuetangx\" / \"vocab\"\n",
    "PRETRAINED_PATH = REPO_ROOT / CFG[\"warmstart_config\"][\"pretrained_path\"]\n",
    "\n",
    "K = CFG[\"episode_config\"][\"K\"]\n",
    "Q = CFG[\"episode_config\"][\"Q\"]\n",
    "\n",
    "EPISODES_TRAIN = EPISODES_DIR / f\"episodes_train_K{K}_Q{Q}.parquet\"\n",
    "EPISODES_VAL = EPISODES_DIR / f\"episodes_val_K{K}_Q{Q}.parquet\"\n",
    "EPISODES_TEST = EPISODES_DIR / f\"episodes_test_K{K}_Q{Q}.parquet\"\n",
    "COURSE2ID_PATH = VOCAB_DIR / \"course2id.json\"\n",
    "\n",
    "# Output paths\n",
    "CONTRIB_MODELS_DIR = MODELS / \"contributions\"\n",
    "CONTRIB_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_DIR = CONTRIB_MODELS_DIR / \"checkpoints\"\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REPORT_DIR = REPORTS / \"08_warmstart_maml_xuetangx\" / RUN_TAG\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_MODEL = CONTRIB_MODELS_DIR / \"warmstart_maml_K5.pth\"\n",
    "OUT_RESULTS = RESULTS / \"warmstart_maml_K5_Q10.json\"\n",
    "REPORT_PATH = REPORT_DIR / \"report.json\"\n",
    "\n",
    "print(f\"[CELL 08-02] Input episodes: {EPISODES_TRAIN}\")\n",
    "print(f\"[CELL 08-02] Pre-trained GRU4Rec: {PRETRAINED_PATH}\")\n",
    "print(f\"[CELL 08-02] Pre-trained exists: {PRETRAINED_PATH.exists()}\")\n",
    "print(f\"[CELL 08-02] Output model: {OUT_MODEL}\")\n",
    "print(f\"[CELL 08-02] Output results: {OUT_RESULTS}\")\n",
    "\n",
    "cell_end(\"CELL 08-02\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 08-03] start=2026-02-03T02:39:49 | Load episodes, pairs, and vocabulary\n",
      "[CELL 08-03] Vocabulary size: 1518 courses\n",
      "[CELL 08-03] Train episodes: 47,357\n",
      "[CELL 08-03] Val episodes:   341\n",
      "[CELL 08-03] Test episodes:  313\n",
      "[CELL 08-03] Train pairs: 225,168\n",
      "[CELL 08-03] Val pairs:   28,559\n",
      "[CELL 08-03] Test pairs:  28,252\n",
      "[CELL 08-03] Created pair lookups for train/val/test\n",
      "[CELL 08-03] get_episode_data() helper function defined\n",
      "[CELL 08-03] n_items=1518\n",
      "[CELL 08-03] n_train=47357\n",
      "[CELL 08-03] n_val=341\n",
      "[CELL 08-03] n_test=313\n",
      "[CELL 08-03] done in 4.7s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-03] Load data\n",
    "\n",
    "t0 = cell_start(\"CELL 08-03\", \"Load episodes, pairs, and vocabulary\")\n",
    "\n",
    "# Load vocabulary\n",
    "with open(COURSE2ID_PATH, \"r\") as f:\n",
    "    course2id = json.load(f)\n",
    "n_items = len(course2id)\n",
    "print(f\"[CELL 08-03] Vocabulary size: {n_items} courses\")\n",
    "\n",
    "# Load episodes (contain pair IDs only)\n",
    "episodes_train = pd.read_parquet(EPISODES_TRAIN)\n",
    "episodes_val = pd.read_parquet(EPISODES_VAL)\n",
    "episodes_test = pd.read_parquet(EPISODES_TEST)\n",
    "\n",
    "print(f\"[CELL 08-03] Train episodes: {len(episodes_train):,}\")\n",
    "print(f\"[CELL 08-03] Val episodes:   {len(episodes_val):,}\")\n",
    "print(f\"[CELL 08-03] Test episodes:  {len(episodes_test):,}\")\n",
    "\n",
    "# Load pairs (contain actual prefixes and labels)\n",
    "PAIRS_DIR = DATA_PROCESSED / \"xuetangx\" / \"pairs\"\n",
    "pairs_train = pd.read_parquet(PAIRS_DIR / \"pairs_train.parquet\")\n",
    "pairs_val = pd.read_parquet(PAIRS_DIR / \"pairs_val.parquet\")\n",
    "pairs_test = pd.read_parquet(PAIRS_DIR / \"pairs_test.parquet\")\n",
    "\n",
    "print(f\"[CELL 08-03] Train pairs: {len(pairs_train):,}\")\n",
    "print(f\"[CELL 08-03] Val pairs:   {len(pairs_val):,}\")\n",
    "print(f\"[CELL 08-03] Test pairs:  {len(pairs_test):,}\")\n",
    "\n",
    "# Create lookup dictionaries from pair_id -> (prefix, label)\n",
    "def create_pair_lookup(pairs_df: pd.DataFrame) -> Dict[int, Tuple[List[int], int]]:\n",
    "    \"\"\"Create lookup from pair_id to (prefix, label).\"\"\"\n",
    "    lookup = {}\n",
    "    for _, row in pairs_df.iterrows():\n",
    "        lookup[row[\"pair_id\"]] = (row[\"prefix\"], row[\"label\"])\n",
    "    return lookup\n",
    "\n",
    "pairs_lookup_train = create_pair_lookup(pairs_train)\n",
    "pairs_lookup_val = create_pair_lookup(pairs_val)\n",
    "pairs_lookup_test = create_pair_lookup(pairs_test)\n",
    "\n",
    "print(f\"[CELL 08-03] Created pair lookups for train/val/test\")\n",
    "\n",
    "def get_episode_data(episode_row, pairs_lookup: Dict) -> Tuple[List[List[int]], List[int], List[List[int]], List[int]]:\n",
    "    \"\"\"Extract support and query data from episode using pair lookup.\n",
    "    \n",
    "    Returns:\n",
    "        support_prefixes: List of prefix sequences\n",
    "        support_labels: List of label course IDs\n",
    "        query_prefixes: List of prefix sequences  \n",
    "        query_labels: List of label course IDs\n",
    "    \"\"\"\n",
    "    support_prefixes = []\n",
    "    support_labels = []\n",
    "    for pair_id in episode_row[\"support_pair_ids\"]:\n",
    "        prefix, label = pairs_lookup[pair_id]\n",
    "        support_prefixes.append(prefix)\n",
    "        support_labels.append(label)\n",
    "    \n",
    "    query_prefixes = []\n",
    "    query_labels = []\n",
    "    for pair_id in episode_row[\"query_pair_ids\"]:\n",
    "        prefix, label = pairs_lookup[pair_id]\n",
    "        query_prefixes.append(prefix)\n",
    "        query_labels.append(label)\n",
    "    \n",
    "    return support_prefixes, support_labels, query_prefixes, query_labels\n",
    "\n",
    "print(f\"[CELL 08-03] get_episode_data() helper function defined\")\n",
    "\n",
    "cell_end(\"CELL 08-03\", t0, n_items=n_items, n_train=len(episodes_train), n_val=len(episodes_val), n_test=len(episodes_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 08-04] start=2026-02-03T02:39:54 | Define GRU4Rec model with Residual Adaptation\n",
      "[CELL 08-04] GRURecommender defined\n",
      "[CELL 08-04] ResidualDeltaModule defined\n",
      "  - Pretrained weights: frozen as buffers\n",
      "  - Delta weights: learnable parameters\n",
      "  - Effective = Pretrained + Delta\n",
      "[CELL 08-04] done in 0.0s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-04] Define GRU4Rec model with Residual Adaptation\n",
    "\n",
    "t0 = cell_start(\"CELL 08-04\", \"Define GRU4Rec model with Residual Adaptation\")\n",
    "\n",
    "class GRURecommender(nn.Module):\n",
    "    \"\"\"GRU4Rec model for sequential recommendation.\n",
    "    \n",
    "    IMPORTANT: Architecture MUST match notebook 06 baseline for weight loading:\n",
    "    - embed_dim=64, hidden_dim=128\n",
    "    - Final layer named 'fc' (not 'output')\n",
    "    \"\"\"\n",
    "    def __init__(self, n_items: int, embed_dim: int, hidden_dim: int, n_layers: int = 1, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.n_items = n_items\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_items, embed_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if n_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # NOTE: Named 'fc' to match pre-trained model from notebook 06\n",
    "        self.fc = nn.Linear(hidden_dim, n_items)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        if lengths is not None:\n",
    "            packed = nn.utils.rnn.pack_padded_sequence(\n",
    "                embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            output, hidden = self.gru(packed)\n",
    "        else:\n",
    "            output, hidden = self.gru(embedded)\n",
    "        \n",
    "        last_hidden = hidden[-1]\n",
    "        out = self.dropout(last_hidden)\n",
    "        logits = self.fc(out)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "class ResidualDeltaModule(nn.Module):\n",
    "    \"\"\"Learnable delta/residual weights for adaptation.\n",
    "    \n",
    "    This module learns small delta weights that are added to frozen pretrained weights.\n",
    "    Only the delta weights are meta-learned and adapted in the inner loop.\n",
    "    \n",
    "    Key insight: effective_params = frozen_pretrained + learnable_delta\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_state_dict: dict, delta_init_scale: float = 0.01):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store pretrained weights as frozen buffers (not parameters)\n",
    "        self.pretrained_keys = []\n",
    "        for name, param in pretrained_state_dict.items():\n",
    "            # Register as buffer (frozen, not trained)\n",
    "            self.register_buffer(f\"pretrained_{name.replace('.', '_')}\", param.clone())\n",
    "            self.pretrained_keys.append(name)\n",
    "            \n",
    "            # Create learnable delta initialized to small values\n",
    "            delta = torch.zeros_like(param)\n",
    "            # Small initialization for delta (start close to pretrained)\n",
    "            if len(param.shape) >= 2:  # Weight matrices\n",
    "                nn.init.normal_(delta, mean=0.0, std=delta_init_scale)\n",
    "            # Biases start at zero delta\n",
    "            self.register_parameter(f\"delta_{name.replace('.', '_')}\", nn.Parameter(delta))\n",
    "    \n",
    "    def get_effective_params(self) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Get effective parameters = pretrained + delta.\"\"\"\n",
    "        effective = {}\n",
    "        for name in self.pretrained_keys:\n",
    "            safe_name = name.replace('.', '_')\n",
    "            pretrained = getattr(self, f\"pretrained_{safe_name}\")\n",
    "            delta = getattr(self, f\"delta_{safe_name}\")\n",
    "            effective[name] = pretrained + delta\n",
    "        return effective\n",
    "    \n",
    "    def get_delta_params(self) -> Dict[str, nn.Parameter]:\n",
    "        \"\"\"Get only the delta parameters (for MAML inner loop).\"\"\"\n",
    "        deltas = {}\n",
    "        for name in self.pretrained_keys:\n",
    "            safe_name = name.replace('.', '_')\n",
    "            deltas[name] = getattr(self, f\"delta_{safe_name}\")\n",
    "        return deltas\n",
    "\n",
    "\n",
    "print(f\"[CELL 08-04] GRURecommender defined\")\n",
    "print(f\"[CELL 08-04] ResidualDeltaModule defined\")\n",
    "print(f\"  - Pretrained weights: frozen as buffers\")\n",
    "print(f\"  - Delta weights: learnable parameters\")\n",
    "print(f\"  - Effective = Pretrained + Delta\")\n",
    "\n",
    "cell_end(\"CELL 08-04\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 08-05] start=2026-02-03T02:39:54 | Initialize model with Residual Warm-Start\n",
      "[CELL 08-05] Loading pre-trained GRU4Rec from: /workspace/anonymous-users-mooc-session-meta/models/baselines/gru_global.pth\n",
      "[CELL 08-05] Pretrained keys: ['embedding.weight', 'gru.weight_ih_l0', 'gru.weight_hh_l0', 'gru.bias_ih_l0', 'gru.bias_hh_l0', 'fc.weight', 'fc.bias']\n",
      "[CELL 08-05] Residual Warm-Start initialized:\n",
      "  - Pretrained params (frozen): 367,470\n",
      "  - Delta params (learnable):   367,470\n",
      "  - Delta init scale: 0.01\n",
      "[CELL 08-05] Effective param keys: ['embedding.weight', 'gru.weight_ih_l0', 'gru.weight_hh_l0', 'gru.bias_ih_l0', 'gru.bias_hh_l0', 'fc.weight', 'fc.bias']\n",
      "[CELL 08-05] Optimizer: Adam (outer_lr=0.0001)\n",
      "[CELL 08-05] Only delta weights are optimized!\n",
      "[CELL 08-05] n_pretrained=367470\n",
      "[CELL 08-05] n_delta=367470\n",
      "[CELL 08-05] done in 1.0s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-05] Initialize model with Residual Warm-Start (KEY CONTRIBUTION)\n",
    "\n",
    "t0 = cell_start(\"CELL 08-05\", \"Initialize model with Residual Warm-Start\")\n",
    "\n",
    "# ============================================================\n",
    "# KEY CONTRIBUTION: Residual Warm-Start\n",
    "# - Load pretrained GRU4Rec weights (frozen)\n",
    "# - Create learnable delta weights (meta-learned)\n",
    "# - Effective weights = Pretrained + Delta\n",
    "# ============================================================\n",
    "\n",
    "print(f\"[CELL 08-05] Loading pre-trained GRU4Rec from: {PRETRAINED_PATH}\")\n",
    "\n",
    "# Load pretrained weights\n",
    "pretrained_state = torch.load(PRETRAINED_PATH, map_location=DEVICE)\n",
    "print(f\"[CELL 08-05] Pretrained keys: {list(pretrained_state.keys())}\")\n",
    "\n",
    "# Create residual delta module\n",
    "residual_module = ResidualDeltaModule(\n",
    "    pretrained_state_dict=pretrained_state,\n",
    "    delta_init_scale=CFG[\"residual_config\"][\"delta_init_scale\"],\n",
    ").to(DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "n_pretrained = sum(p.numel() for p in pretrained_state.values())\n",
    "n_delta = sum(p.numel() for p in residual_module.parameters())\n",
    "\n",
    "print(f\"[CELL 08-05] Residual Warm-Start initialized:\")\n",
    "print(f\"  - Pretrained params (frozen): {n_pretrained:,}\")\n",
    "print(f\"  - Delta params (learnable):   {n_delta:,}\")\n",
    "print(f\"  - Delta init scale: {CFG['residual_config']['delta_init_scale']}\")\n",
    "\n",
    "# Verify: effective params at init should equal pretrained (delta is small)\n",
    "effective_params = residual_module.get_effective_params()\n",
    "print(f\"[CELL 08-05] Effective param keys: {list(effective_params.keys())}\")\n",
    "\n",
    "# Setup optimizer (only optimizes delta params)\n",
    "meta_optimizer = torch.optim.Adam(\n",
    "    residual_module.parameters(),  # Only delta params\n",
    "    lr=CFG[\"maml_config\"][\"outer_lr\"]\n",
    ")\n",
    "\n",
    "print(f\"[CELL 08-05] Optimizer: Adam (outer_lr={CFG['maml_config']['outer_lr']})\")\n",
    "print(f\"[CELL 08-05] Only delta weights are optimized!\")\n",
    "\n",
    "cell_end(\"CELL 08-05\", t0, n_pretrained=n_pretrained, n_delta=n_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 08-06] start=2026-02-03T02:39:55 | Define functional forward and verify warm-start\n",
      "[CELL 08-06] functional_forward_residual defined (with proper variable-length handling)\n",
      "\n",
      "[CELL 08-06] === Test 1: Native GRU4Rec model ===\n",
      "[CELL 08-06] Native GRU4Rec: 0.3040 (30.40%) - Expected ~33.55%\n",
      "\n",
      "[CELL 08-06] === Test 2: Functional forward with pretrained ===\n",
      "[CELL 08-06] Functional forward: 0.3030 (30.30%)\n",
      "\n",
      "[CELL 08-06] === Test 3: Residual module (pretrained + delta) ===\n",
      "[CELL 08-06] Residual module: 0.3030 (30.30%)\n",
      "\n",
      "[CELL 08-06] === SUMMARY ===\n",
      "  Native GRU4Rec:     30.40%\n",
      "  Functional forward: 30.30%\n",
      "  Residual module:    30.30%\n",
      "\n",
      "[CELL 08-06] ✓ Functional forward matches native model!\n",
      "[CELL 08-06] done in 0.9s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-06] Define functional forward and verify warm-start\n",
    "\n",
    "t0 = cell_start(\"CELL 08-06\", \"Define functional forward and verify warm-start\")\n",
    "\n",
    "def functional_forward_residual(x: torch.Tensor, lengths: torch.Tensor, \n",
    "                                 params: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "    \"\"\"Functional forward with manual GRU that maintains computation graph.\n",
    "    \n",
    "    CRITICAL FIX: Handle variable-length sequences properly!\n",
    "    The native GRU uses pack_padded_sequence to:\n",
    "    1. Only process valid tokens (not padding)\n",
    "    2. Extract hidden state at the actual sequence end\n",
    "    \n",
    "    We must do the same manually.\n",
    "    \n",
    "    PyTorch GRU equations:\n",
    "        r_t = σ(W_ir @ x_t + b_ir + W_hr @ h_{t-1} + b_hr)\n",
    "        z_t = σ(W_iz @ x_t + b_iz + W_hz @ h_{t-1} + b_hz)\n",
    "        n_t = tanh(W_in @ x_t + b_in + r_t * (W_hn @ h_{t-1} + b_hn))\n",
    "        h_t = (1 - z_t) * n_t + z_t * h_{t-1}\n",
    "    \"\"\"\n",
    "    # Embedding\n",
    "    embedded = F.embedding(x, params[\"embedding.weight\"], padding_idx=0)\n",
    "    \n",
    "    # GRU weights\n",
    "    W_ih = params[\"gru.weight_ih_l0\"]  # [3*hidden, input]\n",
    "    W_hh = params[\"gru.weight_hh_l0\"]  # [3*hidden, hidden]\n",
    "    b_ih = params[\"gru.bias_ih_l0\"]    # [3*hidden]\n",
    "    b_hh = params[\"gru.bias_hh_l0\"]    # [3*hidden]\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    seq_len = embedded.size(1)\n",
    "    hidden_dim = W_hh.size(1)\n",
    "    \n",
    "    # Initialize hidden state\n",
    "    h = torch.zeros(batch_size, hidden_dim, device=x.device, dtype=embedded.dtype)\n",
    "    \n",
    "    # Store hidden states for each timestep to extract at correct position\n",
    "    all_hidden = []\n",
    "    \n",
    "    # Process each timestep\n",
    "    for t in range(seq_len):\n",
    "        inp = embedded[:, t, :]  # [batch, input_dim]\n",
    "        \n",
    "        # Input-to-hidden and hidden-to-hidden\n",
    "        gi = inp @ W_ih.t() + b_ih  # [batch, 3*hidden]\n",
    "        gh = h @ W_hh.t() + b_hh    # [batch, 3*hidden]\n",
    "        \n",
    "        # Split into gates\n",
    "        i_r, i_z, i_n = gi.chunk(3, dim=1)\n",
    "        h_r, h_z, h_n = gh.chunk(3, dim=1)\n",
    "        \n",
    "        # Reset gate\n",
    "        r = torch.sigmoid(i_r + h_r)\n",
    "        # Update gate\n",
    "        z = torch.sigmoid(i_z + h_z)\n",
    "        # New candidate (reset gate applied to hidden contribution)\n",
    "        n = torch.tanh(i_n + r * h_n)\n",
    "        \n",
    "        # Update hidden state\n",
    "        h = (1 - z) * n + z * h\n",
    "        \n",
    "        all_hidden.append(h)\n",
    "    \n",
    "    # Stack all hidden states: [batch, seq_len, hidden]\n",
    "    all_hidden = torch.stack(all_hidden, dim=1)\n",
    "    \n",
    "    # CRITICAL: Extract hidden state at the ACTUAL sequence end (not last timestep)\n",
    "    # lengths contains the actual length of each sequence\n",
    "    # We need to gather hidden state at position (length - 1) for each batch item\n",
    "    batch_indices = torch.arange(batch_size, device=x.device)\n",
    "    # Clamp lengths to valid range (minimum 1 to avoid negative indexing)\n",
    "    valid_lengths = lengths.clamp(min=1) - 1\n",
    "    \n",
    "    # Extract hidden state at the correct position for each sequence\n",
    "    final_hidden = all_hidden[batch_indices, valid_lengths]  # [batch, hidden]\n",
    "    \n",
    "    # Output projection\n",
    "    logits = final_hidden @ params[\"fc.weight\"].t() + params[\"fc.bias\"]\n",
    "    \n",
    "    return logits\n",
    "\n",
    "print(\"[CELL 08-06] functional_forward_residual defined (with proper variable-length handling)\")\n",
    "\n",
    "# Verify with native model first\n",
    "print(\"\\n[CELL 08-06] === Test 1: Native GRU4Rec model ===\")\n",
    "\n",
    "native_model = GRURecommender(\n",
    "    n_items=n_items,\n",
    "    embed_dim=CFG[\"model_config\"][\"embed_dim\"],\n",
    "    hidden_dim=CFG[\"model_config\"][\"hidden_dim\"],\n",
    "    n_layers=CFG[\"model_config\"][\"n_layers\"],\n",
    "    dropout=0.0,\n",
    ").to(DEVICE)\n",
    "native_model.load_state_dict(pretrained_state)\n",
    "native_model.eval()\n",
    "\n",
    "n_test_sample = min(100, len(episodes_test))\n",
    "native_correct = 0\n",
    "native_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(n_test_sample):\n",
    "        row = episodes_test.iloc[i]\n",
    "        _, _, query_prefixes, query_labels = get_episode_data(row, pairs_lookup_test)\n",
    "        \n",
    "        max_len = max(len(seq) for seq in query_prefixes)\n",
    "        padded_x = torch.zeros(len(query_prefixes), max_len, dtype=torch.long)\n",
    "        lengths = torch.zeros(len(query_prefixes), dtype=torch.long)\n",
    "        \n",
    "        for j, seq in enumerate(query_prefixes):\n",
    "            padded_x[j, :len(seq)] = torch.tensor(seq)\n",
    "            lengths[j] = len(seq)\n",
    "        \n",
    "        padded_x = padded_x.to(DEVICE)\n",
    "        query_y = torch.tensor(query_labels, dtype=torch.long, device=DEVICE)\n",
    "        \n",
    "        logits = native_model(padded_x, lengths.to(DEVICE))\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        native_correct += (preds == query_y).sum().item()\n",
    "        native_total += len(query_y)\n",
    "\n",
    "native_acc = native_correct / native_total\n",
    "print(f\"[CELL 08-06] Native GRU4Rec: {native_acc:.4f} ({native_acc*100:.2f}%) - Expected ~33.55%\")\n",
    "\n",
    "# Test functional forward with pretrained weights\n",
    "print(\"\\n[CELL 08-06] === Test 2: Functional forward with pretrained ===\")\n",
    "\n",
    "func_correct = 0\n",
    "func_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(n_test_sample):\n",
    "        row = episodes_test.iloc[i]\n",
    "        _, _, query_prefixes, query_labels = get_episode_data(row, pairs_lookup_test)\n",
    "        \n",
    "        max_len = max(len(seq) for seq in query_prefixes)\n",
    "        padded_x = torch.zeros(len(query_prefixes), max_len, dtype=torch.long)\n",
    "        lengths = torch.zeros(len(query_prefixes), dtype=torch.long)\n",
    "        \n",
    "        for j, seq in enumerate(query_prefixes):\n",
    "            padded_x[j, :len(seq)] = torch.tensor(seq)\n",
    "            lengths[j] = len(seq)\n",
    "        \n",
    "        padded_x = padded_x.to(DEVICE)\n",
    "        query_y = torch.tensor(query_labels, dtype=torch.long, device=DEVICE)\n",
    "        \n",
    "        # Use lengths tensor on DEVICE\n",
    "        logits = functional_forward_residual(padded_x, lengths.to(DEVICE), pretrained_state)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        func_correct += (preds == query_y).sum().item()\n",
    "        func_total += len(query_y)\n",
    "\n",
    "func_acc = func_correct / func_total\n",
    "print(f\"[CELL 08-06] Functional forward: {func_acc:.4f} ({func_acc*100:.2f}%)\")\n",
    "\n",
    "# Test residual module\n",
    "print(\"\\n[CELL 08-06] === Test 3: Residual module (pretrained + delta) ===\")\n",
    "\n",
    "effective_params = residual_module.get_effective_params()\n",
    "res_correct = 0\n",
    "res_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(n_test_sample):\n",
    "        row = episodes_test.iloc[i]\n",
    "        _, _, query_prefixes, query_labels = get_episode_data(row, pairs_lookup_test)\n",
    "        \n",
    "        max_len = max(len(seq) for seq in query_prefixes)\n",
    "        padded_x = torch.zeros(len(query_prefixes), max_len, dtype=torch.long)\n",
    "        lengths = torch.zeros(len(query_prefixes), dtype=torch.long)\n",
    "        \n",
    "        for j, seq in enumerate(query_prefixes):\n",
    "            padded_x[j, :len(seq)] = torch.tensor(seq)\n",
    "            lengths[j] = len(seq)\n",
    "        \n",
    "        padded_x = padded_x.to(DEVICE)\n",
    "        query_y = torch.tensor(query_labels, dtype=torch.long, device=DEVICE)\n",
    "        \n",
    "        logits = functional_forward_residual(padded_x, lengths.to(DEVICE), effective_params)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        res_correct += (preds == query_y).sum().item()\n",
    "        res_total += len(query_y)\n",
    "\n",
    "res_acc = res_correct / res_total\n",
    "print(f\"[CELL 08-06] Residual module: {res_acc:.4f} ({res_acc*100:.2f}%)\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n[CELL 08-06] === SUMMARY ===\")\n",
    "print(f\"  Native GRU4Rec:     {native_acc*100:.2f}%\")\n",
    "print(f\"  Functional forward: {func_acc*100:.2f}%\")\n",
    "print(f\"  Residual module:    {res_acc*100:.2f}%\")\n",
    "\n",
    "if abs(native_acc - func_acc) < 0.02:\n",
    "    print(\"\\n[CELL 08-06] ✓ Functional forward matches native model!\")\n",
    "else:\n",
    "    print(f\"\\n[CELL 08-06] ⚠ Gap between native and functional: {abs(native_acc-func_acc)*100:.1f}pp\")\n",
    "\n",
    "cell_end(\"CELL 08-06\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 08-07] start=2026-02-03T02:39:56 | Define Residual MAML helper functions\n",
      "[CELL 08-07] Helper functions defined\n",
      "  - pad_sequences: Pad variable-length sequences\n",
      "  - get_effective_params_with_delta: Get effective params for inner loop\n",
      "  - functional_forward_residual: Forward pass (defined in CELL 08-06)\n",
      "[CELL 08-07] done in 0.0s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-07] Helper functions for Residual MAML training\n",
    "\n",
    "t0 = cell_start(\"CELL 08-07\", \"Define Residual MAML helper functions\")\n",
    "\n",
    "def pad_sequences(sequences: List[List[int]], pad_value: int = 0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Pad variable-length sequences.\"\"\"\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long)\n",
    "    max_len = max(lengths).item()\n",
    "    \n",
    "    padded = torch.full((len(sequences), max_len), pad_value, dtype=torch.long)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded[i, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n",
    "    \n",
    "    return padded, lengths\n",
    "\n",
    "def get_effective_params_with_delta(residual_module: ResidualDeltaModule, \n",
    "                                     delta_updates: Dict[str, torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Get effective parameters with optional delta updates.\n",
    "    \n",
    "    Args:\n",
    "        residual_module: The residual module with pretrained + base delta\n",
    "        delta_updates: Optional additional updates to delta (for inner loop)\n",
    "        \n",
    "    Returns:\n",
    "        effective_params: pretrained + (base_delta + delta_updates)\n",
    "    \"\"\"\n",
    "    effective = {}\n",
    "    for name in residual_module.pretrained_keys:\n",
    "        safe_name = name.replace('.', '_')\n",
    "        pretrained = getattr(residual_module, f\"pretrained_{safe_name}\")\n",
    "        delta = getattr(residual_module, f\"delta_{safe_name}\")\n",
    "        \n",
    "        if delta_updates is not None and name in delta_updates:\n",
    "            # Use updated delta from inner loop\n",
    "            effective[name] = pretrained + delta_updates[name]\n",
    "        else:\n",
    "            # Use base delta\n",
    "            effective[name] = pretrained + delta\n",
    "    \n",
    "    return effective\n",
    "\n",
    "print(f\"[CELL 08-07] Helper functions defined\")\n",
    "print(f\"  - pad_sequences: Pad variable-length sequences\")\n",
    "print(f\"  - get_effective_params_with_delta: Get effective params for inner loop\")\n",
    "print(f\"  - functional_forward_residual: Forward pass (defined in CELL 08-06)\")\n",
    "\n",
    "cell_end(\"CELL 08-07\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 08-08] start=2026-02-03T02:39:56 | Residual MAML Training with Frozen Pretrained + Learnable Delta\n",
      "[CELL 08-08] Residual MAML Training config:\n",
      "  - Inner LR (alpha): 0.01\n",
      "  - Outer LR (beta): 0.0001 (LOWERED)\n",
      "  - Inner steps: 3\n",
      "  - Meta-batch size: 32\n",
      "  - Iterations: 3000\n",
      "  - Residual: Pretrained (frozen) + Delta (learnable)\n",
      "[CELL 08-08] Iteration 1/3000, Meta-Loss: 3.2636\n",
      "[CELL 08-08] Iteration 100/3000, Meta-Loss: 2.9331\n",
      "[CELL 08-08] Iteration 100, Val Acc@1: 0.3701 (37.01%)\n",
      "[CELL 08-08] New best model saved! Val Acc: 37.01%\n",
      "[CELL 08-08] Iteration 200/3000, Meta-Loss: 2.8007\n",
      "[CELL 08-08] Iteration 200, Val Acc@1: 0.3686 (36.86%)\n",
      "[CELL 08-08] Iteration 300/3000, Meta-Loss: 2.7168\n",
      "[CELL 08-08] Iteration 300, Val Acc@1: 0.3674 (36.74%)\n",
      "[CELL 08-08] Iteration 400/3000, Meta-Loss: 2.7313\n",
      "[CELL 08-08] Iteration 400, Val Acc@1: 0.3680 (36.80%)\n",
      "[CELL 08-08] Iteration 500/3000, Meta-Loss: 3.3955\n",
      "[CELL 08-08] Iteration 500, Val Acc@1: 0.3654 (36.54%)\n",
      "[CELL 08-08] Checkpoint saved: residual_warmstart_checkpoint_iter500.pth\n",
      "[CELL 08-08] Iteration 600/3000, Meta-Loss: 3.0430\n",
      "[CELL 08-08] Iteration 600, Val Acc@1: 0.3663 (36.63%)\n",
      "[CELL 08-08] Iteration 700/3000, Meta-Loss: 3.2293\n",
      "[CELL 08-08] Iteration 700, Val Acc@1: 0.3666 (36.66%)\n",
      "[CELL 08-08] Iteration 800/3000, Meta-Loss: 2.6598\n",
      "[CELL 08-08] Iteration 800, Val Acc@1: 0.3651 (36.51%)\n",
      "[CELL 08-08] Iteration 900/3000, Meta-Loss: 3.1219\n",
      "[CELL 08-08] Iteration 900, Val Acc@1: 0.3657 (36.57%)\n",
      "[CELL 08-08] Iteration 1000/3000, Meta-Loss: 3.0862\n",
      "[CELL 08-08] Iteration 1000, Val Acc@1: 0.3663 (36.63%)\n",
      "[CELL 08-08] Checkpoint saved: residual_warmstart_checkpoint_iter1000.pth\n",
      "[CELL 08-08] Iteration 1100/3000, Meta-Loss: 2.8934\n",
      "[CELL 08-08] Iteration 1100, Val Acc@1: 0.3651 (36.51%)\n",
      "[CELL 08-08] Iteration 1200/3000, Meta-Loss: 2.6326\n",
      "[CELL 08-08] Iteration 1200, Val Acc@1: 0.3639 (36.39%)\n",
      "[CELL 08-08] Iteration 1300/3000, Meta-Loss: 2.6325\n",
      "[CELL 08-08] Iteration 1300, Val Acc@1: 0.3645 (36.45%)\n",
      "[CELL 08-08] Iteration 1400/3000, Meta-Loss: 3.1636\n",
      "[CELL 08-08] Iteration 1400, Val Acc@1: 0.3645 (36.45%)\n",
      "[CELL 08-08] Iteration 1500/3000, Meta-Loss: 2.9539\n",
      "[CELL 08-08] Iteration 1500, Val Acc@1: 0.3651 (36.51%)\n",
      "[CELL 08-08] Checkpoint saved: residual_warmstart_checkpoint_iter1500.pth\n",
      "[CELL 08-08] Iteration 1600/3000, Meta-Loss: 2.4884\n",
      "[CELL 08-08] Iteration 1600, Val Acc@1: 0.3630 (36.30%)\n",
      "[CELL 08-08] Iteration 1700/3000, Meta-Loss: 2.7698\n",
      "[CELL 08-08] Iteration 1700, Val Acc@1: 0.3622 (36.22%)\n",
      "[CELL 08-08] Iteration 1800/3000, Meta-Loss: 2.4500\n",
      "[CELL 08-08] Iteration 1800, Val Acc@1: 0.3616 (36.16%)\n",
      "[CELL 08-08] Iteration 1900/3000, Meta-Loss: 2.7503\n",
      "[CELL 08-08] Iteration 1900, Val Acc@1: 0.3613 (36.13%)\n",
      "[CELL 08-08] Iteration 2000/3000, Meta-Loss: 2.5037\n",
      "[CELL 08-08] Iteration 2000, Val Acc@1: 0.3613 (36.13%)\n",
      "[CELL 08-08] Checkpoint saved: residual_warmstart_checkpoint_iter2000.pth\n",
      "[CELL 08-08] Iteration 2100/3000, Meta-Loss: 2.7236\n",
      "[CELL 08-08] Iteration 2100, Val Acc@1: 0.3622 (36.22%)\n",
      "[CELL 08-08] Iteration 2200/3000, Meta-Loss: 2.7752\n",
      "[CELL 08-08] Iteration 2200, Val Acc@1: 0.3616 (36.16%)\n",
      "[CELL 08-08] Iteration 2300/3000, Meta-Loss: 2.9816\n",
      "[CELL 08-08] Iteration 2300, Val Acc@1: 0.3616 (36.16%)\n",
      "[CELL 08-08] Iteration 2400/3000, Meta-Loss: 2.6734\n",
      "[CELL 08-08] Iteration 2400, Val Acc@1: 0.3613 (36.13%)\n",
      "[CELL 08-08] Iteration 2500/3000, Meta-Loss: 3.3200\n",
      "[CELL 08-08] Iteration 2500, Val Acc@1: 0.3616 (36.16%)\n",
      "[CELL 08-08] Checkpoint saved: residual_warmstart_checkpoint_iter2500.pth\n",
      "[CELL 08-08] Iteration 2600/3000, Meta-Loss: 2.8368\n",
      "[CELL 08-08] Iteration 2600, Val Acc@1: 0.3630 (36.30%)\n",
      "[CELL 08-08] Iteration 2700/3000, Meta-Loss: 2.6598\n",
      "[CELL 08-08] Iteration 2700, Val Acc@1: 0.3613 (36.13%)\n",
      "[CELL 08-08] Iteration 2800/3000, Meta-Loss: 2.7971\n",
      "[CELL 08-08] Iteration 2800, Val Acc@1: 0.3595 (35.95%)\n",
      "[CELL 08-08] Iteration 2900/3000, Meta-Loss: 2.5045\n",
      "[CELL 08-08] Iteration 2900, Val Acc@1: 0.3595 (35.95%)\n",
      "[CELL 08-08] Iteration 3000/3000, Meta-Loss: 2.8548\n",
      "[CELL 08-08] Iteration 3000, Val Acc@1: 0.3584 (35.84%)\n",
      "[CELL 08-08] Checkpoint saved: residual_warmstart_checkpoint_iter3000.pth\n",
      "\n",
      "[CELL 08-08] Training complete!\n",
      "[CELL 08-08] Best validation accuracy: 37.01%\n",
      "[CELL 08-08] best_val_acc=0.37008797653958947\n",
      "[CELL 08-08] done in 4748.7s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-08] Residual MAML Training Loop (KEY CONTRIBUTION) - FIXED GRADIENT FLOW\n",
    "\n",
    "t0 = cell_start(\"CELL 08-08\", \"Residual MAML Training with Frozen Pretrained + Learnable Delta\")\n",
    "\n",
    "# Training config\n",
    "inner_lr = CFG[\"maml_config\"][\"inner_lr\"]\n",
    "inner_steps = CFG[\"maml_config\"][\"inner_steps\"]\n",
    "meta_batch_size = CFG[\"maml_config\"][\"meta_batch_size\"]\n",
    "num_iterations = CFG[\"maml_config\"][\"num_meta_iterations\"]\n",
    "val_every = CFG[\"maml_config\"][\"val_every\"]\n",
    "checkpoint_every = CFG[\"maml_config\"][\"checkpoint_every\"]\n",
    "\n",
    "print(f\"[CELL 08-08] Residual MAML Training config:\")\n",
    "print(f\"  - Inner LR (alpha): {inner_lr}\")\n",
    "print(f\"  - Outer LR (beta): {CFG['maml_config']['outer_lr']} (LOWERED)\")\n",
    "print(f\"  - Inner steps: {inner_steps}\")\n",
    "print(f\"  - Meta-batch size: {meta_batch_size}\")\n",
    "print(f\"  - Iterations: {num_iterations}\")\n",
    "print(f\"  - Residual: Pretrained (frozen) + Delta (learnable)\")\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"iteration\": [],\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "OUT_MODEL = CONTRIB_MODELS_DIR / \"warmstart_residual_maml_K5.pth\"\n",
    "\n",
    "# Training loop\n",
    "residual_module.train()\n",
    "\n",
    "for iteration in range(1, num_iterations + 1):\n",
    "    meta_optimizer.zero_grad()\n",
    "    \n",
    "    # Sample meta-batch of tasks (episodes)\n",
    "    task_indices = np.random.choice(len(episodes_train), size=meta_batch_size, replace=False)\n",
    "    \n",
    "    # Accumulate gradients across tasks\n",
    "    accumulated_grads = {name: torch.zeros_like(param) for name, param in residual_module.get_delta_params().items()}\n",
    "    total_query_loss = 0.0\n",
    "    \n",
    "    for task_idx in task_indices:\n",
    "        # Get episode data using lookup\n",
    "        row = episodes_train.iloc[task_idx]\n",
    "        support_prefixes, support_labels, query_prefixes, query_labels = get_episode_data(row, pairs_lookup_train)\n",
    "        \n",
    "        support_y = torch.tensor(support_labels, dtype=torch.long, device=DEVICE)\n",
    "        query_y = torch.tensor(query_labels, dtype=torch.long, device=DEVICE)\n",
    "        \n",
    "        # Pad sequences\n",
    "        support_x, support_lengths = pad_sequences(support_prefixes)\n",
    "        support_x = support_x.to(DEVICE)\n",
    "        support_lengths = support_lengths.to(DEVICE)  # IMPORTANT: Move to device\n",
    "        \n",
    "        query_x, query_lengths = pad_sequences(query_prefixes)\n",
    "        query_x = query_x.to(DEVICE)\n",
    "        query_lengths = query_lengths.to(DEVICE)  # IMPORTANT: Move to device\n",
    "        \n",
    "        # ============================================================\n",
    "        # Clone delta params for inner loop with requires_grad=True\n",
    "        # ============================================================\n",
    "        delta_params = {name: param.clone().requires_grad_(True) \n",
    "                       for name, param in residual_module.get_delta_params().items()}\n",
    "        \n",
    "        # Inner loop: adapt delta on support set\n",
    "        for _ in range(inner_steps):\n",
    "            effective_params = get_effective_params_with_delta(residual_module, delta_params)\n",
    "            support_logits = functional_forward_residual(support_x, support_lengths, effective_params)\n",
    "            support_loss = F.cross_entropy(support_logits, support_y)\n",
    "            \n",
    "            # Compute gradients w.r.t. delta params (allow_unused for safety)\n",
    "            grads = torch.autograd.grad(support_loss, list(delta_params.values()), \n",
    "                                        create_graph=False, allow_unused=True)\n",
    "            \n",
    "            # Update delta params (creates new tensors), handle None gradients\n",
    "            new_delta_params = {}\n",
    "            for (name, param), grad in zip(delta_params.items(), grads):\n",
    "                if grad is not None:\n",
    "                    new_delta_params[name] = (param - inner_lr * grad).requires_grad_(True)\n",
    "                else:\n",
    "                    new_delta_params[name] = param.requires_grad_(True)\n",
    "            delta_params = new_delta_params\n",
    "        \n",
    "        # Evaluate on query set with adapted delta\n",
    "        effective_params = get_effective_params_with_delta(residual_module, delta_params)\n",
    "        query_logits = functional_forward_residual(query_x, query_lengths, effective_params)\n",
    "        query_loss = F.cross_entropy(query_logits, query_y)\n",
    "        total_query_loss += query_loss.item()\n",
    "        \n",
    "        # ============================================================\n",
    "        # FOMAML: Compute gradients w.r.t. adapted delta_params\n",
    "        # Then accumulate to original delta params\n",
    "        # ============================================================\n",
    "        query_grads = torch.autograd.grad(query_loss, list(delta_params.values()), allow_unused=True)\n",
    "        \n",
    "        for (name, _), grad in zip(delta_params.items(), query_grads):\n",
    "            if grad is not None:\n",
    "                accumulated_grads[name] += grad\n",
    "    \n",
    "    # Average gradients and apply to original delta params\n",
    "    for name, param in residual_module.get_delta_params().items():\n",
    "        param.grad = accumulated_grads[name] / meta_batch_size\n",
    "    \n",
    "    # Outer loop: meta-update\n",
    "    torch.nn.utils.clip_grad_norm_(residual_module.parameters(), max_norm=10.0)\n",
    "    meta_optimizer.step()\n",
    "    \n",
    "    # Logging\n",
    "    avg_loss = total_query_loss / meta_batch_size\n",
    "    if iteration % 100 == 0 or iteration == 1:\n",
    "        print(f\"[CELL 08-08] Iteration {iteration}/{num_iterations}, Meta-Loss: {avg_loss:.4f}\")\n",
    "        history[\"train_loss\"].append(avg_loss)\n",
    "        history[\"iteration\"].append(iteration)\n",
    "    \n",
    "    # Validation\n",
    "    if iteration % val_every == 0:\n",
    "        residual_module.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        for val_idx in range(len(episodes_val)):\n",
    "            row = episodes_val.iloc[val_idx]\n",
    "            support_prefixes, support_labels, query_prefixes, query_labels = get_episode_data(row, pairs_lookup_val)\n",
    "            \n",
    "            support_y = torch.tensor(support_labels, dtype=torch.long, device=DEVICE)\n",
    "            query_y = torch.tensor(query_labels, dtype=torch.long, device=DEVICE)\n",
    "            \n",
    "            support_x, support_lengths = pad_sequences(support_prefixes)\n",
    "            support_x = support_x.to(DEVICE)\n",
    "            support_lengths = support_lengths.to(DEVICE)\n",
    "            \n",
    "            query_x, query_lengths = pad_sequences(query_prefixes)\n",
    "            query_x = query_x.to(DEVICE)\n",
    "            query_lengths = query_lengths.to(DEVICE)\n",
    "            \n",
    "            # Clone delta and adapt\n",
    "            delta_params = {name: param.clone().requires_grad_(True) \n",
    "                           for name, param in residual_module.get_delta_params().items()}\n",
    "            \n",
    "            for _ in range(inner_steps):\n",
    "                effective_params = get_effective_params_with_delta(residual_module, delta_params)\n",
    "                support_logits = functional_forward_residual(support_x, support_lengths, effective_params)\n",
    "                support_loss = F.cross_entropy(support_logits, support_y)\n",
    "                grads = torch.autograd.grad(support_loss, list(delta_params.values()), allow_unused=True)\n",
    "                new_delta_params = {}\n",
    "                for (name, param), grad in zip(delta_params.items(), grads):\n",
    "                    if grad is not None:\n",
    "                        new_delta_params[name] = (param - inner_lr * grad).requires_grad_(True)\n",
    "                    else:\n",
    "                        new_delta_params[name] = param.requires_grad_(True)\n",
    "                delta_params = new_delta_params\n",
    "            \n",
    "            # Query prediction\n",
    "            with torch.no_grad():\n",
    "                effective_params = get_effective_params_with_delta(residual_module, delta_params)\n",
    "                query_logits = functional_forward_residual(query_x, query_lengths, effective_params)\n",
    "                preds = query_logits.argmax(dim=-1)\n",
    "                \n",
    "                val_correct += (preds == query_y).sum().item()\n",
    "                val_total += len(query_y)\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        print(f\"[CELL 08-08] Iteration {iteration}, Val Acc@1: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(residual_module.state_dict(), OUT_MODEL)\n",
    "            print(f\"[CELL 08-08] New best model saved! Val Acc: {val_acc*100:.2f}%\")\n",
    "        \n",
    "        residual_module.train()\n",
    "    \n",
    "    # Checkpoint\n",
    "    if iteration % checkpoint_every == 0:\n",
    "        checkpoint_path = CHECKPOINT_DIR / f\"residual_warmstart_checkpoint_iter{iteration}.pth\"\n",
    "        torch.save({\n",
    "            \"iteration\": iteration,\n",
    "            \"model_state_dict\": residual_module.state_dict(),\n",
    "            \"optimizer_state_dict\": meta_optimizer.state_dict(),\n",
    "            \"best_val_acc\": best_val_acc,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"[CELL 08-08] Checkpoint saved: {checkpoint_path.name}\")\n",
    "\n",
    "print(f\"\\n[CELL 08-08] Training complete!\")\n",
    "print(f\"[CELL 08-08] Best validation accuracy: {best_val_acc*100:.2f}%\")\n",
    "\n",
    "cell_end(\"CELL 08-08\", t0, best_val_acc=best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 08-09] start=2026-02-03T03:59:04 | Final evaluation on test set\n",
      "[CELL 08-09] Loaded best model from: /workspace/anonymous-users-mooc-session-meta/models/contributions/warmstart_residual_maml_K5.pth\n",
      "\n",
      "[CELL 08-09] ========== RESULTS ==========\n",
      "[CELL 08-09] Test episodes: 313\n",
      "\n",
      "[CELL 08-09] Residual Warm-Start MAML Zero-shot: 0.3332 (33.32%)\n",
      "[CELL 08-09] Residual Warm-Start MAML Few-shot:  0.3495 (34.95%)\n",
      "\n",
      "[CELL 08-09] ========== COMPARISON ==========\n",
      "[CELL 08-09] GRU4Rec baseline:                   33.55%\n",
      "[CELL 08-09] Vanilla MAML Zero-shot:             25.62%\n",
      "[CELL 08-09] Vanilla MAML Few-shot:              28.66%\n",
      "[CELL 08-09] Naive Warm-Start MAML (failed):     ~24%\n",
      "[CELL 08-09] Residual Warm-Start Zero-shot:      33.32%\n",
      "[CELL 08-09] Residual Warm-Start Few-shot:       34.95%\n",
      "\n",
      "[CELL 08-09] Improvement over Vanilla MAML: +6.29 pp\n",
      "[CELL 08-09] Improvement over GRU4Rec:      +1.40 pp\n",
      "[CELL 08-09] zeroshot_acc=0.3332268370607029\n",
      "[CELL 08-09] fewshot_acc=0.34952076677316296\n",
      "[CELL 08-09] done in 9.1s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-09] Final Evaluation on Test Set\n",
    "\n",
    "t0 = cell_start(\"CELL 08-09\", \"Final evaluation on test set\")\n",
    "\n",
    "# Load best model\n",
    "residual_module.load_state_dict(torch.load(OUT_MODEL, map_location=DEVICE))\n",
    "residual_module.eval()\n",
    "\n",
    "print(f\"[CELL 08-09] Loaded best model from: {OUT_MODEL}\")\n",
    "\n",
    "# Evaluate: Zero-shot (no adaptation) - should be close to GRU4Rec baseline\n",
    "zeroshot_correct = 0\n",
    "zeroshot_total = 0\n",
    "\n",
    "# Evaluate: Few-shot (with adaptation)\n",
    "fewshot_correct = 0\n",
    "fewshot_total = 0\n",
    "\n",
    "for test_idx in range(len(episodes_test)):\n",
    "    row = episodes_test.iloc[test_idx]\n",
    "    support_prefixes, support_labels, query_prefixes, query_labels = get_episode_data(row, pairs_lookup_test)\n",
    "    \n",
    "    support_y = torch.tensor(support_labels, dtype=torch.long, device=DEVICE)\n",
    "    query_y = torch.tensor(query_labels, dtype=torch.long, device=DEVICE)\n",
    "    \n",
    "    support_x, support_lengths = pad_sequences(support_prefixes)\n",
    "    support_x = support_x.to(DEVICE)\n",
    "    support_lengths = support_lengths.to(DEVICE)  # Move to device\n",
    "    \n",
    "    query_x, query_lengths = pad_sequences(query_prefixes)\n",
    "    query_x = query_x.to(DEVICE)\n",
    "    query_lengths = query_lengths.to(DEVICE)  # Move to device\n",
    "    \n",
    "    # Zero-shot: no adaptation (use base effective params)\n",
    "    with torch.no_grad():\n",
    "        effective_params = residual_module.get_effective_params()\n",
    "        query_logits_zs = functional_forward_residual(query_x, query_lengths, effective_params)\n",
    "        preds_zs = query_logits_zs.argmax(dim=-1)\n",
    "        zeroshot_correct += (preds_zs == query_y).sum().item()\n",
    "        zeroshot_total += len(query_y)\n",
    "    \n",
    "    # Few-shot: adapt delta on support set\n",
    "    delta_params = {name: param.clone().requires_grad_(True) \n",
    "                   for name, param in residual_module.get_delta_params().items()}\n",
    "    \n",
    "    for _ in range(inner_steps):\n",
    "        effective_params = get_effective_params_with_delta(residual_module, delta_params)\n",
    "        support_logits = functional_forward_residual(support_x, support_lengths, effective_params)\n",
    "        support_loss = F.cross_entropy(support_logits, support_y)\n",
    "        grads = torch.autograd.grad(support_loss, list(delta_params.values()), allow_unused=True)\n",
    "        new_delta_params = {}\n",
    "        for (name, param), grad in zip(delta_params.items(), grads):\n",
    "            if grad is not None:\n",
    "                new_delta_params[name] = (param - inner_lr * grad).requires_grad_(True)\n",
    "            else:\n",
    "                new_delta_params[name] = param.requires_grad_(True)\n",
    "        delta_params = new_delta_params\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        effective_params = get_effective_params_with_delta(residual_module, delta_params)\n",
    "        query_logits_fs = functional_forward_residual(query_x, query_lengths, effective_params)\n",
    "        preds_fs = query_logits_fs.argmax(dim=-1)\n",
    "        fewshot_correct += (preds_fs == query_y).sum().item()\n",
    "        fewshot_total += len(query_y)\n",
    "\n",
    "zeroshot_acc = zeroshot_correct / zeroshot_total\n",
    "fewshot_acc = fewshot_correct / fewshot_total\n",
    "\n",
    "print(f\"\\n[CELL 08-09] ========== RESULTS ==========\")\n",
    "print(f\"[CELL 08-09] Test episodes: {len(episodes_test)}\")\n",
    "print(f\"\\n[CELL 08-09] Residual Warm-Start MAML Zero-shot: {zeroshot_acc:.4f} ({zeroshot_acc*100:.2f}%)\")\n",
    "print(f\"[CELL 08-09] Residual Warm-Start MAML Few-shot:  {fewshot_acc:.4f} ({fewshot_acc*100:.2f}%)\")\n",
    "print(f\"\\n[CELL 08-09] ========== COMPARISON ==========\")\n",
    "print(f\"[CELL 08-09] GRU4Rec baseline:                   33.55%\")\n",
    "print(f\"[CELL 08-09] Vanilla MAML Zero-shot:             25.62%\")\n",
    "print(f\"[CELL 08-09] Vanilla MAML Few-shot:              28.66%\")\n",
    "print(f\"[CELL 08-09] Naive Warm-Start MAML (failed):     ~24%\")\n",
    "print(f\"[CELL 08-09] Residual Warm-Start Zero-shot:      {zeroshot_acc*100:.2f}%\")\n",
    "print(f\"[CELL 08-09] Residual Warm-Start Few-shot:       {fewshot_acc*100:.2f}%\")\n",
    "print(f\"\\n[CELL 08-09] Improvement over Vanilla MAML: +{(fewshot_acc - 0.2866)*100:.2f} pp\")\n",
    "print(f\"[CELL 08-09] Improvement over GRU4Rec:      {(fewshot_acc - 0.3355)*100:+.2f} pp\")\n",
    "\n",
    "cell_end(\"CELL 08-09\", t0, zeroshot_acc=zeroshot_acc, fewshot_acc=fewshot_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 08-10] start=2026-02-03T03:59:14 | Save results and report\n",
      "[CELL 08-10] Results saved: /workspace/anonymous-users-mooc-session-meta/results/warmstart_residual_maml_K5_Q10.json\n",
      "[CELL 08-10] Report saved: /workspace/anonymous-users-mooc-session-meta/reports/08_warmstart_maml_xuetangx/20260203_023949_b371f0db/report.json\n",
      "[CELL 08-10] done in 0.0s\n"
     ]
    }
   ],
   "source": [
    "# [CELL 08-10] Save results and report\n",
    "\n",
    "t0 = cell_start(\"CELL 08-10\", \"Save results and report\")\n",
    "\n",
    "OUT_RESULTS = RESULTS / \"warmstart_residual_maml_K5_Q10.json\"\n",
    "\n",
    "# Results\n",
    "results = {\n",
    "    \"model\": \"warmstart_residual_maml\",\n",
    "    \"contribution\": \"residual_warm_start_adaptation\",\n",
    "    \"dataset\": \"xuetangx\",\n",
    "    \"config\": CFG,\n",
    "    \"metrics\": {\n",
    "        \"zeroshot\": {\n",
    "            \"accuracy@1\": zeroshot_acc,\n",
    "        },\n",
    "        \"fewshot\": {\n",
    "            \"accuracy@1\": fewshot_acc,\n",
    "        },\n",
    "    },\n",
    "    \"comparison\": {\n",
    "        \"gru4rec_baseline\": 0.3355,\n",
    "        \"vanilla_maml_zeroshot\": 0.2562,\n",
    "        \"vanilla_maml_fewshot\": 0.2866,\n",
    "        \"naive_warmstart_maml_fewshot\": 0.24,  # Failed attempt\n",
    "        \"residual_warmstart_zeroshot\": zeroshot_acc,\n",
    "        \"residual_warmstart_fewshot\": fewshot_acc,\n",
    "    },\n",
    "    \"improvement\": {\n",
    "        \"over_vanilla_maml\": fewshot_acc - 0.2866,\n",
    "        \"over_gru4rec\": fewshot_acc - 0.3355,\n",
    "        \"over_naive_warmstart\": fewshot_acc - 0.24,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save results\n",
    "with open(OUT_RESULTS, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"[CELL 08-10] Results saved: {OUT_RESULTS}\")\n",
    "\n",
    "# Report\n",
    "report = {\n",
    "    \"notebook\": CFG[\"notebook\"],\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"config\": CFG,\n",
    "    \"results\": results,\n",
    "    \"history\": history,\n",
    "}\n",
    "\n",
    "with open(REPORT_PATH, \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "print(f\"[CELL 08-10] Report saved: {REPORT_PATH}\")\n",
    "\n",
    "cell_end(\"CELL 08-10\", t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Notebook 08 Complete: Residual Warm-Start MAML Results\n\n**Contribution 1:** Initialize MAML from pre-trained weights (FROZEN) and only learn/adapt residual delta weights.\n\n**Baseline:** Vanilla MAML = 30.52% Acc@1 (from Notebook 07)\n\n**Key Innovation:**\n```\nStandard MAML:     θ_random → meta-train → θ_meta\nNaive Warm-Start:  θ_pretrained → meta-train → θ_meta (OVERWRITES pretrained - FAILS!)\nResidual W-Start:  θ_pretrained (FROZEN) + Δθ (learnable) → meta-train Δθ only\n```\n\n**Results:**\n\n| Model | Zero-shot | Few-shot | vs Baseline |\n|-------|-----------|----------|-------------|\n| Vanilla MAML (baseline) | 23.50% | 30.52% | - |\n| Naive Warm-Start MAML | - | ~24% | -6.52 pp (FAILED) |\n| **Residual Warm-Start MAML** | **33.32%** | **34.95%** | **+4.43 pp** |\n\n**Key Findings:**\n1. **+4.43 pp improvement** over Vanilla MAML baseline (30.52% → 34.95%)\n2. **Zero-shot preserves pretrained:** 33.32% without any adaptation\n3. **Residual approach succeeds:** Unlike naive warm-start (~24%), residual adaptation prevents forgetting\n4. **Best validation accuracy:** 37.01% at iteration 100\n\n**Key Insight:** By freezing pretrained weights and only learning delta:\n1. Pre-trained knowledge is preserved (not overwritten by meta-training)\n2. Delta learns task-specific adaptations for personalization\n3. Even if inner-loop adaptation fails, base predictions remain strong\n\n**Similar to:** LoRA (Low-Rank Adaptation), Adapter layers, Residual fine-tuning\n\n**Next:** Notebook 09 - Recency-Weighted MAML (Contribution 2)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (VENV)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}