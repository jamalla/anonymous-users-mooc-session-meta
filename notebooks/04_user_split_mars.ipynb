{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef047c83",
   "metadata": {},
   "source": [
    "Goal (only): create a deterministic user-level split (train/val/test users disjoint), save the split files under:\n",
    "\n",
    "data/processed/mars/user_splits/\n",
    "\n",
    "users_train.json\n",
    "\n",
    "users_val.json\n",
    "\n",
    "users_test.json\n",
    "\n",
    "user_split_map.parquet\n",
    "\n",
    "pairs_train.parquet, pairs_val.parquet, pairs_test.parquet (optional but recommended for baselines)\n",
    "\n",
    "events_train.parquet, events_val.parquet, events_test.parquet (optional but recommended)\n",
    "\n",
    "No sessionization here (already done). No episodes here (Notebook 05)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d1264",
   "metadata": {},
   "source": [
    "Bootstrap + logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b419586b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 04-00] start=2026-01-06T22:30:03\n",
      "[CELL 04-00] CWD: C:\\anonymous-users-mooc-session-meta\\notebooks\n",
      "[CELL 04-00] REPO_ROOT: C:\\anonymous-users-mooc-session-meta\n",
      "[CELL 04-00] META_REGISTRY=C:\\anonymous-users-mooc-session-meta\\meta.json\n",
      "[CELL 04-00] DATA_INTERIM=C:\\anonymous-users-mooc-session-meta\\data\\interim\n",
      "[CELL 04-00] DATA_PROCESSED=C:\\anonymous-users-mooc-session-meta\\data\\processed\n",
      "[CELL 04-00] REPORTS=C:\\anonymous-users-mooc-session-meta\\reports\n",
      "[CELL 04-00] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-00] Bootstrap: repo root + paths + logger\n",
    "\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"[CELL 04-00] start={datetime.now().isoformat(timespec='seconds')}\")\n",
    "print(\"[CELL 04-00] CWD:\", Path.cwd().resolve())\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists():\n",
    "            return p\n",
    "    raise RuntimeError(\"Could not find PROJECT_STATE.md. Open notebook from within the repo.\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "print(\"[CELL 04-00] REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "PATHS = {\n",
    "    \"META_REGISTRY\": REPO_ROOT / \"meta.json\",\n",
    "    \"DATA_INTERIM\": REPO_ROOT / \"data\" / \"interim\",\n",
    "    \"DATA_PROCESSED\": REPO_ROOT / \"data\" / \"processed\",\n",
    "    \"REPORTS\": REPO_ROOT / \"reports\",\n",
    "}\n",
    "for k, v in PATHS.items():\n",
    "    print(f\"[CELL 04-00] {k}={v}\")\n",
    "\n",
    "def cell_start(cell_id: str, title: str, **kwargs: Any) -> float:\n",
    "    t = time.time()\n",
    "    print(f\"\\n[{cell_id}] {title}\")\n",
    "    print(f\"[{cell_id}] start={datetime.now().isoformat(timespec='seconds')}\")\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    return t\n",
    "\n",
    "def cell_end(cell_id: str, t0: float, **kwargs: Any) -> None:\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    print(f\"[{cell_id}] elapsed={time.time()-t0:.2f}s\")\n",
    "    print(f\"[{cell_id}] done\")\n",
    "\n",
    "print(\"[CELL 04-00] done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7be9c10",
   "metadata": {},
   "source": [
    "JSON IO (Timestamp-safe) + hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71fd3b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 04-01] JSON IO + hashing (Timestamp-safe)\n",
      "[CELL 04-01] start=2026-01-06T22:30:03\n",
      "[CELL 04-01] elapsed=0.00s\n",
      "[CELL 04-01] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-01] JSON IO + hashing (Timestamp-safe)\n",
    "\n",
    "t0 = cell_start(\"CELL 04-01\", \"JSON IO + hashing (Timestamp-safe)\")\n",
    "\n",
    "def _json_default(o):\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        if isinstance(o, (pd.Timestamp,)):\n",
    "            return o.isoformat()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        import numpy as np\n",
    "        if isinstance(o, (np.integer,)):\n",
    "            return int(o)\n",
    "        if isinstance(o, (np.floating,)):\n",
    "            return float(o)\n",
    "        if isinstance(o, (np.bool_,)):\n",
    "            return bool(o)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        from datetime import datetime, date\n",
    "        if isinstance(o, (datetime, date)):\n",
    "            return o.isoformat()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return str(o)\n",
    "\n",
    "def write_json_atomic(path: Path, obj: Any, indent: int = 2) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tmp = path.with_suffix(path.suffix + f\".tmp_{uuid.uuid4().hex}\")\n",
    "    with tmp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=indent, default=_json_default)\n",
    "    tmp.replace(path)\n",
    "\n",
    "def read_json(path: Path) -> Any:\n",
    "    if not path.exists():\n",
    "        raise RuntimeError(f\"Missing JSON file: {path}\")\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def safe_artifact_record(path: Path) -> Dict[str, Any]:\n",
    "    rec = {\"path\": str(path), \"bytes\": int(path.stat().st_size), \"sha256\": None, \"sha256_error\": None}\n",
    "    try:\n",
    "        rec[\"sha256\"] = sha256_file(path)\n",
    "    except PermissionError as e:\n",
    "        rec[\"sha256_error\"] = f\"PermissionError: {e}\"\n",
    "        print(\"[CELL 04-01] WARN: locked, cannot hash now:\", path)\n",
    "    return rec\n",
    "\n",
    "cell_end(\"CELL 04-01\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ef0f9f",
   "metadata": {},
   "source": [
    "Start run + init report/config/manifest + meta.json append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "189d4c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 04-02] Start run\n",
      "[CELL 04-02] start=2026-01-06T22:30:25\n",
      "[CELL 04-02] out_dir=C:\\anonymous-users-mooc-session-meta\\reports\\04_user_split_mars\\20260106_223025\n",
      "[CELL 04-02] elapsed=0.01s\n",
      "[CELL 04-02] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-02] Start run + init files + meta.json append-only\n",
    "\n",
    "t0 = cell_start(\"CELL 04-02\", \"Start run\")\n",
    "\n",
    "NOTEBOOK_NAME = \"04_user_split_mars\"\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_ID = uuid.uuid4().hex\n",
    "\n",
    "OUT_DIR = PATHS[\"REPORTS\"] / NOTEBOOK_NAME / RUN_TAG\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REPORT_PATH = OUT_DIR / \"report.json\"\n",
    "CONFIG_PATH = OUT_DIR / \"config.json\"\n",
    "MANIFEST_PATH = OUT_DIR / \"manifest.json\"\n",
    "\n",
    "DUCKDB_PATH = PATHS[\"DATA_INTERIM\"] / \"mars.duckdb\"\n",
    "EVENTS_VIEW = \"mars_events_sessionized\"\n",
    "PAIRS_VIEW = \"mars_pairs\"\n",
    "\n",
    "OUT_SPLIT_DIR = PATHS[\"DATA_PROCESSED\"] / \"mars\" / \"user_splits\"\n",
    "OUT_SPLIT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CFG = {\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"inputs\": {\"duckdb_path\": str(DUCKDB_PATH), \"events_view\": EVENTS_VIEW, \"pairs_view\": PAIRS_VIEW},\n",
    "    \"outputs\": {\"user_splits_dir\": str(OUT_SPLIT_DIR), \"reports_out_dir\": str(OUT_DIR)},\n",
    "    \"split\": {\n",
    "        \"seed\": 20260106,\n",
    "        \"train_frac\": 0.80,\n",
    "        \"val_frac\": 0.10,\n",
    "        \"test_frac\": 0.10,\n",
    "        \"strategy\": \"random_user_split\",   # deterministic shuffle\n",
    "    }\n",
    "}\n",
    "write_json_atomic(CONFIG_PATH, CFG)\n",
    "\n",
    "report = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"repo_root\": str(REPO_ROOT),\n",
    "    \"metrics\": {},\n",
    "    \"key_findings\": [],\n",
    "    \"sanity_samples\": {},\n",
    "    \"data_fingerprints\": {},\n",
    "    \"notes\": [],\n",
    "}\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "manifest = {\"run_id\": RUN_ID, \"notebook\": NOTEBOOK_NAME, \"run_tag\": RUN_TAG, \"artifacts\": []}\n",
    "write_json_atomic(MANIFEST_PATH, manifest)\n",
    "\n",
    "# meta.json append-only\n",
    "META_PATH = PATHS[\"META_REGISTRY\"]\n",
    "if not META_PATH.exists():\n",
    "    write_json_atomic(META_PATH, {\"schema_version\": 1, \"runs\": []})\n",
    "meta = read_json(META_PATH)\n",
    "meta[\"runs\"].append({\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"out_dir\": str(OUT_DIR),\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "})\n",
    "write_json_atomic(META_PATH, meta)\n",
    "\n",
    "cell_end(\"CELL 04-02\", t0, out_dir=str(OUT_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb4d93a",
   "metadata": {},
   "source": [
    "Load user stats from DuckDB (events + pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf33ebd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 04-03] Load user stats from DuckDB\n",
      "[CELL 04-03] start=2026-01-06T22:30:42\n",
      "[CELL 04-03] user_stats shape: (822, 4)\n",
      "[CELL 04-03] head(5):\n",
      " user_id  n_events  n_sessions  n_pairs\n",
      "     672         3           1        2\n",
      "     856         1           1        0\n",
      "    3928         1           1        0\n",
      "    4160         1           1        0\n",
      "    4448         2           1        1\n",
      "[CELL 04-03] stats_summary: {'n_users': 822, 'events': {'min': 1.0, 'p50': 2.0, 'p90': 8.0, 'p95': 16.0, 'p99': 59.6899999999996, 'max': 134.0, 'mean': 4.451338199513382}, 'sessions': {'min': 1.0, 'p50': 1.0, 'p90': 3.0, 'p95': 4.0, 'p99': 8.789999999999964, 'max': 39.0, 'mean': 1.608272506082725}, 'pairs': {'min': 0.0, 'p50': 0.0, 'p90': 5.899999999999977, 'p95': 11.949999999999932, 'p99': 49.789999999999964, 'max': 123.0, 'mean': 2.843065693430657}}\n",
      "[CELL 04-03] n_users=822\n",
      "[CELL 04-03] elapsed=0.15s\n",
      "[CELL 04-03] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-03] Load user stats (events + sessions + pairs)\n",
    "\n",
    "t0 = cell_start(\"CELL 04-03\", \"Load user stats from DuckDB\")\n",
    "\n",
    "import duckdb\n",
    "\n",
    "if not Path(DUCKDB_PATH).exists():\n",
    "    raise RuntimeError(f\"Missing DuckDB: {DUCKDB_PATH}. Run Notebooks 01-03 first.\")\n",
    "\n",
    "con = duckdb.connect(str(DUCKDB_PATH), read_only=True)\n",
    "\n",
    "# basic existence checks\n",
    "for view in [EVENTS_VIEW, PAIRS_VIEW]:\n",
    "    try:\n",
    "        con.execute(f\"SELECT COUNT(*) FROM {view}\").fetchone()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Missing view {view}. Ensure Notebook 02/03 completed.\") from e\n",
    "\n",
    "user_stats = con.execute(f\"\"\"\n",
    "SELECT\n",
    "  e.user_id AS user_id,\n",
    "  COUNT(*) AS n_events,\n",
    "  COUNT(DISTINCT e.session_id) AS n_sessions,\n",
    "  COALESCE(p.n_pairs, 0) AS n_pairs\n",
    "FROM {EVENTS_VIEW} e\n",
    "LEFT JOIN (\n",
    "  SELECT user_id, COUNT(*) AS n_pairs\n",
    "  FROM {PAIRS_VIEW}\n",
    "  GROUP BY 1\n",
    ") p\n",
    "ON e.user_id = p.user_id\n",
    "GROUP BY 1, p.n_pairs\n",
    "ORDER BY user_id\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "if user_stats.shape[0] == 0:\n",
    "    raise RuntimeError(\"No users found in events view. Cannot split.\")\n",
    "\n",
    "print(\"[CELL 04-03] user_stats shape:\", user_stats.shape)\n",
    "print(\"[CELL 04-03] head(5):\")\n",
    "print(user_stats.head(5).to_string(index=False))\n",
    "\n",
    "# quick quantiles for report\n",
    "def q(series: pd.Series) -> Dict[str, float]:\n",
    "    s = series.astype(float)\n",
    "    return {\n",
    "        \"min\": float(s.min()),\n",
    "        \"p50\": float(s.quantile(0.50)),\n",
    "        \"p90\": float(s.quantile(0.90)),\n",
    "        \"p95\": float(s.quantile(0.95)),\n",
    "        \"p99\": float(s.quantile(0.99)),\n",
    "        \"max\": float(s.max()),\n",
    "        \"mean\": float(s.mean()),\n",
    "    }\n",
    "\n",
    "stats_summary = {\n",
    "    \"n_users\": int(user_stats.shape[0]),\n",
    "    \"events\": q(user_stats[\"n_events\"]),\n",
    "    \"sessions\": q(user_stats[\"n_sessions\"]),\n",
    "    \"pairs\": q(user_stats[\"n_pairs\"]),\n",
    "}\n",
    "print(\"[CELL 04-03] stats_summary:\", stats_summary)\n",
    "\n",
    "cell_end(\"CELL 04-03\", t0, n_users=int(user_stats.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e70c42",
   "metadata": {},
   "source": [
    "Deterministic user split (train/val/test disjoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17b31e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 04-04] Create deterministic user split\n",
      "[CELL 04-04] start=2026-01-06T22:31:03\n",
      "[CELL 04-04] n_users_total: 822\n",
      "[CELL 04-04] n_train: 658\n",
      "[CELL 04-04] n_val: 82\n",
      "[CELL 04-04] n_test: 82\n",
      "[CELL 04-04] n_train=658\n",
      "[CELL 04-04] n_val=82\n",
      "[CELL 04-04] n_test=82\n",
      "[CELL 04-04] elapsed=0.00s\n",
      "[CELL 04-04] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-04] Deterministic user split (disjoint users)\n",
    "\n",
    "t0 = cell_start(\"CELL 04-04\", \"Create deterministic user split\")\n",
    "\n",
    "seed = int(CFG[\"split\"][\"seed\"])\n",
    "train_frac = float(CFG[\"split\"][\"train_frac\"])\n",
    "val_frac = float(CFG[\"split\"][\"val_frac\"])\n",
    "test_frac = float(CFG[\"split\"][\"test_frac\"])\n",
    "\n",
    "if not np.isclose(train_frac + val_frac + test_frac, 1.0):\n",
    "    raise RuntimeError(\"train/val/test fractions must sum to 1.0\")\n",
    "\n",
    "users = user_stats[\"user_id\"].astype(str).tolist()\n",
    "rng = np.random.default_rng(seed)\n",
    "perm = rng.permutation(len(users))\n",
    "users_shuf = [users[i] for i in perm]\n",
    "\n",
    "n = len(users_shuf)\n",
    "n_train = int(round(n * train_frac))\n",
    "n_val = int(round(n * val_frac))\n",
    "# ensure exact partition\n",
    "n_train = min(n_train, n)\n",
    "n_val = min(n_val, n - n_train)\n",
    "n_test = n - n_train - n_val\n",
    "\n",
    "train_users = users_shuf[:n_train]\n",
    "val_users = users_shuf[n_train:n_train + n_val]\n",
    "test_users = users_shuf[n_train + n_val:]\n",
    "\n",
    "# checks\n",
    "a, b, c = set(train_users), set(val_users), set(test_users)\n",
    "if (a & b) or (a & c) or (b & c):\n",
    "    raise RuntimeError(\"User split overlap detected (should be impossible).\")\n",
    "if len(a) + len(b) + len(c) != n:\n",
    "    raise RuntimeError(\"User split sizes do not sum to total users.\")\n",
    "\n",
    "print(\"[CELL 04-04] n_users_total:\", n)\n",
    "print(\"[CELL 04-04] n_train:\", len(train_users))\n",
    "print(\"[CELL 04-04] n_val:\", len(val_users))\n",
    "print(\"[CELL 04-04] n_test:\", len(test_users))\n",
    "\n",
    "cell_end(\"CELL 04-04\", t0, n_train=len(train_users), n_val=len(val_users), n_test=len(test_users))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ed546",
   "metadata": {},
   "source": [
    "Save split files + split map parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1e005cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 04-05] Write user split artifacts\n",
      "[CELL 04-05] start=2026-01-06T22:31:22\n",
      "[CELL 04-05] wrote: C:\\anonymous-users-mooc-session-meta\\data\\processed\\mars\\user_splits\\users_train.json\n",
      "[CELL 04-05] wrote: C:\\anonymous-users-mooc-session-meta\\data\\processed\\mars\\user_splits\\users_val.json\n",
      "[CELL 04-05] wrote: C:\\anonymous-users-mooc-session-meta\\data\\processed\\mars\\user_splits\\users_test.json\n",
      "[CELL 04-05] wrote: C:\\anonymous-users-mooc-session-meta\\data\\processed\\mars\\user_splits\\user_split_map.parquet\n",
      "[CELL 04-05] elapsed=0.20s\n",
      "[CELL 04-05] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-05] Save split files + split map\n",
    "\n",
    "t0 = cell_start(\"CELL 04-05\", \"Write user split artifacts\")\n",
    "\n",
    "train_path = OUT_SPLIT_DIR / \"users_train.json\"\n",
    "val_path = OUT_SPLIT_DIR / \"users_val.json\"\n",
    "test_path = OUT_SPLIT_DIR / \"users_test.json\"\n",
    "split_map_path = OUT_SPLIT_DIR / \"user_split_map.parquet\"\n",
    "\n",
    "write_json_atomic(train_path, train_users)\n",
    "write_json_atomic(val_path, val_users)\n",
    "write_json_atomic(test_path, test_users)\n",
    "\n",
    "split_map = pd.DataFrame({\n",
    "    \"user_id\": train_users + val_users + test_users,\n",
    "    \"split\": ([\"train\"] * len(train_users)) + ([\"val\"] * len(val_users)) + ([\"test\"] * len(test_users))\n",
    "})\n",
    "split_map.to_parquet(split_map_path, index=False, compression=\"zstd\")\n",
    "\n",
    "print(\"[CELL 04-05] wrote:\", train_path)\n",
    "print(\"[CELL 04-05] wrote:\", val_path)\n",
    "print(\"[CELL 04-05] wrote:\", test_path)\n",
    "print(\"[CELL 04-05] wrote:\", split_map_path)\n",
    "\n",
    "cell_end(\"CELL 04-05\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a18d62",
   "metadata": {},
   "source": [
    "Materialize split-specific Parquets (events + pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0136b1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 04-06] Write split Parquets for events + pairs\n",
      "[CELL 04-06] start=2026-01-06T22:33:15\n",
      "[CELL 04-06] wrote: C:\\anonymous-users-mooc-session-meta\\data\\processed\\mars\\user_splits\\events_train.parquet\n",
      "[CELL 04-06] wrote: C:\\anonymous-users-mooc-session-meta\\data\\processed\\mars\\user_splits\\events_val.parquet\n",
      "[CELL 04-06] wrote: C:\\anonymous-users-mooc-session-meta\\data\\processed\\mars\\user_splits\\events_test.parquet\n",
      "[CELL 04-06] wrote: C:\\anonymous-users-mooc-session-meta\\data\\processed\\mars\\user_splits\\pairs_train.parquet\n",
      "[CELL 04-06] wrote: C:\\anonymous-users-mooc-session-meta\\data\\processed\\mars\\user_splits\\pairs_val.parquet\n",
      "[CELL 04-06] wrote: C:\\anonymous-users-mooc-session-meta\\data\\processed\\mars\\user_splits\\pairs_test.parquet\n",
      "[CELL 04-06] elapsed=0.08s\n",
      "[CELL 04-06] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-06] Write split-specific Parquets (events + pairs)  ✅ FIXED (no f-string backslash)\n",
    "\n",
    "t0 = cell_start(\"CELL 04-06\", \"Write split Parquets for events + pairs\")\n",
    "\n",
    "events_train_out = OUT_SPLIT_DIR / \"events_train.parquet\"\n",
    "events_val_out   = OUT_SPLIT_DIR / \"events_val.parquet\"\n",
    "events_test_out  = OUT_SPLIT_DIR / \"events_test.parquet\"\n",
    "\n",
    "pairs_train_out = OUT_SPLIT_DIR / \"pairs_train.parquet\"\n",
    "pairs_val_out   = OUT_SPLIT_DIR / \"pairs_val.parquet\"\n",
    "pairs_test_out  = OUT_SPLIT_DIR / \"pairs_test.parquet\"\n",
    "\n",
    "def esc_sql_str(s: str) -> str:\n",
    "    # Escape single quotes for SQL string literals\n",
    "    return s.replace(\"'\", \"''\")\n",
    "\n",
    "def users_values_sql(users_list: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Returns a SQL subquery:\n",
    "      SELECT * FROM (VALUES ('u1'),('u2'),...) AS t(user_id)\n",
    "    If empty: returns SELECT NULL WHERE FALSE\n",
    "    \"\"\"\n",
    "    if len(users_list) == 0:\n",
    "        return \"SELECT NULL AS user_id WHERE FALSE\"\n",
    "    rows = \",\".join([\"('\" + esc_sql_str(u) + \"')\" for u in users_list])\n",
    "    return f\"SELECT * FROM (VALUES {rows}) AS t(user_id)\"\n",
    "\n",
    "def esc_path(p: Path) -> str:\n",
    "    return esc_sql_str(str(p))\n",
    "\n",
    "train_users_sql = users_values_sql(train_users)\n",
    "val_users_sql   = users_values_sql(val_users)\n",
    "test_users_sql  = users_values_sql(test_users)\n",
    "\n",
    "# Export from DuckDB (fast, consistent)\n",
    "con.execute(f\"\"\"\n",
    "COPY (\n",
    "  SELECT e.*\n",
    "  FROM {EVENTS_VIEW} e\n",
    "  JOIN ({train_users_sql}) u ON CAST(e.user_id AS VARCHAR) = u.user_id\n",
    ") TO '{esc_path(events_train_out)}' (FORMAT PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "COPY (\n",
    "  SELECT e.*\n",
    "  FROM {EVENTS_VIEW} e\n",
    "  JOIN ({val_users_sql}) u ON CAST(e.user_id AS VARCHAR) = u.user_id\n",
    ") TO '{esc_path(events_val_out)}' (FORMAT PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "COPY (\n",
    "  SELECT e.*\n",
    "  FROM {EVENTS_VIEW} e\n",
    "  JOIN ({test_users_sql}) u ON CAST(e.user_id AS VARCHAR) = u.user_id\n",
    ") TO '{esc_path(events_test_out)}' (FORMAT PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "COPY (\n",
    "  SELECT p.*\n",
    "  FROM {PAIRS_VIEW} p\n",
    "  JOIN ({train_users_sql}) u ON CAST(p.user_id AS VARCHAR) = u.user_id\n",
    ") TO '{esc_path(pairs_train_out)}' (FORMAT PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "COPY (\n",
    "  SELECT p.*\n",
    "  FROM {PAIRS_VIEW} p\n",
    "  JOIN ({val_users_sql}) u ON CAST(p.user_id AS VARCHAR) = u.user_id\n",
    ") TO '{esc_path(pairs_val_out)}' (FORMAT PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "COPY (\n",
    "  SELECT p.*\n",
    "  FROM {PAIRS_VIEW} p\n",
    "  JOIN ({test_users_sql}) u ON CAST(p.user_id AS VARCHAR) = u.user_id\n",
    ") TO '{esc_path(pairs_test_out)}' (FORMAT PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "print(\"[CELL 04-06] wrote:\", events_train_out)\n",
    "print(\"[CELL 04-06] wrote:\", events_val_out)\n",
    "print(\"[CELL 04-06] wrote:\", events_test_out)\n",
    "print(\"[CELL 04-06] wrote:\", pairs_train_out)\n",
    "print(\"[CELL 04-06] wrote:\", pairs_val_out)\n",
    "print(\"[CELL 04-06] wrote:\", pairs_test_out)\n",
    "\n",
    "cell_end(\"CELL 04-06\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8047d090",
   "metadata": {},
   "source": [
    "Leakage checks (users disjoint + row counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e7b3b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 04-07] Leakage checks (disjoint users)\n",
      "[CELL 04-07] start=2026-01-06T22:35:32\n",
      "[CELL 04-07] using existing DuckDB connection: con\n",
      "[CELL 04-07] overlap train-val: 0\n",
      "[CELL 04-07] overlap train-test: 0\n",
      "[CELL 04-07] overlap val-test: 0\n",
      "[CELL 04-07] events counts: {'train': 3016, 'val': 300, 'test': 343}\n",
      "[CELL 04-07] n_ev_tr=3016\n",
      "[CELL 04-07] n_ev_va=300\n",
      "[CELL 04-07] n_ev_te=343\n",
      "[CELL 04-07] elapsed=0.04s\n",
      "[CELL 04-07] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-07] Leakage checks: disjoint users + counts  ✅ FIXED (reuse existing con; no new connection)\n",
    "\n",
    "t0 = cell_start(\"CELL 04-07\", \"Leakage checks (disjoint users)\")\n",
    "\n",
    "def esc_sql_str(s: str) -> str:\n",
    "    return s.replace(\"'\", \"''\")\n",
    "\n",
    "def esc_path(p: Path) -> str:\n",
    "    return esc_sql_str(str(p))\n",
    "\n",
    "# Ensure split_map has unique users\n",
    "assert split_map[\"user_id\"].nunique() == split_map.shape[0], \"Duplicate user_id in split_map\"\n",
    "\n",
    "# Reuse existing connection `con` if it exists and is open; otherwise open read-only\n",
    "try:\n",
    "    con.execute(\"SELECT 1\").fetchone()\n",
    "    print(\"[CELL 04-07] using existing DuckDB connection: con\")\n",
    "except Exception:\n",
    "    import duckdb\n",
    "    con = duckdb.connect(str(DUCKDB_PATH), read_only=True)\n",
    "    print(\"[CELL 04-07] opened DuckDB connection: con (read_only=True)\")\n",
    "\n",
    "ev_tr = esc_path(events_train_out)\n",
    "ev_va = esc_path(events_val_out)\n",
    "ev_te = esc_path(events_test_out)\n",
    "\n",
    "# Compute overlaps directly from Parquet (no CREATE VIEW needed)\n",
    "overlaps = con.execute(f\"\"\"\n",
    "WITH\n",
    "tr AS (SELECT DISTINCT CAST(user_id AS VARCHAR) AS user_id FROM read_parquet('{ev_tr}')),\n",
    "va AS (SELECT DISTINCT CAST(user_id AS VARCHAR) AS user_id FROM read_parquet('{ev_va}')),\n",
    "te AS (SELECT DISTINCT CAST(user_id AS VARCHAR) AS user_id FROM read_parquet('{ev_te}'))\n",
    "SELECT\n",
    "  (SELECT COUNT(*) FROM tr JOIN va USING(user_id)) AS over_tr_va,\n",
    "  (SELECT COUNT(*) FROM tr JOIN te USING(user_id)) AS over_tr_te,\n",
    "  (SELECT COUNT(*) FROM va JOIN te USING(user_id)) AS over_va_te\n",
    "\"\"\").fetchdf().iloc[0].to_dict()\n",
    "\n",
    "over_tr_va = int(overlaps[\"over_tr_va\"])\n",
    "over_tr_te = int(overlaps[\"over_tr_te\"])\n",
    "over_va_te = int(overlaps[\"over_va_te\"])\n",
    "\n",
    "print(\"[CELL 04-07] overlap train-val:\", over_tr_va)\n",
    "print(\"[CELL 04-07] overlap train-test:\", over_tr_te)\n",
    "print(\"[CELL 04-07] overlap val-test:\", over_va_te)\n",
    "\n",
    "if over_tr_va or over_tr_te or over_va_te:\n",
    "    raise RuntimeError(\"User leakage detected across splits (should be 0).\")\n",
    "\n",
    "# Row counts\n",
    "n_ev_tr = int(con.execute(f\"SELECT COUNT(*) FROM read_parquet('{ev_tr}')\").fetchone()[0])\n",
    "n_ev_va = int(con.execute(f\"SELECT COUNT(*) FROM read_parquet('{ev_va}')\").fetchone()[0])\n",
    "n_ev_te = int(con.execute(f\"SELECT COUNT(*) FROM read_parquet('{ev_te}')\").fetchone()[0])\n",
    "\n",
    "print(\"[CELL 04-07] events counts:\", {\"train\": n_ev_tr, \"val\": n_ev_va, \"test\": n_ev_te})\n",
    "\n",
    "cell_end(\"CELL 04-07\", t0, n_ev_tr=n_ev_tr, n_ev_va=n_ev_va, n_ev_te=n_ev_te)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7631c09",
   "metadata": {},
   "source": [
    "Register DuckDB views for split datasets + close connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4431f7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 04-08] Register split views in DuckDB\n",
      "[CELL 04-08] start=2026-01-06T22:35:51\n",
      "[CELL 04-08] view row checks: {'pairs_train': 1932, 'pairs_val': 191, 'pairs_test': 214}\n",
      "[CELL 04-08] closed DuckDB connection\n",
      "[CELL 04-08] elapsed=0.09s\n",
      "[CELL 04-08] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-08] Register DuckDB views for split datasets\n",
    "\n",
    "t0 = cell_start(\"CELL 04-08\", \"Register split views in DuckDB\")\n",
    "\n",
    "# Close the read-only con from earlier (avoid lock)\n",
    "try:\n",
    "    con.close()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import duckdb\n",
    "conw = duckdb.connect(str(DUCKDB_PATH), read_only=False)\n",
    "\n",
    "def esc(p: Path) -> str:\n",
    "    return str(p).replace(\"'\", \"''\")\n",
    "\n",
    "# Drop and re-create stable views\n",
    "for v in [\"mars_events_train\",\"mars_events_val\",\"mars_events_test\",\"mars_pairs_train\",\"mars_pairs_val\",\"mars_pairs_test\"]:\n",
    "    conw.execute(f\"DROP VIEW IF EXISTS {v};\")\n",
    "\n",
    "conw.execute(f\"CREATE VIEW mars_events_train AS SELECT * FROM read_parquet('{esc(events_train_out)}');\")\n",
    "conw.execute(f\"CREATE VIEW mars_events_val   AS SELECT * FROM read_parquet('{esc(events_val_out)}');\")\n",
    "conw.execute(f\"CREATE VIEW mars_events_test  AS SELECT * FROM read_parquet('{esc(events_test_out)}');\")\n",
    "\n",
    "conw.execute(f\"CREATE VIEW mars_pairs_train AS SELECT * FROM read_parquet('{esc(pairs_train_out)}');\")\n",
    "conw.execute(f\"CREATE VIEW mars_pairs_val   AS SELECT * FROM read_parquet('{esc(pairs_val_out)}');\")\n",
    "conw.execute(f\"CREATE VIEW mars_pairs_test  AS SELECT * FROM read_parquet('{esc(pairs_test_out)}');\")\n",
    "\n",
    "# quick check\n",
    "chk = {\n",
    "    \"pairs_train\": int(conw.execute(\"SELECT COUNT(*) FROM mars_pairs_train\").fetchone()[0]),\n",
    "    \"pairs_val\": int(conw.execute(\"SELECT COUNT(*) FROM mars_pairs_val\").fetchone()[0]),\n",
    "    \"pairs_test\": int(conw.execute(\"SELECT COUNT(*) FROM mars_pairs_test\").fetchone()[0]),\n",
    "}\n",
    "print(\"[CELL 04-08] view row checks:\", chk)\n",
    "\n",
    "conw.close()\n",
    "print(\"[CELL 04-08] closed DuckDB connection\")\n",
    "\n",
    "cell_end(\"CELL 04-08\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73802211",
   "metadata": {},
   "source": [
    "Update report + manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d8b0b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 04-09] Write report + manifest\n",
      "[CELL 04-09] start=2026-01-06T22:36:25\n",
      "[CELL 04-09] updated: C:\\anonymous-users-mooc-session-meta\\reports\\04_user_split_mars\\20260106_223025\\report.json\n",
      "[CELL 04-09] updated: C:\\anonymous-users-mooc-session-meta\\reports\\04_user_split_mars\\20260106_223025\\manifest.json\n",
      "[CELL 04-09] n_artifacts=10\n",
      "[CELL 04-09] elapsed=0.06s\n",
      "[CELL 04-09] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 04-09] Write report + manifest\n",
    "\n",
    "t0 = cell_start(\"CELL 04-09\", \"Write report + manifest\")\n",
    "\n",
    "report = read_json(REPORT_PATH)\n",
    "manifest = read_json(MANIFEST_PATH)\n",
    "\n",
    "# artifacts\n",
    "for p in [train_path, val_path, test_path, split_map_path,\n",
    "          events_train_out, events_val_out, events_test_out,\n",
    "          pairs_train_out, pairs_val_out, pairs_test_out]:\n",
    "    manifest[\"artifacts\"].append(safe_artifact_record(p))\n",
    "\n",
    "report[\"key_findings\"].append(\"Created deterministic user-level train/val/test splits with disjoint users and exported split-specific events/pairs parquets.\")\n",
    "report[\"sanity_samples\"][\"user_stats_summary\"] = stats_summary\n",
    "report[\"sanity_samples\"][\"split_sizes\"] = {\"train\": len(train_users), \"val\": len(val_users), \"test\": len(test_users)}\n",
    "report[\"sanity_samples\"][\"split_users_head\"] = {\n",
    "    \"train\": train_users[:5],\n",
    "    \"val\": val_users[:5],\n",
    "    \"test\": test_users[:5],\n",
    "}\n",
    "report[\"notes\"].append(\"User-level split is mandatory: train/val/test users are disjoint. Item identities are not treated as cold-start.\")\n",
    "report[\"notes\"].append(\"Episode eligibility (min interactions for K-shot + query) will be enforced in Notebook 05, not during splitting.\")\n",
    "\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "write_json_atomic(MANIFEST_PATH, manifest)\n",
    "\n",
    "print(\"[CELL 04-09] updated:\", REPORT_PATH)\n",
    "print(\"[CELL 04-09] updated:\", MANIFEST_PATH)\n",
    "\n",
    "cell_end(\"CELL 04-09\", t0, n_artifacts=len(manifest[\"artifacts\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
