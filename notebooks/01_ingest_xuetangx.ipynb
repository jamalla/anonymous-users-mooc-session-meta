{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 01: Ingest XuetangX\n\n**Purpose:** Parse raw XuetangX JSON file → normalized Parquet → DuckDB view.\n\n**Inputs:**\n- `data/raw/xuetangx/20170201-20170801-raw_user_activity.json` (~5GB, Feb-Aug 2017)\n\n**Outputs:**\n- `data/interim/xuetangx_events_raw.parquet` (28M learning events)\n- `data/interim/xuetangx.duckdb` (view: `xuetangx_events_raw`)\n- `reports/01_ingest_xuetangx/<run_tag>/report.json`\n\n**Strategy:**\n- Use newest data file (Feb 2017 - Aug 2017)\n- Keep platform sessions (session_hash)\n- Filter learning events only (load_*, problem_*, video_*, etc.)\n- JSON → Parquet → DuckDB (reproducible, Windows-safe)"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 01-00] start=2026-02-01T02:01:35\n",
      "[CELL 01-00] CWD: /workspace/anonymous-users-mooc-session-meta/notebooks\n",
      "[CELL 01-00] REPO_ROOT: /workspace/anonymous-users-mooc-session-meta\n",
      "[CELL 01-00] PROJECT_STATE=/workspace/anonymous-users-mooc-session-meta/PROJECT_STATE.md\n",
      "[CELL 01-00] META_REGISTRY=/workspace/anonymous-users-mooc-session-meta/meta.json\n",
      "[CELL 01-00] DATA_RAW=/workspace/anonymous-users-mooc-session-meta/data/raw\n",
      "[CELL 01-00] DATA_INTERIM=/workspace/anonymous-users-mooc-session-meta/data/interim\n",
      "[CELL 01-00] DATA_PROCESSED=/workspace/anonymous-users-mooc-session-meta/data/processed\n",
      "[CELL 01-00] REPORTS=/workspace/anonymous-users-mooc-session-meta/reports\n",
      "[CELL 01-00] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 01-00] Bootstrap: repo root + paths + logger\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "t0 = datetime.now()\n",
    "print(f\"[CELL 01-00] start={t0.isoformat(timespec='seconds')}\")\n",
    "print(\"[CELL 01-00] CWD:\", Path.cwd().resolve())\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists():\n",
    "            return p\n",
    "    raise RuntimeError(\"Could not find PROJECT_STATE.md. Open notebook from within the repo.\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "print(\"[CELL 01-00] REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "PATHS = {\n",
    "    \"PROJECT_STATE\": REPO_ROOT / \"PROJECT_STATE.md\",\n",
    "    \"META_REGISTRY\": REPO_ROOT / \"meta.json\",\n",
    "    \"DATA_RAW\": REPO_ROOT / \"data\" / \"raw\",\n",
    "    \"DATA_INTERIM\": REPO_ROOT / \"data\" / \"interim\",\n",
    "    \"DATA_PROCESSED\": REPO_ROOT / \"data\" / \"processed\",\n",
    "    \"REPORTS\": REPO_ROOT / \"reports\",\n",
    "}\n",
    "for k, v in PATHS.items():\n",
    "    print(f\"[CELL 01-00] {k}={v}\")\n",
    "\n",
    "def cell_start(cell_id: str, title: str, **kwargs: Any) -> float:\n",
    "    t = time.time()\n",
    "    print(f\"\\n[{cell_id}] {title}\")\n",
    "    print(f\"[{cell_id}] start={datetime.now().isoformat(timespec='seconds')}\")\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    return t\n",
    "\n",
    "def cell_end(cell_id: str, t0: float, **kwargs: Any) -> None:\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    print(f\"[{cell_id}] elapsed={time.time()-t0:.2f}s\")\n",
    "    print(f\"[{cell_id}] done\")\n",
    "\n",
    "print(\"[CELL 01-00] done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 01-01] Seed everything\n",
      "[CELL 01-01] start=2026-02-01T02:01:35\n",
      "[CELL 01-01] seed=20260107\n",
      "[CELL 01-01] elapsed=0.00s\n",
      "[CELL 01-01] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 01-01] Reproducibility: seed everything\n",
    "\n",
    "t0 = cell_start(\"CELL 01-01\", \"Seed everything\")\n",
    "\n",
    "GLOBAL_SEED = 20260107  # New seed for XuetangX pipeline\n",
    "\n",
    "def seed_everything(seed: int) -> None:\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(GLOBAL_SEED)\n",
    "\n",
    "cell_end(\"CELL 01-01\", t0, seed=GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 01-02] JSON IO + hashing\n",
      "[CELL 01-02] start=2026-02-01T02:01:35\n",
      "[CELL 01-02] elapsed=0.00s\n",
      "[CELL 01-02] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 01-02] JSON IO + hashing helpers\n",
    "\n",
    "t0 = cell_start(\"CELL 01-02\", \"JSON IO + hashing\")\n",
    "\n",
    "def write_json_atomic(path: Path, obj: Any, indent: int = 2) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tmp = path.with_suffix(path.suffix + f\".tmp_{uuid.uuid4().hex}\")\n",
    "    with tmp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=indent)\n",
    "    tmp.replace(path)\n",
    "\n",
    "def read_json(path: Path) -> Any:\n",
    "    if not path.exists():\n",
    "        raise RuntimeError(f\"Missing JSON file: {path}\")\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def assert_nonempty_df(df: pd.DataFrame, name: str) -> None:\n",
    "    if df is None or not isinstance(df, pd.DataFrame) or df.shape[0] == 0:\n",
    "        raise RuntimeError(f\"{name} is empty or invalid DataFrame\")\n",
    "\n",
    "cell_end(\"CELL 01-02\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 01-03] Start run + init run files + meta.json\n",
      "[CELL 01-03] start=2026-02-01T02:01:35\n",
      "[CELL 01-03] out_dir=/workspace/anonymous-users-mooc-session-meta/reports/01_ingest_xuetangx/20260201_020135\n",
      "[CELL 01-03] report=/workspace/anonymous-users-mooc-session-meta/reports/01_ingest_xuetangx/20260201_020135/report.json\n",
      "[CELL 01-03] config=/workspace/anonymous-users-mooc-session-meta/reports/01_ingest_xuetangx/20260201_020135/config.json\n",
      "[CELL 01-03] manifest=/workspace/anonymous-users-mooc-session-meta/reports/01_ingest_xuetangx/20260201_020135/manifest.json\n",
      "[CELL 01-03] meta=/workspace/anonymous-users-mooc-session-meta/meta.json\n",
      "[CELL 01-03] elapsed=0.00s\n",
      "[CELL 01-03] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 01-03] Run tagging + report/config/manifest + meta.json\n",
    "\n",
    "t0 = cell_start(\"CELL 01-03\", \"Start run + init run files + meta.json\")\n",
    "\n",
    "NOTEBOOK_NAME = \"01_ingest_xuetangx\"\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_ID = uuid.uuid4().hex\n",
    "\n",
    "OUT_DIR = PATHS[\"REPORTS\"] / NOTEBOOK_NAME / RUN_TAG\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REPORT_PATH = OUT_DIR / \"report.json\"\n",
    "CONFIG_PATH = OUT_DIR / \"config.json\"\n",
    "MANIFEST_PATH = OUT_DIR / \"manifest.json\"\n",
    "\n",
    "RAW_DIR = PATHS[\"DATA_RAW\"] / \"xuetangx\"\n",
    "OUT_PARQUET = PATHS[\"DATA_INTERIM\"] / \"xuetangx_events_raw.parquet\"\n",
    "OUT_DUCKDB = PATHS[\"DATA_INTERIM\"] / \"xuetangx.duckdb\"\n",
    "\n",
    "CFG = {\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"seed\": GLOBAL_SEED,\n",
    "    \"paths\": {\n",
    "        \"raw_dir\": str(RAW_DIR),\n",
    "        \"out_parquet\": str(OUT_PARQUET),\n",
    "        \"out_duckdb\": str(OUT_DUCKDB),\n",
    "        \"out_dir\": str(OUT_DIR),\n",
    "    },\n",
    "    \"ingest\": {\n",
    "        \"prototype_mode\": True,  # Start with 1 file\n",
    "        \"prototype_file\": \"20170201-20170801-raw_user_activity.json\",  # NEWEST FILE\n",
    "        \"all_files_pattern\": \"*-raw_user_activity.json\",\n",
    "        \"learning_events_only\": True,  # Filter to learning behavior\n",
    "        \"learning_event_prefixes\": [\"load_\", \"problem_\", \"video_\", \"seek_\", \"speed_\", \"pause_\"],\n",
    "        \"parquet_compression\": \"zstd\",\n",
    "        \"duckdb_view\": \"xuetangx_events_raw\",\n",
    "    }\n",
    "}\n",
    "\n",
    "write_json_atomic(CONFIG_PATH, CFG)\n",
    "\n",
    "report = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"repo_root\": str(REPO_ROOT),\n",
    "    \"metrics\": {},\n",
    "    \"key_findings\": [],\n",
    "    \"sanity_samples\": {},\n",
    "    \"data_fingerprints\": {},\n",
    "    \"notes\": [],\n",
    "}\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "manifest = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"artifacts\": [],\n",
    "}\n",
    "write_json_atomic(MANIFEST_PATH, manifest)\n",
    "\n",
    "# meta.json append-only\n",
    "META_PATH = PATHS[\"META_REGISTRY\"]\n",
    "if not META_PATH.exists():\n",
    "    write_json_atomic(META_PATH, {\"schema_version\": 1, \"runs\": []})\n",
    "\n",
    "meta = read_json(META_PATH)\n",
    "if \"runs\" not in meta or not isinstance(meta[\"runs\"], list):\n",
    "    raise RuntimeError(\"meta.json invalid: missing 'runs' list\")\n",
    "\n",
    "meta[\"runs\"].append({\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"out_dir\": str(OUT_DIR),\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "})\n",
    "write_json_atomic(META_PATH, meta)\n",
    "\n",
    "cell_end(\"CELL 01-03\", t0,\n",
    "         out_dir=str(OUT_DIR),\n",
    "         report=str(REPORT_PATH),\n",
    "         config=str(CONFIG_PATH),\n",
    "         manifest=str(MANIFEST_PATH),\n",
    "         meta=str(META_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 01-04] Enumerate raw JSON files\n",
      "[CELL 01-04] start=2026-02-01T02:01:35\n",
      "[CELL 01-04] raw_dir=/workspace/anonymous-users-mooc-session-meta/data/raw/xuetangx\n",
      "[CELL 01-04] Found 1 JSON files:\n",
      "  20170201-20170801-raw_user_activity.json (4.79 GB)\n",
      "\n",
      "[CELL 01-04] PROTOTYPE MODE: using only 20170201-20170801-raw_user_activity.json\n",
      "[CELL 01-04] n_files=1\n",
      "[CELL 01-04] elapsed=0.00s\n",
      "[CELL 01-04] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 01-04] Enumerate raw JSON files + select prototype\n",
    "\n",
    "t0 = cell_start(\"CELL 01-04\", \"Enumerate raw JSON files\", raw_dir=str(RAW_DIR))\n",
    "\n",
    "if not RAW_DIR.exists():\n",
    "    raise RuntimeError(f\"Raw directory not found: {RAW_DIR}\")\n",
    "\n",
    "all_json_files = sorted(RAW_DIR.glob(CFG[\"ingest\"][\"all_files_pattern\"]))\n",
    "print(f\"[CELL 01-04] Found {len(all_json_files)} JSON files:\")\n",
    "for f in all_json_files:\n",
    "    print(f\"  {f.name} ({f.stat().st_size / 1024 / 1024 / 1024:.2f} GB)\")\n",
    "\n",
    "# Prototype mode: use only first file\n",
    "if CFG[\"ingest\"][\"prototype_mode\"]:\n",
    "    prototype_name = CFG[\"ingest\"][\"prototype_file\"]\n",
    "    target_file = RAW_DIR / prototype_name\n",
    "    if not target_file.exists():\n",
    "        raise RuntimeError(f\"Prototype file not found: {target_file}\")\n",
    "    files_to_parse = [target_file]\n",
    "    print(f\"\\n[CELL 01-04] PROTOTYPE MODE: using only {prototype_name}\")\n",
    "else:\n",
    "    files_to_parse = all_json_files\n",
    "    print(f\"\\n[CELL 01-04] FULL MODE: parsing all {len(files_to_parse)} files\")\n",
    "\n",
    "cell_end(\"CELL 01-04\", t0, n_files=len(files_to_parse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 01-05] Parse nested JSON to DataFrame\n",
      "[CELL 01-05] start=2026-02-01T02:01:35\n",
      "  Parsing: 20170201-20170801-raw_user_activity.json (4903.7 MB)...\n",
      "    Progress: 100/1577 courses...\n",
      "    Progress: 200/1577 courses...\n",
      "    Progress: 300/1577 courses...\n",
      "    Progress: 400/1577 courses...\n",
      "    Progress: 500/1577 courses...\n",
      "    Progress: 600/1577 courses...\n",
      "    Progress: 700/1577 courses...\n",
      "    Progress: 800/1577 courses...\n",
      "    Progress: 900/1577 courses...\n",
      "    Progress: 1000/1577 courses...\n",
      "    Progress: 1100/1577 courses...\n",
      "    Progress: 1200/1577 courses...\n",
      "    Progress: 1300/1577 courses...\n",
      "    Progress: 1400/1577 courses...\n",
      "    Progress: 1500/1577 courses...\n",
      "    Events: 56,985,474 total, 28,002,537 kept (learning only)\n",
      "\n",
      "[CELL 01-05] Combined shape: (28002537, 6)\n",
      "[CELL 01-05] Columns: ['course_id', 'user_id', 'session_hash', 'event_type', 'timestamp', '__source_file']\n",
      "\n",
      "[CELL 01-05] Head(3):\n",
      "                              course_id user_id                     session_hash  event_type           timestamp                            __source_file\n",
      "course-v1:TsinghuaX+10610204_tv+2015_T1 1660673 22ef2c411f8cf49bfeeca748ce280679  load_video 2017-05-22T18:23:06 20170201-20170801-raw_user_activity.json\n",
      "course-v1:TsinghuaX+10610204_tv+2015_T1 1660673 22ef2c411f8cf49bfeeca748ce280679 pause_video 2017-05-22T18:23:08 20170201-20170801-raw_user_activity.json\n",
      "course-v1:TsinghuaX+10610204_tv+2015_T1 2021250 125ca511da0b5b5c8bd9b84294d22763 pause_video 2017-02-19T00:45:33 20170201-20170801-raw_user_activity.json\n",
      "[CELL 01-05] rows=28002537\n",
      "[CELL 01-05] cols=6\n",
      "[CELL 01-05] elapsed=95.65s\n",
      "[CELL 01-05] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 01-05] Parse nested JSON → flattened DataFrame\n",
    "\n",
    "t0 = cell_start(\"CELL 01-05\", \"Parse nested JSON to DataFrame\")\n",
    "\n",
    "# Event filtering\n",
    "LEARNING_PREFIXES = tuple(CFG[\"ingest\"][\"learning_event_prefixes\"])\n",
    "FILTER_LEARNING = CFG[\"ingest\"][\"learning_events_only\"]\n",
    "\n",
    "def is_learning_event(event_type: str) -> bool:\n",
    "    \"\"\"Filter to learning events only (exclude UI navigation).\"\"\"\n",
    "    if not FILTER_LEARNING:\n",
    "        return True\n",
    "    return event_type.startswith(LEARNING_PREFIXES)\n",
    "\n",
    "def parse_xuetangx_json(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse XuetangX JSON structure:\n",
    "    [\n",
    "      [course_id, {user_id: {session_hash: [[event, timestamp], ...]}}],\n",
    "      ...\n",
    "    ]\n",
    "    \n",
    "    Returns: DataFrame with [course_id, user_id, session_hash, event_type, timestamp]\n",
    "    \"\"\"\n",
    "    print(f\"  Parsing: {file_path.name} ({file_path.stat().st_size / 1024 / 1024:.1f} MB)...\")\n",
    "    \n",
    "    with file_path.open('r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    rows = []\n",
    "    n_courses = len(data)\n",
    "    n_events_total = 0\n",
    "    n_events_filtered = 0\n",
    "    \n",
    "    for course_idx, course_data in enumerate(data):\n",
    "        if (course_idx + 1) % 100 == 0:\n",
    "            print(f\"    Progress: {course_idx + 1}/{n_courses} courses...\")\n",
    "        \n",
    "        course_id = course_data[0]\n",
    "        users_dict = course_data[1]\n",
    "        \n",
    "        for user_id, sessions_dict in users_dict.items():\n",
    "            for session_hash, events_list in sessions_dict.items():\n",
    "                for event_data in events_list:\n",
    "                    event_type = event_data[0]\n",
    "                    timestamp = event_data[1]\n",
    "                    \n",
    "                    n_events_total += 1\n",
    "                    \n",
    "                    if is_learning_event(event_type):\n",
    "                        rows.append({\n",
    "                            \"course_id\": course_id,\n",
    "                            \"user_id\": user_id,\n",
    "                            \"session_hash\": session_hash,\n",
    "                            \"event_type\": event_type,\n",
    "                            \"timestamp\": timestamp,\n",
    "                        })\n",
    "                        n_events_filtered += 1\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    print(f\"    Events: {n_events_total:,} total, {n_events_filtered:,} kept (learning only)\")\n",
    "    return df\n",
    "\n",
    "# Parse all selected files\n",
    "dfs = []\n",
    "for fpath in files_to_parse:\n",
    "    df_chunk = parse_xuetangx_json(fpath)\n",
    "    dfs.append(df_chunk)\n",
    "\n",
    "events = pd.concat(dfs, ignore_index=True)\n",
    "assert_nonempty_df(events, \"events\")\n",
    "\n",
    "# Add source file tracking\n",
    "events[\"__source_file\"] = \"; \".join([f.name for f in files_to_parse])\n",
    "\n",
    "print(f\"\\n[CELL 01-05] Combined shape: {events.shape}\")\n",
    "print(f\"[CELL 01-05] Columns: {list(events.columns)}\")\n",
    "print(f\"\\n[CELL 01-05] Head(3):\")\n",
    "print(events.head(3).to_string(index=False))\n",
    "\n",
    "cell_end(\"CELL 01-05\", t0, rows=int(events.shape[0]), cols=int(events.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 01-06] Save canonical Parquet\n",
      "[CELL 01-06] start=2026-02-01T02:03:11\n",
      "[CELL 01-06] out_parquet=/workspace/anonymous-users-mooc-session-meta/data/interim/xuetangx_events_raw.parquet\n",
      "[CELL 01-06] Saved: /workspace/anonymous-users-mooc-session-meta/data/interim/xuetangx_events_raw.parquet\n",
      "[CELL 01-06] Size: 95.0 MB\n",
      "[CELL 01-06] SHA256: 98dd880d3b1431623e74da517c4eb41c43ba776c1dff1a52492ad0ae6fec1c01\n",
      "[CELL 01-06] elapsed=5.08s\n",
      "[CELL 01-06] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 01-06] Save canonical Parquet\n",
    "\n",
    "t0 = cell_start(\"CELL 01-06\", \"Save canonical Parquet\", out_parquet=str(OUT_PARQUET))\n",
    "\n",
    "OUT_PARQUET.parent.mkdir(parents=True, exist_ok=True)\n",
    "events.to_parquet(OUT_PARQUET, index=False, compression=CFG[\"ingest\"][\"parquet_compression\"])\n",
    "\n",
    "parq_bytes = int(OUT_PARQUET.stat().st_size)\n",
    "parq_sha = sha256_file(OUT_PARQUET)\n",
    "\n",
    "print(f\"[CELL 01-06] Saved: {OUT_PARQUET}\")\n",
    "print(f\"[CELL 01-06] Size: {parq_bytes / 1024 / 1024:.1f} MB\")\n",
    "print(f\"[CELL 01-06] SHA256: {parq_sha}\")\n",
    "\n",
    "cell_end(\"CELL 01-06\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 01-07] Create DuckDB + view\n",
      "[CELL 01-07] start=2026-02-01T02:03:16\n",
      "[CELL 01-07] out_duckdb=/workspace/anonymous-users-mooc-session-meta/data/interim/xuetangx.duckdb\n",
      "[CELL 01-07] View: xuetangx_events_raw\n",
      "[CELL 01-07] Rows: 28,002,537\n",
      "\n",
      "[CELL 01-07] Schema:\n",
      "  column_name column_type null  key default extra\n",
      "    course_id     VARCHAR  YES None    None  None\n",
      "      user_id     VARCHAR  YES None    None  None\n",
      " session_hash     VARCHAR  YES None    None  None\n",
      "   event_type     VARCHAR  YES None    None  None\n",
      "    timestamp     VARCHAR  YES None    None  None\n",
      "__source_file     VARCHAR  YES None    None  None\n",
      "\n",
      "[CELL 01-07] Closed DuckDB connection\n",
      "[CELL 01-07] rows=28002537\n",
      "[CELL 01-07] elapsed=0.12s\n",
      "[CELL 01-07] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 01-07] DuckDB: create DB + view from Parquet\n",
    "\n",
    "t0 = cell_start(\"CELL 01-07\", \"Create DuckDB + view\", out_duckdb=str(OUT_DUCKDB))\n",
    "\n",
    "try:\n",
    "    import duckdb\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"Missing duckdb. Install via:\\n\"\n",
    "        \"  conda install -c conda-forge duckdb\\n\"\n",
    "        \"or\\n\"\n",
    "        \"  pip install duckdb\\n\"\n",
    "    ) from e\n",
    "\n",
    "OUT_DUCKDB.parent.mkdir(parents=True, exist_ok=True)\n",
    "con = duckdb.connect(str(OUT_DUCKDB))\n",
    "\n",
    "view = CFG[\"ingest\"][\"duckdb_view\"]\n",
    "con.execute(f\"DROP VIEW IF EXISTS {view};\")\n",
    "con.execute(f\"\"\"\n",
    "CREATE VIEW {view} AS\n",
    "SELECT * FROM read_parquet('{str(OUT_PARQUET).replace(\"'\", \"''\")}')\\n\n",
    "\"\"\")\n",
    "\n",
    "n = con.execute(f\"SELECT COUNT(*) FROM {view}\").fetchone()[0]\n",
    "schema_df = con.execute(f\"DESCRIBE {view}\").fetchdf()\n",
    "\n",
    "print(f\"[CELL 01-07] View: {view}\")\n",
    "print(f\"[CELL 01-07] Rows: {int(n):,}\")\n",
    "print(f\"\\n[CELL 01-07] Schema:\")\n",
    "print(schema_df.to_string(index=False))\n",
    "\n",
    "con.close()\n",
    "print(f\"\\n[CELL 01-07] Closed DuckDB connection\")\n",
    "\n",
    "cell_end(\"CELL 01-07\", t0, rows=int(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 01-08] Write fingerprints + sanity sample to report\n",
      "[CELL 01-08] start=2026-02-01T02:03:16\n",
      "[CELL 01-08] Updated: /workspace/anonymous-users-mooc-session-meta/reports/01_ingest_xuetangx/20260201_020135/report.json\n",
      "[CELL 01-08] Updated: /workspace/anonymous-users-mooc-session-meta/reports/01_ingest_xuetangx/20260201_020135/manifest.json\n",
      "[CELL 01-08] elapsed=3.63s\n",
      "[CELL 01-08] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 01-08] Update report + manifest\n",
    "\n",
    "t0 = cell_start(\"CELL 01-08\", \"Write fingerprints + sanity sample to report\")\n",
    "\n",
    "report = read_json(REPORT_PATH)\n",
    "manifest = read_json(MANIFEST_PATH)\n",
    "\n",
    "# Sanity sample\n",
    "head3 = events.head(3).to_dict(orient=\"records\")\n",
    "\n",
    "# Data fingerprints\n",
    "raw_fp = {\n",
    "    \"root\": str(RAW_DIR),\n",
    "    \"n_files_parsed\": len(files_to_parse),\n",
    "    \"files\": [\n",
    "        {\n",
    "            \"name\": f.name,\n",
    "            \"bytes\": int(f.stat().st_size),\n",
    "            \"sha256\": sha256_file(f),\n",
    "        } for f in files_to_parse\n",
    "    ],\n",
    "}\n",
    "\n",
    "report[\"data_fingerprints\"][\"xuetangx_raw_files\"] = raw_fp\n",
    "report[\"data_fingerprints\"][\"xuetangx_events_raw_parquet\"] = {\n",
    "    \"path\": str(OUT_PARQUET),\n",
    "    \"bytes\": parq_bytes,\n",
    "    \"sha256\": parq_sha,\n",
    "}\n",
    "report[\"sanity_samples\"][\"events_head3\"] = head3\n",
    "report[\"notes\"].append(\n",
    "    f\"Parsed {len(files_to_parse)} JSON file(s) in prototype mode. \"\n",
    "    \"Filtered to learning events only (load_*, problem_*, video_*, etc.). \"\n",
    "    \"Kept platform session_hash for sessionization.\"\n",
    ")\n",
    "\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "# Manifest\n",
    "def add_artifact(path: Path) -> None:\n",
    "    rec = {\n",
    "        \"path\": str(path),\n",
    "        \"bytes\": int(path.stat().st_size),\n",
    "        \"sha256\": None,\n",
    "        \"sha256_error\": None,\n",
    "    }\n",
    "    try:\n",
    "        rec[\"sha256\"] = sha256_file(path)\n",
    "    except PermissionError as e:\n",
    "        rec[\"sha256_error\"] = f\"PermissionError: {e}\"\n",
    "        print(f\"[CELL 01-08] WARN: could not hash (locked): {path}\")\n",
    "    manifest[\"artifacts\"].append(rec)\n",
    "\n",
    "add_artifact(OUT_PARQUET)\n",
    "add_artifact(OUT_DUCKDB)\n",
    "\n",
    "write_json_atomic(MANIFEST_PATH, manifest)\n",
    "\n",
    "print(f\"[CELL 01-08] Updated: {REPORT_PATH}\")\n",
    "print(f\"[CELL 01-08] Updated: {MANIFEST_PATH}\")\n",
    "\n",
    "cell_end(\"CELL 01-08\", t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 01-09] Compute basic stats\n",
      "[CELL 01-09] start=2026-02-01T02:03:20\n"
     ]
    }
   ],
   "source": [
    "# [CELL 01-09] Basic stats (users/courses/events)\n",
    "\n",
    "t0 = cell_start(\"CELL 01-09\", \"Compute basic stats\")\n",
    "\n",
    "stats = {\n",
    "    \"n_events\": int(events.shape[0]),\n",
    "    \"n_users\": int(events[\"user_id\"].nunique()),\n",
    "    \"n_courses\": int(events[\"course_id\"].nunique()),\n",
    "    \"n_sessions\": int(events[\"session_hash\"].nunique()),\n",
    "    \"n_event_types\": int(events[\"event_type\"].nunique()),\n",
    "}\n",
    "\n",
    "print(f\"[CELL 01-09] Stats:\")\n",
    "for k, v in stats.items():\n",
    "    print(f\"  {k}: {v:,}\")\n",
    "\n",
    "# Top event types\n",
    "top_events = events[\"event_type\"].value_counts().head(10)\n",
    "print(f\"\\n[CELL 01-09] Top 10 event types:\")\n",
    "print(top_events.to_string())\n",
    "\n",
    "# Save to report\n",
    "report = read_json(REPORT_PATH)\n",
    "report[\"metrics\"] = stats\n",
    "report[\"sanity_samples\"][\"top_event_types\"] = top_events.to_dict()\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "cell_end(\"CELL 01-09\", t0, **stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Notebook 01 Complete\n",
    "\n",
    "**Outputs:**\n",
    "- ✅ `data/interim/xuetangx_events_raw.parquet`\n",
    "- ✅ `data/interim/xuetangx.duckdb` (view: `xuetangx_events_raw`)\n",
    "- ✅ `reports/01_ingest_xuetangx/<run_tag>/report.json`\n",
    "\n",
    "**Next:** Notebook 02 (Sessionize XuetangX)\n",
    "- Validate platform session_hash\n",
    "- Compute gap statistics\n",
    "- Decide: use platform sessions or re-sessionize?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (VENV)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}