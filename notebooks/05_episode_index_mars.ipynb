{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae1254a4",
   "metadata": {},
   "source": [
    "What this notebook will produce (inside existing fixed paths)\n",
    "Under: data/processed/mars/episodes/\n",
    "- episodes_index.parquet (episode_id, user_id, split, K, Q, counts, etc.)\n",
    "- episodes_long.parquet (episode_id, role, row_in_role, pair_id) ← easiest + safest for training\n",
    "\n",
    "We will:\n",
    "- Create a deterministic pair_id for mars_pairs_* (since current pairs don’t have an id)\n",
    "- Attach a timestamp per pair (label event timestamp) so we can order pairs chronologically per user\n",
    "- For each split, for each K in [5,10,20,64] and fixed Q (default 20), select eligible users with n_pairs >= K+Q\n",
    "- Build episodes: support = first K pairs, query = next Q pairs (chronological, disjoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e670c6cc",
   "metadata": {},
   "source": [
    "Bootstrap + paths + logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c0c02ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CELL 05-00] start=2026-01-06T22:41:45\n",
      "[CELL 05-00] CWD: C:\\anonymous-users-mooc-session-meta\\notebooks\n",
      "[CELL 05-00] REPO_ROOT: C:\\anonymous-users-mooc-session-meta\n",
      "[CELL 05-00] META_REGISTRY=C:\\anonymous-users-mooc-session-meta\\meta.json\n",
      "[CELL 05-00] DATA_INTERIM=C:\\anonymous-users-mooc-session-meta\\data\\interim\n",
      "[CELL 05-00] DATA_PROCESSED=C:\\anonymous-users-mooc-session-meta\\data\\processed\n",
      "[CELL 05-00] REPORTS=C:\\anonymous-users-mooc-session-meta\\reports\n",
      "[CELL 05-00] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05-00] Bootstrap: repo root + paths + logger\n",
    "\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"[CELL 05-00] start={datetime.now().isoformat(timespec='seconds')}\")\n",
    "print(\"[CELL 05-00] CWD:\", Path.cwd().resolve())\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"PROJECT_STATE.md\").exists():\n",
    "            return p\n",
    "    raise RuntimeError(\"Could not find PROJECT_STATE.md. Open notebook from within the repo.\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "print(\"[CELL 05-00] REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "PATHS = {\n",
    "    \"META_REGISTRY\": REPO_ROOT / \"meta.json\",\n",
    "    \"DATA_INTERIM\": REPO_ROOT / \"data\" / \"interim\",\n",
    "    \"DATA_PROCESSED\": REPO_ROOT / \"data\" / \"processed\",\n",
    "    \"REPORTS\": REPO_ROOT / \"reports\",\n",
    "}\n",
    "for k, v in PATHS.items():\n",
    "    print(f\"[CELL 05-00] {k}={v}\")\n",
    "\n",
    "def cell_start(cell_id: str, title: str, **kwargs: Any) -> float:\n",
    "    t = time.time()\n",
    "    print(f\"\\n[{cell_id}] {title}\")\n",
    "    print(f\"[{cell_id}] start={datetime.now().isoformat(timespec='seconds')}\")\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    return t\n",
    "\n",
    "def cell_end(cell_id: str, t0: float, **kwargs: Any) -> None:\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"[{cell_id}] {k}={v}\")\n",
    "    print(f\"[{cell_id}] elapsed={time.time()-t0:.2f}s\")\n",
    "    print(f\"[{cell_id}] done\")\n",
    "\n",
    "print(\"[CELL 05-00] done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9091dc",
   "metadata": {},
   "source": [
    "JSON IO (Timestamp-safe) + hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24648f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 05-01] JSON IO + hashing (Timestamp-safe)\n",
      "[CELL 05-01] start=2026-01-06T22:42:03\n",
      "[CELL 05-01] elapsed=0.00s\n",
      "[CELL 05-01] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05-01] JSON IO + hashing (Timestamp-safe)\n",
    "\n",
    "t0 = cell_start(\"CELL 05-01\", \"JSON IO + hashing (Timestamp-safe)\")\n",
    "\n",
    "def _json_default(o):\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        if isinstance(o, (pd.Timestamp,)):\n",
    "            return o.isoformat()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        import numpy as np\n",
    "        if isinstance(o, (np.integer,)):\n",
    "            return int(o)\n",
    "        if isinstance(o, (np.floating,)):\n",
    "            return float(o)\n",
    "        if isinstance(o, (np.bool_,)):\n",
    "            return bool(o)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        from datetime import datetime, date\n",
    "        if isinstance(o, (datetime, date)):\n",
    "            return o.isoformat()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return str(o)\n",
    "\n",
    "def write_json_atomic(path: Path, obj: Any, indent: int = 2) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tmp = path.with_suffix(path.suffix + f\".tmp_{uuid.uuid4().hex}\")\n",
    "    with tmp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=indent, default=_json_default)\n",
    "    tmp.replace(path)\n",
    "\n",
    "def read_json(path: Path) -> Any:\n",
    "    if not path.exists():\n",
    "        raise RuntimeError(f\"Missing JSON file: {path}\")\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def safe_artifact_record(path: Path) -> Dict[str, Any]:\n",
    "    rec = {\"path\": str(path), \"bytes\": int(path.stat().st_size), \"sha256\": None, \"sha256_error\": None}\n",
    "    try:\n",
    "        rec[\"sha256\"] = sha256_file(path)\n",
    "    except PermissionError as e:\n",
    "        rec[\"sha256_error\"] = f\"PermissionError: {e}\"\n",
    "        print(\"[CELL 05-01] WARN: locked, cannot hash now:\", path)\n",
    "    return rec\n",
    "\n",
    "cell_end(\"CELL 05-01\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a8c80f",
   "metadata": {},
   "source": [
    "Start run + config/report/manifest + meta.json append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cbf1f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 05-02] Start run\n",
      "[CELL 05-02] start=2026-01-06T22:42:26\n",
      "[CELL 05-02] out_dir=C:\\anonymous-users-mooc-session-meta\\reports\\05_episode_index_mars\\20260106_224226\n",
      "[CELL 05-02] elapsed=0.01s\n",
      "[CELL 05-02] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05-02] Start run + init files + meta.json append-only\n",
    "\n",
    "t0 = cell_start(\"CELL 05-02\", \"Start run\")\n",
    "\n",
    "NOTEBOOK_NAME = \"05_episode_index_mars\"\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_ID = uuid.uuid4().hex\n",
    "\n",
    "OUT_DIR = PATHS[\"REPORTS\"] / NOTEBOOK_NAME / RUN_TAG\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REPORT_PATH = OUT_DIR / \"report.json\"\n",
    "CONFIG_PATH = OUT_DIR / \"config.json\"\n",
    "MANIFEST_PATH = OUT_DIR / \"manifest.json\"\n",
    "\n",
    "DUCKDB_PATH = PATHS[\"DATA_INTERIM\"] / \"mars.duckdb\"\n",
    "\n",
    "OUT_EP_DIR = PATHS[\"DATA_PROCESSED\"] / \"mars\" / \"episodes\"\n",
    "OUT_EP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CFG = {\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"inputs\": {\n",
    "        \"duckdb_path\": str(DUCKDB_PATH),\n",
    "        \"pairs_views\": [\"mars_pairs_train\",\"mars_pairs_val\",\"mars_pairs_test\"],\n",
    "        \"events_views\": [\"mars_events_train\",\"mars_events_val\",\"mars_events_test\"],\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"episodes_dir\": str(OUT_EP_DIR),\n",
    "        \"reports_out_dir\": str(OUT_DIR),\n",
    "    },\n",
    "    \"episodes\": {\n",
    "        \"K_list\": [5, 10, 20, 64],\n",
    "        \"Q\": 20,\n",
    "        \"max_episodes_per_split\": 500,   # cap; deterministic sampling\n",
    "        \"seed\": 20260106,\n",
    "        \"ordering\": \"by_label_event_ts_epoch\",\n",
    "    }\n",
    "}\n",
    "write_json_atomic(CONFIG_PATH, CFG)\n",
    "\n",
    "report = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"repo_root\": str(REPO_ROOT),\n",
    "    \"metrics\": {},\n",
    "    \"key_findings\": [],\n",
    "    \"sanity_samples\": {},\n",
    "    \"data_fingerprints\": {},\n",
    "    \"notes\": [],\n",
    "}\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "\n",
    "manifest = {\"run_id\": RUN_ID, \"notebook\": NOTEBOOK_NAME, \"run_tag\": RUN_TAG, \"artifacts\": []}\n",
    "write_json_atomic(MANIFEST_PATH, manifest)\n",
    "\n",
    "META_PATH = PATHS[\"META_REGISTRY\"]\n",
    "if not META_PATH.exists():\n",
    "    write_json_atomic(META_PATH, {\"schema_version\": 1, \"runs\": []})\n",
    "meta = read_json(META_PATH)\n",
    "meta[\"runs\"].append({\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"notebook\": NOTEBOOK_NAME,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"out_dir\": str(OUT_DIR),\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "})\n",
    "write_json_atomic(META_PATH, meta)\n",
    "\n",
    "cell_end(\"CELL 05-02\", t0, out_dir=str(OUT_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e217c8",
   "metadata": {},
   "source": [
    "DuckDB: create “pairs_with_id_and_ts” views per split (deterministic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac33d7d",
   "metadata": {},
   "source": [
    "This is the key cell: it gives each pair a stable pair_id and assigns label_ts_epoch by joining with the event at (session_id, pos_in_sess=tpos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7c6390f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 05-03] Create pairs_with_id_and_ts views\n",
      "[CELL 05-03] start=2026-01-06T22:43:04\n",
      "[CELL 05-03] mars_pairs_train_ts rows: 1932\n",
      "[CELL 05-03] mars_pairs_val_ts rows: 191\n",
      "[CELL 05-03] mars_pairs_test_ts rows: 214\n",
      "[CELL 05-03] elapsed=0.09s\n",
      "[CELL 05-03] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05-03] Build pairs_with_id_and_ts views (train/val/test)\n",
    "\n",
    "t0 = cell_start(\"CELL 05-03\", \"Create pairs_with_id_and_ts views\")\n",
    "\n",
    "import duckdb\n",
    "con = duckdb.connect(str(DUCKDB_PATH), read_only=False)\n",
    "\n",
    "def vcheck(v: str) -> None:\n",
    "    try:\n",
    "        con.execute(f\"SELECT COUNT(*) FROM {v}\").fetchone()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Missing required view: {v}\") from e\n",
    "\n",
    "for v in [\"mars_pairs_train\",\"mars_pairs_val\",\"mars_pairs_test\",\"mars_events_train\",\"mars_events_val\",\"mars_events_test\"]:\n",
    "    vcheck(v)\n",
    "\n",
    "# For each split, create a stable view with:\n",
    "# - pair_id: deterministic row_number ordered by (user_id, label_ts_epoch, session_id, tpos, label)\n",
    "# - label_ts_epoch: timestamp of the label event (pos_in_sess == tpos) within session\n",
    "spec = [\n",
    "    (\"train\", \"mars_pairs_train\", \"mars_events_train\"),\n",
    "    (\"val\",   \"mars_pairs_val\",   \"mars_events_val\"),\n",
    "    (\"test\",  \"mars_pairs_test\",  \"mars_events_test\"),\n",
    "]\n",
    "\n",
    "for split, pv, ev in spec:\n",
    "    outv = f\"mars_pairs_{split}_ts\"\n",
    "    con.execute(f\"DROP VIEW IF EXISTS {outv};\")\n",
    "\n",
    "    con.execute(f\"\"\"\n",
    "    CREATE VIEW {outv} AS\n",
    "    WITH base AS (\n",
    "      SELECT\n",
    "        p.*,\n",
    "        e.ts_epoch AS label_ts_epoch\n",
    "      FROM {pv} p\n",
    "      JOIN {ev} e\n",
    "        ON p.session_id = e.session_id\n",
    "       AND p.user_id = e.user_id\n",
    "       AND p.tpos = e.pos_in_sess\n",
    "    ),\n",
    "    numbered AS (\n",
    "      SELECT\n",
    "        *,\n",
    "        ROW_NUMBER() OVER (\n",
    "          ORDER BY CAST(user_id AS VARCHAR), label_ts_epoch, session_id, tpos, label\n",
    "        ) - 1 AS pair_id\n",
    "      FROM base\n",
    "    )\n",
    "    SELECT * FROM numbered\n",
    "    \"\"\")\n",
    "    n = int(con.execute(f\"SELECT COUNT(*) FROM {outv}\").fetchone()[0])\n",
    "    print(f\"[CELL 05-03] {outv} rows:\", n)\n",
    "\n",
    "cell_end(\"CELL 05-03\", t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf94e0",
   "metadata": {},
   "source": [
    "Eligibility counts for each (split, K, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "954e1070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 05-04] Eligibility counts per split and K\n",
      "[CELL 05-04] start=2026-01-06T22:43:58\n",
      "[CELL 05-04] split=train n_users_with_pairs=291 total_pairs=1932\n",
      "[CELL 05-04] split=train K=5 Q=20 need=25 eligible_users=18\n",
      "[CELL 05-04] split=train K=10 Q=20 need=30 eligible_users=15\n",
      "[CELL 05-04] split=train K=20 Q=20 need=40 eligible_users=12\n",
      "[CELL 05-04] split=train K=64 Q=20 need=84 eligible_users=2\n",
      "[CELL 05-04] split=val n_users_with_pairs=43 total_pairs=191\n",
      "[CELL 05-04] split=val K=5 Q=20 need=25 eligible_users=2\n",
      "[CELL 05-04] split=val K=10 Q=20 need=30 eligible_users=2\n",
      "[CELL 05-04] split=val K=20 Q=20 need=40 eligible_users=0\n",
      "[CELL 05-04] split=val K=64 Q=20 need=84 eligible_users=0\n",
      "[CELL 05-04] split=test n_users_with_pairs=44 total_pairs=214\n",
      "[CELL 05-04] split=test K=5 Q=20 need=25 eligible_users=1\n",
      "[CELL 05-04] split=test K=10 Q=20 need=30 eligible_users=1\n",
      "[CELL 05-04] split=test K=20 Q=20 need=40 eligible_users=0\n",
      "[CELL 05-04] split=test K=64 Q=20 need=84 eligible_users=0\n",
      "\n",
      "[CELL 05-04] eligibility table:\n",
      "split  K  Q  need_pairs  n_users_with_pairs  n_users_eligible  eligible_pairs_min  eligible_pairs_p50  eligible_pairs_max\n",
      " test  5 20          25                  44                 1                30.0                30.0                30.0\n",
      " test 10 20          30                  44                 1                30.0                30.0                30.0\n",
      " test 20 20          40                  44                 0                 NaN                 NaN                 NaN\n",
      " test 64 20          84                  44                 0                 NaN                 NaN                 NaN\n",
      "train  5 20          25                 291                18                26.0                49.5               123.0\n",
      "train 10 20          30                 291                15                31.0                53.0               123.0\n",
      "train 20 20          40                 291                12                40.0                55.5               123.0\n",
      "train 64 20          84                 291                 2               118.0               120.5               123.0\n",
      "  val  5 20          25                  43                 2                36.0                36.5                37.0\n",
      "  val 10 20          30                  43                 2                36.0                36.5                37.0\n",
      "  val 20 20          40                  43                 0                 NaN                 NaN                 NaN\n",
      "  val 64 20          84                  43                 0                 NaN                 NaN                 NaN\n",
      "[CELL 05-04] Q=20\n",
      "[CELL 05-04] elapsed=0.04s\n",
      "[CELL 05-04] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05-04] Episode eligibility counts per (split, K, Q)\n",
    "\n",
    "t0 = cell_start(\"CELL 05-04\", \"Eligibility counts per split and K\")\n",
    "\n",
    "K_list = list(map(int, CFG[\"episodes\"][\"K_list\"]))\n",
    "Q = int(CFG[\"episodes\"][\"Q\"])\n",
    "\n",
    "spec = [\n",
    "    (\"train\", \"mars_pairs_train_ts\"),\n",
    "    (\"val\",   \"mars_pairs_val_ts\"),\n",
    "    (\"test\",  \"mars_pairs_test_ts\"),\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for split, view in spec:\n",
    "    user_counts = con.execute(f\"\"\"\n",
    "    SELECT CAST(user_id AS VARCHAR) AS user_id, COUNT(*) AS n_pairs\n",
    "    FROM {view}\n",
    "    GROUP BY 1\n",
    "    \"\"\").fetchdf()\n",
    "\n",
    "    n_users = int(user_counts.shape[0])\n",
    "    total_pairs = int(user_counts[\"n_pairs\"].sum())\n",
    "    print(f\"[CELL 05-04] split={split} n_users_with_pairs={n_users} total_pairs={total_pairs}\")\n",
    "\n",
    "    for K in K_list:\n",
    "        need = K + Q\n",
    "        eligible = user_counts[user_counts[\"n_pairs\"] >= need]\n",
    "        rows.append({\n",
    "            \"split\": split,\n",
    "            \"K\": int(K),\n",
    "            \"Q\": int(Q),\n",
    "            \"need_pairs\": int(need),\n",
    "            \"n_users_with_pairs\": int(n_users),\n",
    "            \"n_users_eligible\": int(eligible.shape[0]),\n",
    "            \"eligible_pairs_min\": int(eligible[\"n_pairs\"].min()) if eligible.shape[0] else None,\n",
    "            \"eligible_pairs_p50\": float(eligible[\"n_pairs\"].median()) if eligible.shape[0] else None,\n",
    "            \"eligible_pairs_max\": int(eligible[\"n_pairs\"].max()) if eligible.shape[0] else None,\n",
    "        })\n",
    "        print(f\"[CELL 05-04] split={split} K={K} Q={Q} need={need} eligible_users={int(eligible.shape[0])}\")\n",
    "\n",
    "elig = pd.DataFrame(rows).sort_values([\"split\", \"K\"]).reset_index(drop=True)\n",
    "print(\"\\n[CELL 05-04] eligibility table:\")\n",
    "print(elig.to_string(index=False))\n",
    "\n",
    "cell_end(\"CELL 05-04\", t0, Q=Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b952fa1",
   "metadata": {},
   "source": [
    "Build episodes (deterministic sampling + chronological support/query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf1d2f6",
   "metadata": {},
   "source": [
    "This creates:\n",
    "- episodes_index (one row per episode)\n",
    "- episodes_long (many rows per episode: support/query membership by pair_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b0c88c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 05-05] Build episodes (user-as-task)\n",
      "[CELL 05-05] start=2026-01-06T22:44:55\n",
      "[CELL 05-05] split=train K=5 eligible=18 sampled=18\n",
      "[CELL 05-05] split=train K=10 eligible=15 sampled=15\n",
      "[CELL 05-05] split=train K=20 eligible=12 sampled=12\n",
      "[CELL 05-05] split=train K=64 eligible=2 sampled=2\n",
      "[CELL 05-05] split=val K=5 eligible=2 sampled=2\n",
      "[CELL 05-05] split=val K=10 eligible=2 sampled=2\n",
      "[CELL 05-05] split=val K=20 -> eligible_users=0 (skip)\n",
      "[CELL 05-05] split=val K=64 -> eligible_users=0 (skip)\n",
      "[CELL 05-05] split=test K=5 eligible=1 sampled=1\n",
      "[CELL 05-05] split=test K=10 eligible=1 sampled=1\n",
      "[CELL 05-05] split=test K=20 -> eligible_users=0 (skip)\n",
      "[CELL 05-05] split=test K=64 -> eligible_users=0 (skip)\n",
      "[CELL 05-05] episodes_index shape: (53, 9)\n",
      "[CELL 05-05] episodes_long shape: (1713, 4)\n",
      "[CELL 05-05] episodes_index head3:\n",
      "          episode_id split user_id  K  Q  need_pairs  n_pairs_user  support_last_ts  query_first_ts\n",
      "train_K5_Q20_e000000 train  604039  5 20          25           123       1626326027      1626326184\n",
      "train_K5_Q20_e000001 train  592627  5 20          25            26       1620204523      1620204623\n",
      "train_K5_Q20_e000002 train  378317  5 20          25            26       1563174467      1563174539\n",
      "[CELL 05-05] n_episodes=53\n",
      "[CELL 05-05] elapsed=0.44s\n",
      "[CELL 05-05] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05-05] Build episodes_index + episodes_long (chronological, deterministic)\n",
    "\n",
    "t0 = cell_start(\"CELL 05-05\", \"Build episodes (user-as-task)\")\n",
    "\n",
    "seed = int(CFG[\"episodes\"][\"seed\"])\n",
    "max_eps = int(CFG[\"episodes\"][\"max_episodes_per_split\"])\n",
    "K_list = list(map(int, CFG[\"episodes\"][\"K_list\"]))\n",
    "Q = int(CFG[\"episodes\"][\"Q\"])\n",
    "\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "spec = [\n",
    "    (\"train\", \"mars_pairs_train_ts\"),\n",
    "    (\"val\",   \"mars_pairs_val_ts\"),\n",
    "    (\"test\",  \"mars_pairs_test_ts\"),\n",
    "]\n",
    "\n",
    "episodes_index_rows = []\n",
    "episodes_long_rows = []\n",
    "\n",
    "episode_counter = 0\n",
    "\n",
    "for split, view in spec:\n",
    "    # per-user counts\n",
    "    uc = con.execute(f\"\"\"\n",
    "    SELECT CAST(user_id AS VARCHAR) AS user_id, COUNT(*) AS n_pairs\n",
    "    FROM {view}\n",
    "    GROUP BY 1\n",
    "    \"\"\").fetchdf()\n",
    "\n",
    "    for K in K_list:\n",
    "        need = K + Q\n",
    "        eligible_users = uc.loc[uc[\"n_pairs\"] >= need, \"user_id\"].astype(str).tolist()\n",
    "\n",
    "        if len(eligible_users) == 0:\n",
    "            print(f\"[CELL 05-05] split={split} K={K} -> eligible_users=0 (skip)\")\n",
    "            continue\n",
    "\n",
    "        # deterministic sampling: shuffle with seed, then take first max_eps\n",
    "        eligible_users_sorted = sorted(eligible_users)  # ensure deterministic base order\n",
    "        perm = rng.permutation(len(eligible_users_sorted))\n",
    "        sampled = [eligible_users_sorted[i] for i in perm[: min(max_eps, len(eligible_users_sorted))]]\n",
    "\n",
    "        print(f\"[CELL 05-05] split={split} K={K} eligible={len(eligible_users)} sampled={len(sampled)}\")\n",
    "\n",
    "        for u in sampled:\n",
    "            # pull all pairs for this user ordered by label_ts_epoch then tie-breakers\n",
    "            dfu = con.execute(f\"\"\"\n",
    "            SELECT\n",
    "              pair_id, session_id, tpos, label, label_ts_epoch\n",
    "            FROM {view}\n",
    "            WHERE CAST(user_id AS VARCHAR) = '{u.replace(\"'\", \"''\")}'\n",
    "            ORDER BY label_ts_epoch, session_id, tpos, label, pair_id\n",
    "            \"\"\").fetchdf()\n",
    "\n",
    "            if dfu.shape[0] < need:\n",
    "                # should not happen but keep explicit\n",
    "                continue\n",
    "\n",
    "            support = dfu.iloc[:K]\n",
    "            query = dfu.iloc[K:K+Q]\n",
    "\n",
    "            # safety: disjointness and ordering\n",
    "            s_ids = set(support[\"pair_id\"].astype(int).tolist())\n",
    "            q_ids = set(query[\"pair_id\"].astype(int).tolist())\n",
    "            if s_ids & q_ids:\n",
    "                raise RuntimeError(\"support/query overlap detected (pair_id)\")\n",
    "\n",
    "            # chronological check: last support ts <= first query ts\n",
    "            if int(support[\"label_ts_epoch\"].max()) > int(query[\"label_ts_epoch\"].min()):\n",
    "                raise RuntimeError(\"Chronology violated: support occurs after query for user\")\n",
    "\n",
    "            episode_id = f\"{split}_K{K}_Q{Q}_e{episode_counter:06d}\"\n",
    "            episode_counter += 1\n",
    "\n",
    "            episodes_index_rows.append({\n",
    "                \"episode_id\": episode_id,\n",
    "                \"split\": split,\n",
    "                \"user_id\": u,\n",
    "                \"K\": int(K),\n",
    "                \"Q\": int(Q),\n",
    "                \"need_pairs\": int(need),\n",
    "                \"n_pairs_user\": int(dfu.shape[0]),\n",
    "                \"support_last_ts\": int(support[\"label_ts_epoch\"].max()),\n",
    "                \"query_first_ts\": int(query[\"label_ts_epoch\"].min()),\n",
    "            })\n",
    "\n",
    "            # long format\n",
    "            for i, pid in enumerate(support[\"pair_id\"].astype(int).tolist()):\n",
    "                episodes_long_rows.append({\n",
    "                    \"episode_id\": episode_id,\n",
    "                    \"role\": \"support\",\n",
    "                    \"row_in_role\": int(i),\n",
    "                    \"pair_id\": int(pid),\n",
    "                })\n",
    "            for i, pid in enumerate(query[\"pair_id\"].astype(int).tolist()):\n",
    "                episodes_long_rows.append({\n",
    "                    \"episode_id\": episode_id,\n",
    "                    \"role\": \"query\",\n",
    "                    \"row_in_role\": int(i),\n",
    "                    \"pair_id\": int(pid),\n",
    "                })\n",
    "\n",
    "episodes_index = pd.DataFrame(episodes_index_rows)\n",
    "episodes_long = pd.DataFrame(episodes_long_rows)\n",
    "\n",
    "print(\"[CELL 05-05] episodes_index shape:\", episodes_index.shape)\n",
    "print(\"[CELL 05-05] episodes_long shape:\", episodes_long.shape)\n",
    "print(\"[CELL 05-05] episodes_index head3:\")\n",
    "print(episodes_index.head(3).to_string(index=False))\n",
    "\n",
    "if episodes_index.shape[0] == 0:\n",
    "    print(\"[CELL 05-05] WARNING: no episodes created for any K/Q. We don't know yet if K/Q is feasible for MARS until we reduce K/Q or Q.\")\n",
    "    # Do not raise here; we want the report to capture this result.\n",
    "\n",
    "cell_end(\"CELL 05-05\", t0, n_episodes=int(episodes_index.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7505d1",
   "metadata": {},
   "source": [
    "Save episode artifacts to data/processed/mars/episodes/ + register views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee325df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 05-06] Write episodes_index/long to parquet + views\n",
      "[CELL 05-06] start=2026-01-06T22:45:24\n",
      "[CELL 05-06] wrote: C:\\anonymous-users-mooc-session-meta\\data\\processed\\mars\\episodes\\episodes_index.parquet\n",
      "[CELL 05-06] wrote: C:\\anonymous-users-mooc-session-meta\\data\\processed\\mars\\episodes\\episodes_long.parquet\n",
      "[CELL 05-06] mars_episodes_index rows: 53\n",
      "[CELL 05-06] n_episodes=53\n",
      "[CELL 05-06] elapsed=0.05s\n",
      "[CELL 05-06] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05-06] Save episode artifacts + register DuckDB views\n",
    "\n",
    "t0 = cell_start(\"CELL 05-06\", \"Write episodes_index/long to parquet + views\")\n",
    "\n",
    "episodes_index_out = Path(OUT_EP_DIR) / \"episodes_index.parquet\"\n",
    "episodes_long_out  = Path(OUT_EP_DIR) / \"episodes_long.parquet\"\n",
    "\n",
    "episodes_index.to_parquet(episodes_index_out, index=False, compression=\"zstd\")\n",
    "episodes_long.to_parquet(episodes_long_out, index=False, compression=\"zstd\")\n",
    "\n",
    "print(\"[CELL 05-06] wrote:\", episodes_index_out)\n",
    "print(\"[CELL 05-06] wrote:\", episodes_long_out)\n",
    "\n",
    "# register stable views\n",
    "def esc(p: Path) -> str:\n",
    "    return str(p).replace(\"'\", \"''\")\n",
    "\n",
    "con.execute(\"DROP VIEW IF EXISTS mars_episodes_index;\")\n",
    "con.execute(\"DROP VIEW IF EXISTS mars_episodes_long;\")\n",
    "\n",
    "con.execute(f\"CREATE VIEW mars_episodes_index AS SELECT * FROM read_parquet('{esc(episodes_index_out)}');\")\n",
    "con.execute(f\"CREATE VIEW mars_episodes_long  AS SELECT * FROM read_parquet('{esc(episodes_long_out)}');\")\n",
    "\n",
    "# check\n",
    "chk = int(con.execute(\"SELECT COUNT(*) FROM mars_episodes_index\").fetchone()[0])\n",
    "print(\"[CELL 05-06] mars_episodes_index rows:\", chk)\n",
    "\n",
    "cell_end(\"CELL 05-06\", t0, n_episodes=chk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9739c58c",
   "metadata": {},
   "source": [
    "Update report + manifest + close DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30f096dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CELL 05-07] Write report + manifest\n",
      "[CELL 05-07] start=2026-01-06T22:45:42\n",
      "[CELL 05-07] updated: C:\\anonymous-users-mooc-session-meta\\reports\\05_episode_index_mars\\20260106_224226\\report.json\n",
      "[CELL 05-07] updated: C:\\anonymous-users-mooc-session-meta\\reports\\05_episode_index_mars\\20260106_224226\\manifest.json\n",
      "[CELL 05-07] closed DuckDB connection\n",
      "[CELL 05-07] n_artifacts=5\n",
      "[CELL 05-07] elapsed=0.05s\n",
      "[CELL 05-07] done\n"
     ]
    }
   ],
   "source": [
    "# [CELL 05-07] Write report + manifest (and close DuckDB)\n",
    "\n",
    "t0 = cell_start(\"CELL 05-07\", \"Write report + manifest\")\n",
    "\n",
    "report = read_json(REPORT_PATH)\n",
    "manifest = read_json(MANIFEST_PATH)\n",
    "\n",
    "# artifacts\n",
    "manifest[\"artifacts\"].append(safe_artifact_record(Path(CONFIG_PATH)))\n",
    "manifest[\"artifacts\"].append(safe_artifact_record(Path(REPORT_PATH)))\n",
    "manifest[\"artifacts\"].append(safe_artifact_record(Path(MANIFEST_PATH)))\n",
    "manifest[\"artifacts\"].append(safe_artifact_record(episodes_index_out))\n",
    "manifest[\"artifacts\"].append(safe_artifact_record(episodes_long_out))\n",
    "\n",
    "# summarize eligibility + episodes\n",
    "report[\"sanity_samples\"][\"eligibility_table\"] = elig.to_dict(orient=\"records\")\n",
    "report[\"sanity_samples\"][\"episodes_index_head3\"] = episodes_index.head(3).to_dict(orient=\"records\")\n",
    "report[\"sanity_samples\"][\"episode_counts_by_split_K\"] = (\n",
    "    episodes_index.groupby([\"split\",\"K\"]).size().reset_index(name=\"n_episodes\").to_dict(orient=\"records\")\n",
    "    if episodes_index.shape[0] else []\n",
    ")\n",
    "\n",
    "if episodes_index.shape[0]:\n",
    "    report[\"key_findings\"].append(\"Built episodic user-as-task indices with chronological support→query and disjointness checks.\")\n",
    "else:\n",
    "    report[\"key_findings\"].append(\"No episodes were created with current K/Q; we don’t know yet if larger K is feasible for MARS and may need smaller K or smaller Q.\")\n",
    "\n",
    "write_json_atomic(REPORT_PATH, report)\n",
    "write_json_atomic(MANIFEST_PATH, manifest)\n",
    "\n",
    "print(\"[CELL 05-07] updated:\", REPORT_PATH)\n",
    "print(\"[CELL 05-07] updated:\", MANIFEST_PATH)\n",
    "\n",
    "# close DB to avoid Windows locks\n",
    "try:\n",
    "    con.close()\n",
    "    print(\"[CELL 05-07] closed DuckDB connection\")\n",
    "except Exception as e:\n",
    "    print(\"[CELL 05-07] con.close skipped:\", repr(e))\n",
    "\n",
    "cell_end(\"CELL 05-07\", t0, n_artifacts=len(manifest[\"artifacts\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0adb683",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
